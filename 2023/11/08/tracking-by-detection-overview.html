<!DOCTYPE html>
<html lang="en" id="top">

<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->

  <meta charset="utf-8">
  <title>Miguel Méndez | Multi-Object Tracking by Detection: A Comprehensive Guide</title>
  <meta name="author" content="Miguel Mendez">
  <meta property="og:website" content="Miguel Mendez personal website">
  
  
  <meta name="description" property="og:description" content="Dive into the complexities of object tracking in computer vision with this detailed overview of tracking by detection. Discover the intricacies and advancements of key algorithms like SORT, DeepSORT, and ByteTrack, and learn to select the right one for your project. From foundational concepts to cutting-edge techniques, the performance and applications of multi-object tracking for real-world scenarios is dissected. SORT vs DeepSORT vs ByteTrack vs BoT-SORT vs SMILEtrack.">
  

  
  <meta name="image" property="og:image" content="https://mmeendez8.github.io/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/thumbnail.png">
  

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->

  <!-- preload fonts -->
  <link rel="preload" href="/libs/external/fonts/Graphik-Regular.woff2"" as="font" type="font/woff2" crossorigin>
  <link rel="preload" href="/libs/external/fonts/Graphik-Semibold.woff2"" as="font" type="font/woff2" crossorigin>
  <link rel="preload" href="/libs/external/fonts/Tiempos-Headline-Semibold.woff2"" as="font" type="font/woff2" crossorigin>

  <link rel="stylesheet" href=/libs/custom/my_css.css>
  <link rel="stylesheet" href=/libs/external/fonts/fonts.css>

  <!-- hack for non critical css https://web.dev/defer-non-critical-css/ -->
  <link rel="preload" href=/libs/custom/syntax.css as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript><link rel="stylesheet" href=/libs/custom/syntax.css></noscript>
  
  <!-- non critical lighthouse css -->
  <link rel="preload" href=/libs/external/lightbox/lightbox.css as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript><link rel="stylesheet" href=/libs/external/lightbox/lightbox.css></noscript>

  <!-- Fontello
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet"
    href=/libs/external/fontello-bb2d1770/css/fontello.css>

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href=/libs/icon.png>
  <link rel="shortcut icon" type="image/png" href=/libs/icon.png>

  <!-- Google Analytics -->
  <!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-PTDG7548');</script>
<!-- End Google Tag Manager -->

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-LMHYVFNF1J"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-LMHYVFNF1J');
</script>
    <!-- From https://developer.twitter.com/en/docs/twitter-for-websites/javascript-api/guides/set-up-twitter-for-websites -->
  <script>
  // Log any kind of Web Intent event to Google Analytics
  // Category: "twitter_web_intents"
  // Action: Intent Event Type
  // Label: Identifier for action taken: tweet_id, screen_name/user_id, click region

  // First, load the widgets.js file asynchronously
  window.twttr = (function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0],
      t = window.twttr || {};
    if (d.getElementById(id)) return;
    js = d.createElement(s);
    js.id = id;
    js.src = "https://platform.twitter.com/widgets.js";
    fjs.parentNode.insertBefore(js, fjs);

    t._e = [];
    t.ready = function(f) {
      t._e.push(f);
    };

    return t;
  }(document, "script", "twitter-wjs"));

  // Define our custom event handlers
  function clickEventToAnalytics (intentEvent) {
    if (!intentEvent) return;
    gtag('event', 'Twitter Share', {
      'event_category': 'twitter_share',
      'event_label': label
    });
  }

  // Wait for the asynchronous resources to load
  twttr.ready(function (twttr) {
    // Now bind our custom intent events
    twttr.events.bind('click', clickEventToAnalytics);
  });
  
</script>

  <!-- Twitter cards -->
  <meta name="twitter:site" content="@https://twitter.com/mmeendez8">
  <meta name="twitter:title" content="Multi-Object Tracking by Detection: A Comprehensive Guide">
  
  
  <meta name="twitter:description" content="Dive into the complexities of object tracking in computer vision with this detailed overview of tracking by detection. Discover the intricacies and advancements of key algorithms like SORT, DeepSORT, and ByteTrack, and learn to select the right one for your project. From foundational concepts to cutting-edge techniques, the performance and applications of multi-object tracking for real-world scenarios is dissected. SORT vs DeepSORT vs ByteTrack vs BoT-SORT vs SMILEtrack.">
  
  

  
  <meta name="twitter:card"  content="summary_large_image">
  <meta name="twitter:image" content="https://mmeendez8.github.io/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/thumbnail.png">
  

  <!-- end of Twitter cards -->
  

  <!-- Mathjax -->
  
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [["$", "$"], ["\\(", "\\)"]],
          processEscapes: true
      }
  });
</script>
  
</head>

<body>

  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PTDG7548"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  <header class="the-post-header">
    <div class="container">
      <a href="/">
        <h3>Miguel Méndez</h3>
      </a>
      <div>
        <a  href=/index.html#posts>
         <h3 class="posts-link">Posts</h3>
        </a>
      </div>
    </div>
</header>

<div class="the-post-title-placeholder">
  <div class="offset">
    <div class="the-post-title-text">
      <span class="the-post-date">November 08, 2023 </span>
      <h1 class="the-post-title">Multi-Object Tracking by Detection: A Comprehensive Guide</h1>
      <p>Exploring the Advancements and Accuracy in Multi-Object Tracking Technologies</p>
    </div>
  </div>

  <div class="the-post-title-image">
    <img src="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/thumbnail-800-1b32f1d19.png" alt="Multi-Object Tracking by Detection: A Comprehensive Guide" srcset="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/thumbnail-400-6df784522.webp 400w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/thumbnail-600-6df784522.webp 600w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/thumbnail-800-6df784522.webp 800w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/thumbnail-1000-6df784522.webp 1000w" sizes="(max-width: 767px) 100vw, 50vw" width="1389" height="955">

  </div>
</div>

<div class="share-button">
  <a href="https://twitter.com/share?ref_src=twsrc%5Etfw" class="twitter-share-button" data-show-count="false" ,
    data-size="large" , data-text="Multi-Object Tracking by Detection: A Comprehensive Guide" , data-via="mmeendez8" , data-url="https://mmeendez8.github.io/2023/11/08/tracking-by-detection-overview.html">
  </a>
</div>

<div class="container the-post-content">

  <h2 id="table-of-contents">Table of Contents</h2>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th> </th>
        <th>Year</th>
        <th>Appearence Features</th>
        <th>Camera Compensation</th>
        <th>HOTA MOT20</th>
        <th>Extra data</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><a href="#sort">SORT</a></td>
        <td>2016</td>
        <td>❌</td>
        <td>❌</td>
        <td> </td>
        <td>❌</td>
      </tr>
      <tr>
        <td><a href="#deepsort">DeepSORT</a></td>
        <td>2017</td>
        <td>✅</td>
        <td>❌</td>
        <td> </td>
        <td>✅</td>
      </tr>
      <tr>
        <td><a href="#bytetrack">ByteTrack</a></td>
        <td>2021</td>
        <td>✅</td>
        <td>❌</td>
        <td>61.3</td>
        <td>✅</td>
      </tr>
      <tr>
        <td><a href="#bot-sort">BoT-SORT</a></td>
        <td>2022</td>
        <td>✅</td>
        <td>✅</td>
        <td>63.3</td>
        <td>✅</td>
      </tr>
      <tr>
        <td><a href="#smiletrack">SMILEtrack</a></td>
        <td>2022</td>
        <td>✅</td>
        <td>✅  (?)</td>
        <td>63.4</td>
        <td>✅</td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="introduction">Introduction</h2>

<p>Tracking by detection is an object tracking approach that first detects objects in each frame of a video and then associates the detections across frames. This process involves matching detections by analyzing their location, appearance, or motion characteristics. Tracking by detection has become the most popular method for addressing object tracking due to the rapid development of reliable object detectors.</p>

<p>The intention of this blog is to keep myself updated with the bibliography of tracking by detection methods. My intention is to regularly update this blog with new information and resources I find interesting.
I have included the SORT and DeepSORT papers in the list, despite being older methods, as they laid the groundwork for many of the techniques covered here.</p>

<h2 id="sort">SORT</h2>

<p>It is a very good and simple work from 2016 that quickly became a standard in the field. The author’s main goal was to create the fastest possible tracker relying on the quality of the object detector predictions. Appearance features of the objects are not used; the system relies solely on bounding box position and size.</p>

<p>They employ two classical methods:</p>

<ul>
  <li>
    <p><strong>Kalman Filter:</strong> is in charge of handling motion prediction, this is, figuring out where a track is going to move in the next frame given previous states. Track states are modeled with six different variables:</p>

\[\mathbf{x} = [u,v,s,r,\dot{u},\dot{v},\dot{s}]^T,\]

    <p>These are the center of the target bounding box ($u, v$), the scales and aspect ratio of it ($s, r$) and their velocity components ($\dot{u},\dot{v},\dot{s}$).</p>
  </li>
  <li>
    <p><strong>Hungarian method:</strong> used in the data association step to match new predictions with tracks based on IoU metric.</p>
  </li>
</ul>

<div class="post-center-image">
    <a href="/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/sort.jpg">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/sort-800-ef8ea65f7.jpg" alt="SORT architecture diagram" srcset="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/sort-400-0129c6d0c.webp 400w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/sort-600-0129c6d0c.webp 600w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/sort-800-0129c6d0c.webp 800w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/sort-1000-0129c6d0c.webp 1000w" sizes="(max-width: 767px) 100vw, 80vw" width="1352" height="739" />
</a>

</div>

<p class="image-caption"><em>SORT architecture diagram</em></p>

<ol>
  <li>An object detector returns bounding boxes for frame 0.</li>
  <li>In T=0, a new track is created for each of the predicted bounding boxes</li>
  <li>KF will predict a new position for each of the tracks</li>
  <li>Object detector returns bounding boxes for frame 1</li>
  <li>These bounding boxes are associated with tracks positions predicted by KF</li>
  <li>New tracks are created for unmatched bounding boxes</li>
  <li>Unmatched tracks can be terminated if they are not matched to any detection for $T_{Lost}$ frames.</li>
  <li>Matched tracks and new tracks are passed to the next time step</li>
  <li>Back to 3</li>
</ol>

<h2 id="deepsort">DeepSORT</h2>

<p>DeepSORT is an extension of SORT that uses appearance features. It enhances SORT by adding a simple CNN extension that extracts appearance features from bounding boxes, improving object tracking, especially during occlusions. An object can be re-identified using appearance similarity after being occluded for a long period of time</p>

<p>Each track maintains a gallery of the last \(n\) appearance descriptors, enabling cosine distance calculations between new detections and descriptors. Track age, determined by frames since the last association, plays a crucial role in the association process. DeepSORT adopts a cascade approach, prioritizing tracks with lower ages over a single-step association between predicted Kalman states and new measurements.</p>

<div class="post-center-image">
    <a href="/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/deepsort.jpg">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/deepsort-800-58a403e1a.jpg" alt="DeepSORT architecture diagram" srcset="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/deepsort-400-d49ec61a3.webp 400w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/deepsort-600-d49ec61a3.webp 600w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/deepsort-800-d49ec61a3.webp 800w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/deepsort-1000-d49ec61a3.webp 1000w" sizes="(max-width: 767px) 100vw, 80vw" width="1378" height="725" />
</a>

</div>

<p class="image-caption"><em>DeepSORT architecture diagram</em></p>

<p>There is a small modification on the Kalman Filter prediction step that is included in the <a href="https://github.com/nwojke/deep_sort/blob/master/deep_sort/kalman_filter.py#L108" target="_blank" rel="noopener noreferrer">code</a> but not mentioned in the original paper. The matrices \(Q\), \(R\) of the Kalman Filter were chosen in SORT to be time indepent, however in DeepSORT it was suggested to choose \(Q%\), \(R\) as functions of the scale of the bounding box. This can be due to the scale is less likely to change over time than other features and it can be also be used to compensate for changes in camera’s viewpoint.</p>

<p>The cascade association step would look like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">track_age</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">maximum_age</span><span class="p">):</span>
    <span class="n">tracks_to_associate</span> <span class="o">=</span> <span class="nf">get_tracks_with_age</span><span class="p">(</span><span class="n">tracks</span><span class="p">,</span> <span class="n">track_age</span><span class="p">)</span>
    <span class="nf">associate</span><span class="p">(</span><span class="n">tracks_to_associate</span><span class="p">,</span> <span class="n">detections</span><span class="p">)</span>
    <span class="nf">remove_associated_detections</span><span class="p">(</span><span class="n">detections</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="bytetrack">ByteTrack</h2>

<p>ByteTrack is a recent object tracking algorithm that proposes a simple but effective optimization for the data association step. Most methods filter out detections with low confidence scores. This is because low-confidence detections are more likely to be false positives, or to correspond to objects that are not present in the scene. However, this can lead to problems when tracking objects that are partially occluded or that undergo significant appearance changes.</p>

<p>ByteTrack addresses this problem by using all detections, regardless of their confidence score. The algorithm works in two steps:</p>

<ol>
  <li><strong>High-confidence detections</strong>: High-confidence detections are associated with tracks using intersection-over-union (IoU) or appearance features. Both approaches are evaluated in the results section of the paper.</li>
  <li><strong>Low-confidence detections</strong>: Low-confidence detections are associated with tracks using only IoU. This is because low-confidence detections are more likely to be spurious or inaccurate, so it is important to be more conservative when associating them with tracks.</li>
</ol>

<div class="post-center-image">
    <a href="/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/bytetrack.jpg">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/bytetrack-800-eb704f5e3.jpg" alt="ByteTrack architecture diagram" srcset="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/bytetrack-400-1a0b7d0b3.webp 400w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/bytetrack-600-1a0b7d0b3.webp 600w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/bytetrack-800-1a0b7d0b3.webp 800w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/bytetrack-1000-1a0b7d0b3.webp 1000w" sizes="(max-width: 767px) 100vw, 80vw" width="1311" height="762" />
</a>

</div>

<p class="image-caption"><em>ByteTrack architecture diagram</em></p>

<p>The ByteTrack algorithm has been shown to be very effective and it is currently among the top-performing methods on the <a href="https://paperswithcode.com/sota/multi-object-tracking-on-mot20-1" target="_blank" rel="noopener noreferrer">MOT Challenge leaderboard</a>.</p>

<h2 id="bot-sort">BoT-SORT</h2>

<p>I personally love the BoT-SORT paper. It is build upon ByteTrack and it combines three different ideas that work very well together. These are:</p>

<ol>
  <li>
    <p><strong>Kalman Filter update</strong>: SORT introduced a way of modelling the track state vector using a seven-tuple \(\mathbf{x} = [x_c,y_c,a,h,\dot{x_c},\dot{y_c},\dot{s}]^T\). BoT-SORT proposes to replace the scale and aspect ratio of the bounding box  (\(s\), \(a\)) with the widht and height (\(w\), \(h\)) to create an eight-tuple:</p>

\[\mathbf{x} = [x_c,y_c,w,h,\dot{x_c},\dot{y_c},\dot{w}, \dot{h}]^T\]

    <p>They also choose Q, R matrices from the Kalman Filter as functions of the bounding box width and height. Recall that in DeepSORT, only the scale of the bounding box influenced on the Q, R matrices (see section 3.1 of <a href="https://arxiv.org/pdf/2206.14651v2.pdf" target="_blank" rel="noopener noreferrer">BoT-SORT paper</a> for more details).</p>
  </li>
  <li>
    <p><strong>Camera Motion Compensation</strong>: In dynamic camera situations, objects that are static can appear to move, and objects that are moving can appear to be static. The Kalman Filter does not take camera motion into account for its predictions, so BoT-SORT proposes to incorporate this knowledge. To do this, they use the global motion compensation technique (GMC) from the OpenCV Video Stabilization module. This technique extracts keypoints from consecutive frames and computes the homography matrix between the matching pairs. This matrix can then be used to transform the prediction bounding box from the coordinate system of frame \(k − 1\) to the coordinates of the next frame \(k\) (see section 3.2 of <a href="https://arxiv.org/pdf/2206.14651v2.pdf" target="_blank" rel="noopener noreferrer">BoT-SORT paper</a> to a full formulation on how incorporate the homography matrix in the prediction step).</p>

    <div class="post-center-image">
     <a href="/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/cmc.png">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/cmc-800-3538f10c9.png" alt="Camera movement example" srcset="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/cmc-400-ed5e82503.webp 400w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/cmc-600-ed5e82503.webp 600w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/cmc-800-ed5e82503.webp 800w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/cmc-1000-ed5e82503.webp 1000w" sizes="(max-width: 767px) 100vw, 80vw" width="1152" height="1296" />
</a>

 </div>

    <p class="image-caption"><em>Player is static on the pitch while throwing the ball but location on the image changes due to camera movement.</em></p>
  </li>
  <li>
    <ol>
      <li><strong>IoU - ReID Fusion</strong>:  BoT-SORT proposes a new way of solving the association step by combining motion and appearance information. The cost matrix elements are computed as follows:</li>
    </ol>

\[\hat{d}^{cos}_{i,j} = 
 \begin{equation}
 \begin{cases}
 0.5 \cdot {d}^{cos}_{i,j}, ({d}^{cos}_{i,j} &lt; \theta_{emb}) \hat{} ({d}^{iou}_{i,j} &lt; \theta_{iou})\\
 1, \text{otherwise}
 \end{cases}
 \end{equation}\]

\[C_{i,j} = min(d^{iou}_{i,j}, \hat{d}^{cos}_{i,j})\]

    <p>The appearence distance is recomputed as shown in the first equation. The idea is to filter out pairs with large iou or large appearance distance (two different thresholds are used here).  Then, the cost matrix element is updated as the minimum between the IoU and the new appearance distance. This method seems to be handcrafted, and the authors likely spent a significant amount of time evaluating different thresholds on the MOT17 dataset to arrive at this formulation. Note thresholds are callibrated using MOT17 validation set.</p>
  </li>
</ol>

<div class="post-center-image">
    <a href="/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/botsort.jpg">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/botsort-800-ed3f0f9ff.jpg" alt="BoT-SORT architecture diagram" srcset="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/botsort-400-f1cf4e2c5.webp 400w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/botsort-600-f1cf4e2c5.webp 600w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/botsort-800-f1cf4e2c5.webp 800w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/botsort-1000-f1cf4e2c5.webp 1000w" sizes="(max-width: 767px) 100vw, 80vw" width="1329" height="752" />
</a>

</div>

<p class="image-caption"><em>ByteTrack architecture diagram</em></p>

<h2 id="smiletrack">SMILEtrack</h2>

<p>This method currently holds the title of being the State-of-the-Art (SOTA) in the MOT17 and MOT20 datasets. It builds upon ByteTrack but throws in a handful of fresh ideas designed to give appearance features more importance.</p>

<p>I spent a couple hours trying to understand the paper but I have to admit it felt very confusing to me, so I went straight to the <a href="https://github.com/pingyang1117/SMILEtrack_Official" target="_blank" rel="noopener noreferrer">code</a>. Things got even trickier there; I noticed quite a few things that didn’t align with what was mentioned in the paper. As a results, so I opened an <a href="https://github.com/pingyang1117/SMILEtrack_Official/issues/3" target="_blank" rel="noopener noreferrer">issue</a> on the project’s GitHub repository. I’ll update this section once I hear back from the authors.</p>

<!-- Let's see what they are:

1. **Similarity Learning Module (SLM)**: It is a Siamese network that computes appearence similarity between two objects using a Patch Self-Attention (PSA) block. Think about it as a boosted feature descriptor that incorporates attention mechanism following ViT style.

2. **Similarity Matching Cascade (SMC)**: Very similar to ByteTrack, it splits the data association step in two parts depending on detection scores. First, high confidence detections are tried to be matched with the tracks, for then proceeding with the low confidence ones. In both cases, IoU and appearence features are used to compute the cost matrix. The key addition is a new **GATE function** that is used right after the high confidence association. Unmatched objects with high scores might find matches in subsequent frames due to occlusions or lighting changes. When an object passes this GATE function, a new track is created for it.

Idea is, if IoU is high but they don't look alike, probably occlusion. If they don't match but they have some past track that looks alike, create a new track for it (this i don't understand very well)
There is camera motion correction in the code but not in the paper!!!!
 -->

<h2 id="references">References</h2>

<ul>
  <li>[<a href="https://arxiv.org/pdf/1602.00763.pdf">1</a>] Bewley, A., Ge, Z., Ott, L., Ramos, F., &amp; Upcroft, B. (2016, September). Simple online and realtime tracking. In 2016 IEEE international conference on image processing (ICIP) (pp. 3464-3468). IEEE.</li>
  <li>[<a href="https://arxiv.org/pdf/1703.07402.pdf">2</a>] Wojke, N., Bewley, A., &amp; Paulus, D. (2017, September). Simple online and realtime tracking with a deep association metric. In 2017 IEEE international conference on image processing (ICIP) (pp. 3645-3649). IEEE.</li>
  <li>[<a href="https://arxiv.org/pdf/2110.06864.pdf">3</a>] Zhang, Y., Sun, P., Jiang, Y., Yu, D., Weng, F., Yuan, Z., … &amp; Wang, X. (2022, October). Bytetrack: Multi-object tracking by associating every detection box. In European Conference on Computer Vision (pp. 1-21). Cham: Springer Nature Switzerland.</li>
  <li>[<a href="https://arxiv.org/pdf/2206.14651.pdf">4</a>] Aharon, N., Orfaig, R., &amp; Bobrovsky, B. Z. (2022). BoT-SORT: Robust associations multi-pedestrian tracking. arXiv preprint arXiv:2206.14651.</li>
  <li>[<a href="https://arxiv.org/pdf/2211.08824.pdf">5</a>] Wang, Y. H. (2022). SMILEtrack: SiMIlarity LEarning for Multiple Object Tracking. arXiv preprint arXiv:2211.08824.</li>
</ul>

  
</div>

<div class="comments">
  <script src="https://utteranc.es/client.js"
        repo="mmeendez8/mmeendez8.github.io"
        issue-term="url"
        theme="github-light"
        crossorigin="anonymous"
        async>
  </script>
</div>

    <div class="footer">
      <div class="footer-sign">
      <p> <a  href=/index.html#top>Miguel Méndez </a></p>
      </div>

      <div class="footer-thanks">
        <p>based on <a href="http://web.media.mit.edu/~msaveski" target="_blank" rel="noopener">Martin Saveski</a> and <a href='https://marinaaisa.com/' target="_blank" rel="noopener">Marina Aisa</a> templates</p>
      </div>

      <div class="footer-icons">
        <a href='https://github.com/mmeendez8' target="_blank" rel="noopener">
          <i class="icon-github-circled" aria-hidden="true"></i>
        </a>

        <a href='https://www.linkedin.com/in/miguel-mendez/' target="_blank" rel="noopener">
          <i class="icon-linkedin-squared" aria-hidden="true"></i>
        </a>

        <a href='https://twitter.com/mmeendez8' target="_blank" rel="noopener">
          <i class="icon-twitter-squared" aria-hidden="true"></i>
        </a>

        <a href='https://medium.com/@miguelmendez_' target="_blank" rel="noopener">
          <i class="icon-medium" aria-hidden="true"></i>
        </a>

        <a href='https://stackoverflow.com/users/8380638/m33n' target="_blank" rel="noopener">
          <i class="icon-stackoverflow" aria-hidden="true"></i>
        </a>

        <a href='/feed.xml' target="_blank" rel="noopener">
          <i class="icon-rss-squared" aria-hidden="true"></i>
        </a>
        
      </div>

    </div>

  <!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <script type="text/javascript" src=/libs/external/lightbox/lightbox.js defer></script>
</body>

</html>
