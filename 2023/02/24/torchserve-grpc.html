<!DOCTYPE html>
<html lang="en" id="top">

<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->

  <meta charset="utf-8">
  <title>Miguel Méndez | Image Transmission for Computer Vision: A Comparison of Torchserve's REST and gRPC</title>
  <meta name="author" content="Miguel Mendez">
  <meta property="og:website" content="Miguel Mendez personal website">
  
  
  <meta name="description" property="og:description" content=" This post compares the performance of Torchserve's REST and gRPC communication protocols for transmitting images to a computer vision deep learning model. We conducted benchmarks for structured data, Base64 encoded images, and binary image transmission. The goal is to help practitioners make informed decisions when choosing the right communication protocol for their specific use case, taking into account factors such as ease of implementation and familiarity with the technology.">
  

  
  <meta name="image" property="og:image" content="https://mmeendez8.github.io/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/thumbnail.jpg">
  

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->

  <!-- preload fonts -->
  <link rel="preload" href="/libs/external/fonts/Graphik-Regular.woff2"" as="font" type="font/woff2" crossorigin>
  <link rel="preload" href="/libs/external/fonts/Graphik-Semibold.woff2"" as="font" type="font/woff2" crossorigin>
  <link rel="preload" href="/libs/external/fonts/Tiempos-Headline-Semibold.woff2"" as="font" type="font/woff2" crossorigin>

  <link rel="stylesheet" href=/libs/custom/my_css.css>
  <link rel="stylesheet" href=/libs/external/fonts/fonts.css>

  <!-- hack for non critical css https://web.dev/defer-non-critical-css/ -->
  <link rel="preload" href=/libs/custom/syntax.css as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript><link rel="stylesheet" href=/libs/custom/syntax.css></noscript>
  
  <!-- non critical lighthouse css -->
  <link rel="preload" href=/libs/external/lightbox/lightbox.css as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript><link rel="stylesheet" href=/libs/external/lightbox/lightbox.css></noscript>

  <!-- Fontello
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet"
    href=/libs/external/fontello-bb2d1770/css/fontello.css>

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href=/libs/icon.png>
  <link rel="shortcut icon" type="image/png" href=/libs/icon.png>

  <!-- Google Analytics -->
  <!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-PTDG7548');</script>
<!-- End Google Tag Manager -->

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-LMHYVFNF1J"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-LMHYVFNF1J');
</script>
    <!-- From https://developer.twitter.com/en/docs/twitter-for-websites/javascript-api/guides/set-up-twitter-for-websites -->
  <script>
  // Log any kind of Web Intent event to Google Analytics
  // Category: "twitter_web_intents"
  // Action: Intent Event Type
  // Label: Identifier for action taken: tweet_id, screen_name/user_id, click region

  // First, load the widgets.js file asynchronously
  window.twttr = (function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0],
      t = window.twttr || {};
    if (d.getElementById(id)) return;
    js = d.createElement(s);
    js.id = id;
    js.src = "https://platform.twitter.com/widgets.js";
    fjs.parentNode.insertBefore(js, fjs);

    t._e = [];
    t.ready = function(f) {
      t._e.push(f);
    };

    return t;
  }(document, "script", "twitter-wjs"));

  // Define our custom event handlers
  function clickEventToAnalytics (intentEvent) {
    if (!intentEvent) return;
    gtag('event', 'Twitter Share', {
      'event_category': 'twitter_share',
      'event_label': label
    });
  }

  // Wait for the asynchronous resources to load
  twttr.ready(function (twttr) {
    // Now bind our custom intent events
    twttr.events.bind('click', clickEventToAnalytics);
  });
  
</script>

  <!-- Twitter cards -->
  <meta name="twitter:site" content="@https://twitter.com/mmeendez8">
  <meta name="twitter:title" content="Image Transmission for Computer Vision: A Comparison of Torchserve's REST and gRPC">
  
  
  <meta name="twitter:description" content=" This post compares the performance of Torchserve's REST and gRPC communication protocols for transmitting images to a computer vision deep learning model. We conducted benchmarks for structured data, Base64 encoded images, and binary image transmission. The goal is to help practitioners make informed decisions when choosing the right communication protocol for their specific use case, taking into account factors such as ease of implementation and familiarity with the technology.">
  
  

  
  <meta name="twitter:card"  content="summary_large_image">
  <meta name="twitter:image" content="https://mmeendez8.github.io/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/thumbnail.jpg">
  

  <!-- end of Twitter cards -->
  

  <!-- Mathjax -->
  
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [["$", "$"], ["\\(", "\\)"]],
          processEscapes: true
      }
  });
</script>
  
</head>

<body>

  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PTDG7548"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  <header class="the-post-header">
    <div class="container">
      <a href="/">
        <h3>Miguel Méndez</h3>
      </a>
      <div>
        <a  href=/index.html#posts>
         <h3 class="posts-link">Posts</h3>
        </a>
      </div>
    </div>
</header>

<div class="the-post-title-placeholder">
  <div class="offset">
    <div class="the-post-title-text">
      <span class="the-post-date">February 24, 2023 </span>
      <h1 class="the-post-title">Image Transmission for Computer Vision: A Comparison of Torchserve's REST and gRPC</h1>
      <p>Benchmarking protocols performance for sending images</p>
    </div>
  </div>

  <div class="the-post-title-image">
    <img src="/generated/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/thumbnail-800-0a3487f5b.jpg" alt="Image Transmission for Computer Vision: A Comparison of Torchserve's REST and gRPC" srcset="/generated/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/thumbnail-400-0fd0326d8.webp 400w, /generated/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/thumbnail-600-0fd0326d8.webp 600w, /generated/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/thumbnail-800-0fd0326d8.webp 800w, /generated/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/thumbnail-1000-0fd0326d8.webp 1000w" sizes="(max-width: 767px) 100vw, 50vw" width="3200" height="1879">

  </div>
</div>

<div class="share-button">
  <a href="https://twitter.com/share?ref_src=twsrc%5Etfw" class="twitter-share-button" data-show-count="false" ,
    data-size="large" , data-text="Image Transmission for Computer Vision: A Comparison of Torchserve's REST and gRPC" , data-via="mmeendez8" , data-url="https://mmeendez8.github.io/2023/02/24/torchserve-grpc.html">
  </a>
</div>

<div class="container the-post-content">

  <p>Special thanks to <a href="https://www.linkedin.com/in/jguzmanfd/">Javier Guzman</a> for working with me in completing the benchmarking discussed in this post.</p>

<p>In the past few weeks, we have been exploring the potential advantages of adopting gRPC to enhance the performance of our services. Although I have conducted extensive research on this topic, I have not been able to find relevant information that specifically addresses our use case, which involves transmitting images to a model server and receiving a response in the most efficient manner. While there are numerous benchmarks that demonstrate significant performance improvements when migrating from REST to gRPC using structured data, it has been challenging to locate a similar benchmark for image transmission… And that is the main reason behind this post!</p>

<p>All the code for the different benchmarks can be found in <a href="https://github.com/mmeendez8/grpc_vs_rest" target="_blank" rel="noopener noreferrer">this Github repository</a>. You can find instructions in the README file. It’s important to note that our primary objective was to conduct this testing on our cloud infrastructure, where both the servers and clients were deployed on the same Kubernetes cluster. This allowed us to replicate a real-world scenario as closely as possible.</p>

<h2 id="some-thoughts-on-grpc">Some thoughts on gRPC</h2>

<p>When you start reading about gRPC, you soon realize that it involves two main features that can really help you to speed up your system communications.</p>

<h3 id="http2">HTTP2</h3>

<p>gRPC is built on the HTTP/2 protocol, which was specifically designed to address the latency issues of its predecessor, HTTP/1.1. There are two key features of HTTP/2 that are particularly relevant to our benchmarking efforts:</p>

<ul>
  <li>
    <p><strong>Multiplexed streams</strong>: With HTTP/2, multiple requests and responses can be transmitted over a single connection. While HTTP/1.1 can also reuse connections through pooling, the ability to multiplex streams becomes more important as the number of servers increases. When multiple HTTP requests are performed in a very short span of time, HTTP/1.1 has no way to share those connections. Therefore, it will create new connections to the content server for each HTTP request (see <a href="https://blog.codavel.com/http2-multiplexing" target="_blank" rel="noopener noreferrer">here</a> for a extended explanation)</p>
  </li>
  <li>
    <p><strong>Binary protocol</strong>: Unlike HTTP/1.1, which is text-based, HTTP/2 uses a binary protocol which facilitates more efficient parsing. This can have a significant impact on performance, particularly when dealing with large datasets such as images.</p>
  </li>
</ul>

<h3 id="protobuf">Protobuf</h3>

<p>Protocol Buffers, also known as Protobuf, is a language-agnostic binary serialization format developed by Google. It is used for efficient data <strong>serialization of structured data</strong> and communication between applications. It is faster than JSON for two reasons:</p>

<ul>
  <li><strong>Messages are shorter</strong>. In Protobuf messages do not contain any metadata or extra information such as field names and data types. This is not needed since the data schema has been strictly predefined in the <code class="language-plaintext highlighter-rouge">.proto</code> file. It also uses a compact binary representation, variable-length encoding, which means that the number of bytes required to represent a value depends on its size.</li>
  <li><strong>Serialization is faster</strong>. Converting messages to and from bytes is faster than in JSON because of its binary format and predefined schema. Decoding can be optimized and parallelized.</li>
</ul>

<p>In <a href="https://nilsmagnus.github.io/post/proto-json-sizes/" target="_blank" rel="noopener noreferrer">this post</a> you can see a good comparison of Protobuf vs JSON sizes for structured data. TLDR: Protobuf is always smaller than gzipped json but seems to lose its clear advantage when mesage sizes are large.</p>

<h3 id="how-does-this-apply-to-images">How does this apply to images?</h3>

<p>Structured data is text that has been predefined and formatted to a set structure. Protobuf can take advantage of the schema definitions of the data to speed up serialization and compression size. However, images do not fall under the category of structured text. Basically if you want to convert an image to bytes in an efficient manner and without losing information you have to use specific handcrafted methods that have been carefully designed for this, such as JPEG, PNG… In other words, Protobuf is not going to help you here since compression and serialization will depend on your image library. See this example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># create random 100x100 rgb image
</span><span class="n">image</span> <span class="o">=</span> <span class="n">numpy</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="mi">255</span>
<span class="c1"># serialize image to jpg using opencv
</span><span class="n">encoded_image</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">imencode</span><span class="p">(</span><span class="sh">"</span><span class="s">.jpg</span><span class="sh">"</span><span class="p">,</span> <span class="n">image</span><span class="p">)[</span><span class="mi">1</span><span class="p">].</span><span class="nf">tobytes</span><span class="p">()</span>
<span class="c1"># fake send with grpc
</span><span class="n">grpc</span><span class="p">.</span><span class="nf">send</span><span class="p">(</span><span class="n">encoded_image</span><span class="p">)</span>
</code></pre></div></div>

<p>The key feature here is that Protobuf is not really helping. Given that it is one of key points of gRPC, differences between REST and gRPC cannot be that high here… Let’s check this with real numbers:</p>

<h2 id="1-base-benchmark">1. Base benchmark</h2>

<p>First thing we wanted to do is check if we were able to reproduce those benchmarks we found on the web. The idea is simple, create two equivalent REST and gRPC servers and measure the time they take to process and respond to different requests.
The gRPC server has been implemented using <a href="https://grpc.io/docs/languages/python/basics/" target="_blank" rel="noopener noreferrer">python grpc library</a> and we have used <a href="https://fastapi.tiangolo.com/" target="_blank" rel="noopener noreferrer">FastAPI</a> for the REST one.</p>

<div class="post-center-image">
<a href="/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/cat_bytes.png">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/cat_bytes-768-253323f5f.png" alt="Cat being compressed to bytes" srcset="/generated/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/cat_bytes-400-b0be83dae.webp 400w, /generated/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/cat_bytes-600-b0be83dae.webp 600w, /generated/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/cat_bytes-768-b0be83dae.webp 768w" sizes="(max-width: 767px) 100vw, 80vw" width="768" height="768" />
</a>

</div>

<p class="image-caption"><em>This is what Stable Diffusion creates with the prompt “an image of a cat is being encoded into a chunk of bytes”</em></p>

<p>We decided to measure three different requests and using a single response for all of them. The gRPC <code class="language-plaintext highlighter-rouge">.proto</code> file for those requests looks like the following:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BasicRequest</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Structured data request, we expect to match online benchmarks with this
    </span><span class="sh">"""</span>
    <span class="n">field1</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">field2</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">field3</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">field4</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>

<span class="k">class</span> <span class="nc">ImageBase64Request</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Encode image as a string using Base64 encoding. 
    This is a very bad solution (but simple to do) that should always be avoided
    </span><span class="sh">"""</span>
    <span class="n">image</span><span class="p">:</span> <span class="nb">str</span>

<span class="k">class</span> <span class="nc">ImageBinaryRequest</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Contains an image encoded as bytes.
    </span><span class="sh">"""</span>
    <span class="n">image</span><span class="p">:</span> <span class="nb">bytes</span>

<span class="k">class</span> <span class="nc">BasicResponse</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">prediction1</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span>
    <span class="n">prediction2</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>
</code></pre></div></div>

<p>Note REST’s requests and responses are identical to these so we can make a fair comparison.</p>

<p>Our client does a very simple thing, sends twenty concurrent requests to each server and waits for a response. It repeats this ten times for then computing the average time it took. Pseudocode for the client it is shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">start</span><span class="o">=</span> <span class="nf">time</span><span class="p">()</span>
    <span class="nf">send_concurrent_request_to_specific_server</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">times</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="nf">start</span><span class="p">())</span>

<span class="n">average_time</span> <span class="o">=</span> <span class="nf">mean</span><span class="p">(</span><span class="n">times</span><span class="p">)</span>
</code></pre></div></div>

<p>We tested this for three different image sizes. Results are collected below:</p>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th> </th>
        <th>Basic (0.001 MB)</th>
        <th>B64 (0.306 MB)</th>
        <th>Binary (0.229 MB)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>REST</td>
        <td>0.0723</td>
        <td>0.0943</td>
        <td>0.0572</td>
      </tr>
      <tr>
        <td>gRPC</td>
        <td>0.0093 (x7.7)</td>
        <td>0.0179 (x5.2)</td>
        <td>0.0120 (x4.7)</td>
      </tr>
    </tbody>
  </table>

  <p>Table 1. Results for small images: 360x640</p>

</div>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th> </th>
        <th>Basic (0.001 MB)</th>
        <th>B64 (1.160 MB)</th>
        <th>Binary (0.870 MB)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>REST</td>
        <td>0.0611</td>
        <td>0.2350</td>
        <td>0.0872</td>
      </tr>
      <tr>
        <td>gRPC</td>
        <td>0.0090 (x6.7)</td>
        <td>0.0926 (x2.5)</td>
        <td>0.0570 (x1.5)</td>
      </tr>
    </tbody>
  </table>

  <p>Table 2. Results for medium images: 720x1280</p>

</div>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th> </th>
        <th>Basic (0.001 MB)</th>
        <th>B64 (3.094 MB)</th>
        <th>Binary (2.320 MB)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>REST</td>
        <td>0.0583</td>
        <td>0.8056</td>
        <td>0.1909 (x1.03)</td>
      </tr>
      <tr>
        <td>gRPC</td>
        <td>0.0097 (x6)</td>
        <td>0.2793 (x2.9)</td>
        <td>0.1974</td>
      </tr>
    </tbody>
  </table>

  <p>Table 3. Results for large images: 1080x1920</p>

</div>

<p>We can extract some conclussion from previous tables:</p>

<ol>
  <li>gRPC achieves around a x6 improvement with respect to REST for structured data (Basic column). The results <a href="https://medium.com/@EmperorRXF/evaluating-performance-of-rest-vs-grpc-1b8bdf0b22da#:~:text=gRPC%20is%20roughly%207%20times,of%20HTTP%2F2%20by%20gRPC." target="_blank" rel="noopener noreferrer">match online benchmarks</a> and we know this is because we are taking advantage of Protobuf serialization and HTTP2 protocol.</li>
  <li>For Base64 and Binary we observe a relation between image size and gRPC performance. As the image size increase, the difference between REST and gRPC are smaller.</li>
  <li>In the Base64 case, gRPC helps to serialize faster and in a more optimal way the string. We know from <a href="https://nilsmagnus.github.io/post/proto-json-sizes/" target="_blank" rel="noopener noreferrer">this post</a> that Protobuf loses its advantage when message size increases.</li>
  <li>Binary is a special case as we know we are not getting any advantage from using Protobuf for our serialization and message size (this is determined by the image format we chose). On the contrary it is harming our performance. There is still  some encoding going on in Protobuf, since it needs to format our chunk of image bytes inside the Protobuf message format. This little thing might be making REST as good as gRPC for large images!</li>
</ol>

<h2 id="torchserve-benchmark">Torchserve benchmark</h2>

<p>I have been using <a href="https://pytorch.org/serve/">TorchServe</a> for a while now and I am quite happy with it. It provides all the flexibility I need and it is quite simple to set up. Model handlers allow you to customize every detail for your specific model without really worrying about other complex things such as batching and queing requests. 
I do not intend to give an overview of TorchServe or make a comparison of its advantages compared to other inference servers, I will leave that for a plausible future post.</p>

<p>The documentation for Torchserve’s <a href="https://pytorch.org/serve/grpc_api.html" target="_blank" rel="noopener noreferrer">gRPC API</a> could be improved, as it currently requires users to download the official repository to generate a Python gRPC client stub from the proto files. However, I have attached these files to the repository, so you can easily run the benchmark without having to worry about this step.</p>

<p>The experiment is very similar to the previous one, sending 20 concurrent request and repeating that 10 times to measure the average time. I am going to use one of the pytorch vision model examples, <a href="https://pytorch.org/hub/pytorch_vision_densenet/" target="_blank" rel="noopener noreferrer">densenet161</a>. The model is not important here since we do not really care about inference results. Let’s see some results:</p>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th> </th>
        <th>B64 (0.306 MB)</th>
        <th>Binary (0.229)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>REST</td>
        <td>0.884</td>
        <td>0.628</td>
      </tr>
      <tr>
        <td>gRPC</td>
        <td>X</td>
        <td>0.645</td>
      </tr>
    </tbody>
  </table>

  <p>Table 4. Results for small images: 360x640</p>

</div>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th> </th>
        <th>B64 (0.306 MB)</th>
        <th>Binary (0.229)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>REST</td>
        <td>1.262</td>
        <td>0.946</td>
      </tr>
      <tr>
        <td>gRPC</td>
        <td>X</td>
        <td>0.927</td>
      </tr>
    </tbody>
  </table>

  <p>Table 5. Results for medium images: 720x1280</p>

</div>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th> </th>
        <th>B64 (0.306 MB)</th>
        <th>Binary (0.229)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>REST</td>
        <td>2.188</td>
        <td>1.384</td>
      </tr>
      <tr>
        <td>gRPC</td>
        <td>X</td>
        <td>1.422</td>
      </tr>
    </tbody>
  </table>

  <p>Table 6. Results for large images: 1080x1920</p>

</div>

<p>Note there are not results for B64 gRPC since this is not allowed by Torchserve schema definition.</p>

<p>Translating the insights gained from benchmarking with the base servers can be challenging. The tables indicate that Base64 encoding should be avoided and that there are no significant performance differences between using gRPC and REST.</p>

<p>Two factors contribute to the similar performance results for gRPC and REST. Firstly, the model’s inference time is considerably longer than the networking time, making it difficult to discern the small gains obtained by changing the transmission protocol. For example, sending 20 large images concurrently in the simple base case (Table 3) took roughly 0.19s, whereas we are now spending approximately 1.4 seconds (Table 6), highlighting the significant impact of model inference time on the comparison.</p>

<p>Secondly, the Torchserve implementation plays a role in these results. It has been observed that Torchserve’s <code class="language-plaintext highlighter-rouge">.proto</code> definition for <a href="https://github.com/pytorch/serve/blob/master/frontend/server/src/main/resources/proto/inference.proto#L20-L23" target="_blank" rel="noopener noreferrer">prediction response</a> is too generic and it cannot be personalized with your model specifics.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">message</span> <span class="n">PredictionResponse</span> <span class="p">{</span>
    <span class="o">//</span> <span class="n">Response</span> <span class="n">content</span> <span class="k">for</span> <span class="n">prediction</span>
    <span class="nb">bytes</span> <span class="n">prediction</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>This means that your response will be converted to a chunk of bytes so you would not be getting any advantage from Protobuf serialization (similar to what happens with images). For example if our model returns three lists of bounding boxes, class and scores, the <code class="language-plaintext highlighter-rouge">.proto</code> file for our response could be something like:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">message</span> <span class="n">PredictionResponse</span> <span class="p">{</span>
    <span class="n">repeated</span> <span class="nb">float</span> <span class="n">scores</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
    <span class="n">repeated</span> <span class="n">int32</span> <span class="n">scores</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
    <span class="n">repeated</span> <span class="n">repeated</span> <span class="n">int32</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The differences between this response and the one provided by Torchserve are clear. You do not get any of the Protobuf advantage since the Torchserve schema definition is too general. A better or more customizable definition such as the one provided by Tfserving, of the <code class="language-plaintext highlighter-rouge">.proto</code> file could help boost performance.</p>

<h2 id="conclusion">Conclusion</h2>

<p>If you’re using Torchserve to serve computer vision models, it’s recommended to steer clear of gRPC. Our findings show that there are no performance benefits to using gRPC. Moreover, it adds code complexity while hindering debugging due to its non-human-readable messages. Since REST is more commonly used, most developers are already familiar with it. Switching to gRPC in this scenario comes with a learning curve that doesn’t offer any significant advantages.</p>

<p><em>Any ideas for future posts or is there something you would like to comment? Please feel free to reach out via <a href="https://twitter.com/mmeendez8" target="_blank" rel="noopener noreferrer">Twitter</a> or <a href="https://github.com/mmeendez8" target="_blank" rel="noopener noreferrer">Github</a></em></p>


  
</div>

<div class="comments">
  <script src="https://utteranc.es/client.js"
        repo="mmeendez8/mmeendez8.github.io"
        issue-term="url"
        theme="github-light"
        crossorigin="anonymous"
        async>
  </script>
</div>

    <div class="footer">
      <div class="footer-sign">
      <p> <a  href=/index.html#top>Miguel Méndez </a></p>
      </div>

      <div class="footer-thanks">
        <p>based on <a href="http://web.media.mit.edu/~msaveski" target="_blank" rel="noopener">Martin Saveski</a> and <a href='https://marinaaisa.com/' target="_blank" rel="noopener">Marina Aisa</a> templates</p>
      </div>

      <div class="footer-icons">
        <a href='https://github.com/mmeendez8' target="_blank" rel="noopener">
          <i class="icon-github-circled" aria-hidden="true"></i>
        </a>

        <a href='https://www.linkedin.com/in/miguel-mendez/' target="_blank" rel="noopener">
          <i class="icon-linkedin-squared" aria-hidden="true"></i>
        </a>

        <a href='https://twitter.com/mmeendez8' target="_blank" rel="noopener">
          <i class="icon-twitter-squared" aria-hidden="true"></i>
        </a>

        <a href='https://medium.com/@miguelmendez_' target="_blank" rel="noopener">
          <i class="icon-medium" aria-hidden="true"></i>
        </a>

        <a href='https://stackoverflow.com/users/8380638/m33n' target="_blank" rel="noopener">
          <i class="icon-stackoverflow" aria-hidden="true"></i>
        </a>

        <a href='/feed.xml' target="_blank" rel="noopener">
          <i class="icon-rss-squared" aria-hidden="true"></i>
        </a>
        
      </div>

    </div>

  <!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <script type="text/javascript" src=/libs/external/lightbox/lightbox.js defer></script>
</body>

</html>
