<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://mmeendez8.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://mmeendez8.github.io/" rel="alternate" type="text/html" /><updated>2024-03-04T10:45:49+00:00</updated><id>https://mmeendez8.github.io/feed.xml</id><entry><title type="html">NFL Field Mapping: A Journey Through Sports Analytics and Homography</title><link href="https://mmeendez8.github.io/2024/02/07/nfl-field-mapping.html" rel="alternate" type="text/html" title="NFL Field Mapping: A Journey Through Sports Analytics and Homography" /><published>2024-02-07T00:00:00+00:00</published><updated>2024-02-07T00:00:00+00:00</updated><id>https://mmeendez8.github.io/2024/02/07/nfl-field-mapping</id><content type="html" xml:base="https://mmeendez8.github.io/2024/02/07/nfl-field-mapping.html"><![CDATA[<p>I have been working in sports analytics for 2 years now. I am mainly focusing on the computer vision side of things but saying ‚Äúsports analytics‚Äù is a good way to make it sound more interesting. My goal is simple: extract as much information as possible from sports event video feeds and ensure the data is high-quality. In order to achieve this, I must be able to pinpoint the real world location of the objects observed in the video feed. In other words, map pixels in the video feed to real world coordinates. That mapping is what I refer to as homography. I have already written a post in the company blog about this. If you are interested in why it matters and what it can be used for, you can check it out <a href="https://statsbomb.com/articles/football/creating-better-data-ai-homography-estimation/" target="_blank" rel="noopener noreferrer">here</a>.</p>

<p>Lately, I‚Äôve been inspired by the incredible projects people have been creating with the help of Copilot. This sparked my curiosity to explore firsthand the experience of coding with a heavy reliance on this tool. I‚Äôve decided to challenge myself by attempting to write some JavaScript code. So, be kind and withhold judgement on any of the code you see. After all, I‚Äôm no JavaScript developer; I‚Äôm essentially in GPT-4‚Äôs hands here üòé.</p>

<h2 id="the-goal">The goal</h2>

<p>The goal is straightforward: for any given American football NFL event video feed, I want to map the pixels in the video to their corresponding real-world coordinates. Essentially, I want to pinpoint the location of the ball, the players, the goal, etc. The simplest approach to tackle this problem consists of working at the frame level and figuring out how to match each image to a predefined pitch template.</p>

<p>When I joined <a href="https://statsbomb.com/" target="_blank" rel="noopener noreferrer">StatsBomb</a>, there was a similar and more advanced tool already developed. My goal here was just to be sure I could replicate the entire process from scratch and have a complete understanding of it. So I wanted to develop a simple web app that allows users to upload an image and then find the correspondence between that image and the pitch template. The pitch template is a basic image of the pitch, including lines and goalposts.</p>

<h2 id="the-result">The result</h2>

<p>Here‚Äôs the result for you to explore directly. There are just a few steps to follow:</p>

<ol>
  <li>Upload image of a NFL game</li>
  <li>Select at least four points in the image and the pitch template. Be sure to select the same points in both images.</li>
  <li>Click on the ‚ÄúCompute homography‚Äù button to see the warped pitch template overlaid on the uploaded image.</li>
</ol>

<p>Make sure to read the following sections for a deeper understanding of how it all works.</p>

<link rel="stylesheet" type="text/css" href="/libs/custom/homography.css" />

<div class="app-container">
    <div>
        <div id="button-container" class="button-container">
            <div>
                <input type="file" id="imageLoader" name="imageLoader" />
            </div>
            <div>
                <button id="removeLastPoint">Remove Last Point</button>
                <button id="computeHomography">Compute Homography</button>
            </div>
        </div>
        <div id="template-container" class="container">
            <div id="templateContainer" class="templateContainer">
                <canvas id="templateCanvas" class="templateCanvas"></canvas>
                <canvas id="pointsTemplateCanvas" class="pointsTemplateCanvas"></canvas>
            </div>
            <div id="canvasContainer" class="canvasContainer">
                <canvas id="imageCanvas" class="imageCanvas"></canvas>
                <canvas id="pointsImageCanvas" class="pointsImageCanvas"></canvas>
            </div>
        </div>
    </div>
    <script src="https://docs.opencv.org/master/opencv.js" type="text/javascript"></script>
    <script type="text/javascript" src="/libs/custom/homography.js"></script>
</div>

<p>You can find all javascript code in <a href="https://github.com/mmeendez8/mmeendez8.github.io/blob/main/libs/custom/homography.js" target="_blank" rel="noopener noreferrer">this file</a>.</p>

<h2 id="some-extra-details">Some extra details</h2>

<h3 id="pitch-template">Pitch template</h3>

<p>The pitch template is a controlled image I use to model the real-world pitch. By mapping image objects to it, I can directly measure their distances and angles. This is crucial for extracting meaningful information later on, such as ball position, player location, and player speed.</p>

<p>First, I must understand how the dimensions of an American football pitch are defined. <a href="https://turftank.com/us/academy/how-big-is-a-football-field/" target="_blank" rel="noopener noreferrer">This</a> page is an excellent resource. It‚Äôs worth noting that I solely focused on NFL dimensions, as NCAA fields differ slightly.</p>

<div class="post-center-image">
    <a href="/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/pitchdims.png">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/pitchdims-800-d46bc69a9.png" alt="Diagram showing NFL pitch dimensions including length and width in yards." srcset="/generated/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/pitchdims-400-fe42e0bc7.webp 400w, /generated/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/pitchdims-600-fe42e0bc7.webp 600w, /generated/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/pitchdims-800-fe42e0bc7.webp 800w, /generated/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/pitchdims-1000-fe42e0bc7.webp 1000w" sizes="(max-width: 767px) 100vw, 80vw" width="1024" height="527" />
</a>

</div>

<p><em>NFL Pitch dimensions obtained from <a href="https://turftank.com/us/academy/how-big-is-a-football-field/" target="_blank" rel="noopener noreferrer">https://turftank.com/us/academy/how-big-is-a-football-field/</a></em></p>

<p>I created a simple image with the same resolution as the NFL pitch, <em>120 x 53.3 px</em>. This means one pixel in the image equals one yard in the real world. Next, I added end zones, hash marks, yard numbers, and all the necessary elements, each positioned accurately. I have to admit that although this task should be relatively simple and mechanical, it took me a while to achieve a decent result. Be sure to check the real code, but see below a small example that can help you realize the amount of handcrafting needed to create a good template:</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1">// Define the pitch dimensions and other constants</span>
    <span class="kd">const</span> <span class="nx">pitchHeight</span> <span class="o">=</span> <span class="mi">53</span> <span class="o">+</span> <span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">;</span>
    <span class="kd">const</span> <span class="nx">pitchWidth</span> <span class="o">=</span> <span class="mi">120</span><span class="p">;</span>
    <span class="kd">const</span> <span class="nx">lineWidth</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span> 
    <span class="kd">const</span> <span class="nx">ctxTemplate</span> <span class="o">=</span> <span class="nx">templateCanvas</span><span class="p">.</span><span class="nf">getContext</span><span class="p">(</span><span class="dl">'</span><span class="s1">2d</span><span class="dl">'</span><span class="p">);</span>

    <span class="c1">// Set size and color of template canvas</span>
    <span class="nx">ctxTemplate</span><span class="p">.</span><span class="nx">strokeStyle</span> <span class="o">=</span> <span class="dl">'</span><span class="s1">red</span><span class="dl">'</span><span class="p">;</span>
    <span class="nx">ctxTemplate</span><span class="p">.</span><span class="nx">fillStyle</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">black</span><span class="dl">"</span><span class="p">;</span>
    <span class="nx">ctxTemplate</span><span class="p">.</span><span class="nf">fillRect</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nx">templateCanvas</span><span class="p">.</span><span class="nx">width</span><span class="p">,</span> <span class="nx">templateCanvas</span><span class="p">.</span><span class="nx">height</span><span class="p">);</span>
    
    <span class="c1">// Translate to avoid cropping the sidelines</span>
    <span class="nx">ctxTemplate</span><span class="p">.</span><span class="nx">lineWidth</span> <span class="o">=</span> <span class="nx">lineWidth</span><span class="p">;</span>
    <span class="nx">ctxTemplate</span><span class="p">.</span><span class="nf">translate</span><span class="p">(</span><span class="nx">lineWidth</span><span class="p">,</span> <span class="nx">lineWidth</span><span class="p">);</span>

    <span class="c1">// Side lines</span>
    <span class="nf">addLine</span><span class="p">({</span> <span class="na">x</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="na">y</span><span class="p">:</span> <span class="mi">0</span> <span class="p">},</span> <span class="p">{</span> <span class="na">x</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="na">y</span><span class="p">:</span> <span class="nx">pitchHeight</span> <span class="p">});</span>
    <span class="nf">addLine</span><span class="p">({</span> <span class="na">x</span><span class="p">:</span> <span class="nx">pitchWidth</span><span class="p">,</span> <span class="na">y</span><span class="p">:</span> <span class="mi">0</span> <span class="p">},</span> <span class="p">{</span> <span class="na">x</span><span class="p">:</span> <span class="nx">pitchWidth</span><span class="p">,</span> <span class="na">y</span><span class="p">:</span> <span class="nx">pitchHeight</span> <span class="p">});</span>

    <span class="c1">// End lines</span>
    <span class="nf">addLine</span><span class="p">({</span> <span class="na">x</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="na">y</span><span class="p">:</span> <span class="mi">0</span> <span class="p">},</span> <span class="p">{</span> <span class="na">x</span><span class="p">:</span> <span class="nx">pitchWidth</span><span class="p">,</span> <span class="na">y</span><span class="p">:</span> <span class="mi">0</span> <span class="p">});</span>
    <span class="nf">addLine</span><span class="p">({</span> <span class="na">x</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="na">y</span><span class="p">:</span> <span class="nx">pitchHeight</span> <span class="p">},</span> <span class="p">{</span> <span class="na">x</span><span class="p">:</span> <span class="nx">pitchWidth</span><span class="p">,</span> <span class="na">y</span><span class="p">:</span> <span class="nx">pitchHeight</span> <span class="p">});</span>
</code></pre></div></div>

<p>I repeated this process for all pitch elements, resulting in the image below:</p>

<div class="post-center-image">
    <a href="/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/pitchtemplate.png">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/pitchtemplate-800-cf747f767.png" alt="NFL resulting pitch template image" srcset="/generated/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/pitchtemplate-400-fb2e1d535.webp 400w, /generated/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/pitchtemplate-600-fb2e1d535.webp 600w, /generated/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/pitchtemplate-800-fb2e1d535.webp 800w, /generated/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/pitchtemplate-1000-fb2e1d535.webp 1000w" sizes="(max-width: 767px) 100vw, 80vw" width="1645" height="824" />
</a>

</div>

<p class="image-caption"><em>NFL resulting pitch template image</em></p>

<p>Bear in mind though that you are observing a scaled-up version of a <em>120 x 53.3 px</em> image in the web app, adjusted to match your screen size.</p>

<h3 id="recovering-the-homography">Recovering the homography</h3>

<p>Homography maps the pitch template to the uploaded image, allowing for corresponding points between the two. The theory behind this is extensive and beyond this post‚Äôs scope. To be fair, I can‚Äôt do better than what my colleague I√±aki Rabanillo has done in his own blog. So, I will just refer you to his <a href="https://inakiraba91.github.io/projective-geometry-estimating-the-homography-matrix.html" target="_blank" rel="noopener noreferrer">post</a>, be sure to check it out since it is a brilliant piece of work.</p>

<p>To sum it up, the problem we need to solve consists mostly of finding a homography transformation that is represented by a 3x3 matrix. This will allow us to go from pixel coordinates $p_i$ in the image to real world coordinates $p_t$ in the template image. To do so, we just need to carry out a matrix multiplication:</p>

\[p_t = H \cdot p_i\]

<p>If you are now wondering how can you multiply a 3x3 matrix by a 2x1 vector, you are right. We need to add a 1 to the vector to make it a 3x1 vector. This is a common trick in computer vision and it is called homogeneous coordinates. It is a way to represent points in a way that makes it easier to perform transformations on them. Be sure to check <a href="https://inakiraba91.github.io/projective-geometry-building-the-homography-matrix-from-scratch.html" target="_blank" rel="noopener noreferrer">I√±aki‚Äôs post</a> for a deeper understanding of this.</p>

<p>The homography matrix $H$ is a 3x3 matrix that has 8 degrees of freedom. This means that we need at least 4 points in the real image and their corresponding pairs in the template image to solve for the homography. This is a simple problem to solve and there are a lot of libraries that can do it for you. I used OpenCV library although you could just create the system of equations and solve it using any linear algebra library (specially if you are used to work with Javascript more than me).</p>

<p>This is the code that retrieves the homography matrix from the list of points and applies it to the template image:</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="kd">function</span> <span class="nf">computeHomography</span><span class="p">()</span> <span class="p">{</span>
        <span class="c1">// Check that both lists of points have the same size and that their size is at least 4</span>
        <span class="k">if </span><span class="p">(</span><span class="nx">pointsImage</span><span class="p">.</span><span class="nx">length</span> <span class="o">!==</span> <span class="nx">pointsTemplate</span><span class="p">.</span><span class="nx">length</span> <span class="o">||</span> <span class="nx">pointsImage</span><span class="p">.</span><span class="nx">length</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">)</span> <span class="p">{</span>
            <span class="nf">alert</span><span class="p">(</span><span class="dl">'</span><span class="s1">Both lists of points must have the same size and contain at least 4 points</span><span class="dl">'</span><span class="p">);</span>
            <span class="k">return</span><span class="p">;</span>
        <span class="p">}</span>

        <span class="c1">// Convert points to cv.Mat format</span>
        <span class="kd">let</span> <span class="nx">imagePoints</span> <span class="o">=</span> <span class="nx">cv</span><span class="p">.</span><span class="nf">matFromArray</span><span class="p">(</span><span class="nx">pointsImage</span><span class="p">.</span><span class="nx">length</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nx">cv</span><span class="p">.</span><span class="nx">CV_32FC2</span><span class="p">,</span> <span class="nx">pointsImage</span><span class="p">.</span><span class="nf">flatMap</span><span class="p">(</span><span class="nx">point</span> <span class="o">=&gt;</span> <span class="p">[</span><span class="nb">Math</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="nx">point</span><span class="p">.</span><span class="nx">x</span><span class="p">),</span> <span class="nb">Math</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="nx">point</span><span class="p">.</span><span class="nx">y</span><span class="p">)]));</span>
        <span class="kd">let</span> <span class="nx">templatePoints</span> <span class="o">=</span> <span class="nx">cv</span><span class="p">.</span><span class="nf">matFromArray</span><span class="p">(</span><span class="nx">pointsTemplate</span><span class="p">.</span><span class="nx">length</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nx">cv</span><span class="p">.</span><span class="nx">CV_32FC2</span><span class="p">,</span> <span class="nx">pointsTemplate</span><span class="p">.</span><span class="nf">flatMap</span><span class="p">(</span><span class="nx">point</span> <span class="o">=&gt;</span> <span class="p">[</span><span class="nb">Math</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="nx">point</span><span class="p">.</span><span class="nx">x</span><span class="p">),</span> <span class="nb">Math</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="nx">point</span><span class="p">.</span><span class="nx">y</span><span class="p">)]));</span>

        <span class="c1">// Compute homography</span>
        <span class="kd">let</span> <span class="nx">homography</span> <span class="o">=</span> <span class="nx">cv</span><span class="p">.</span><span class="nf">findHomography</span><span class="p">(</span><span class="nx">templatePoints</span><span class="p">,</span> <span class="nx">imagePoints</span><span class="p">);</span>

        <span class="c1">// Check if homography is none because of colinear points</span>
        <span class="k">if </span><span class="p">(</span><span class="nx">homography</span><span class="p">.</span><span class="nf">empty</span><span class="p">())</span>
        <span class="p">{</span>
            <span class="nf">alert</span><span class="p">(</span><span class="dl">"</span><span class="s2">Could not found any homography for these sets of points. Be sure they are not colinear.</span><span class="dl">"</span><span class="p">);</span>
            <span class="k">return</span><span class="p">;</span>
        <span class="p">}</span>

        <span class="c1">// Warp the template image using the homography</span>
        <span class="kd">let</span> <span class="nx">warpedTemplate</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">cv</span><span class="p">.</span><span class="nc">Mat</span><span class="p">();</span>
        <span class="nx">cv</span><span class="p">.</span><span class="nf">warpPerspective</span><span class="p">(</span><span class="nx">templateImage</span><span class="p">,</span> <span class="nx">warpedTemplate</span><span class="p">,</span> <span class="nx">homography</span><span class="p">,</span> <span class="nx">sourceImage</span><span class="p">.</span><span class="nf">size</span><span class="p">());</span>

        <span class="c1">// Add the warped template to the source image</span>
        <span class="kd">let</span> <span class="nx">resultWeighted</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">cv</span><span class="p">.</span><span class="nc">Mat</span><span class="p">();</span>
        <span class="nx">cv</span><span class="p">.</span><span class="nf">addWeighted</span><span class="p">(</span><span class="nx">sourceImage</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nx">warpedTemplate</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nx">resultWeighted</span><span class="p">);</span>
        <span class="nx">cv</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="dl">'</span><span class="s1">imageCanvas</span><span class="dl">'</span><span class="p">,</span> <span class="nx">resultWeighted</span><span class="p">);</span>

        <span class="c1">// Clean up memory</span>
        <span class="nx">imagePoints</span><span class="p">.</span><span class="k">delete</span><span class="p">();</span>
        <span class="nx">templatePoints</span><span class="p">.</span><span class="k">delete</span><span class="p">();</span>
        <span class="nx">warpedTemplate</span><span class="p">.</span><span class="k">delete</span><span class="p">();</span>

        <span class="k">return</span> <span class="nx">homography</span><span class="p">;</span>
    <span class="p">}</span>
</code></pre></div></div>

<h3 id="projecting-between-image-and-template">Projecting between image and template</h3>

<p>Once we have the homography matrix, we can project any point in the image to the template (and viceversa). This just requires us to do a simple matrix multiplication. As it was mentioned before:</p>

\[p_t = H \cdot p_i\]

<p>It is also possible to do the opposite, projecting a point in the template to the image:</p>

\[p_i = H^{-1} \cdot p_t\]

<p>Quite simple, right?</p>

<h2 id="conclusion">Conclusion</h2>

<p>I hope you enjoyed this post and the web app. I have been using python for a long time and seeing things coming to life in the browser feels like something new now. I also had time to revisit some of the theory behind the homography and I am always happy to do so. I hope you found it interesting and that you can use it as a starting point for your own projects. I am sure there are a lot of improvements that can be done to this code and I would love to hear your thoughts on it.</p>

<p><em>Any ideas for future posts or is there something you would like to comment? Please feel free to reach out via <a href="https://twitter.com/mmeendez8" target="_blank" rel="noopener noreferrer">Twitter</a> or <a href="https://github.com/mmeendez8" target="_blank" rel="noopener noreferrer">Github</a></em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Dive into the world of sports analytics with a hands-on guide to mapping NFL fields to real-world coordinates using homography. Discover how computer vision techniques can transform video feed data into actionable insights, and explore the development of a web app for image calibration with the help of Copilot. Whether you're a sports enthusiast or a developer curious about the intersection of technology and sports, join to this journey through the fascinating process of sports field registration.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mmeendez8.github.io/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/thumbnail.jpg" /><media:content medium="image" url="https://mmeendez8.github.io/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/thumbnail.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Deploying MLFlow in AWS with Terraform</title><link href="https://mmeendez8.github.io/2023/11/22/deploy-mlflow-terraform.html" rel="alternate" type="text/html" title="Deploying MLFlow in AWS with Terraform" /><published>2023-11-22T00:00:00+00:00</published><updated>2023-11-22T00:00:00+00:00</updated><id>https://mmeendez8.github.io/2023/11/22/deploy-mlflow-terraform</id><content type="html" xml:base="https://mmeendez8.github.io/2023/11/22/deploy-mlflow-terraform.html"><![CDATA[<p>Training and deploying machine learning models is a complex process. There are lots of steps involved ‚Äì think data prep, model training, evaluating how good your model is, and then deploying it. Especially when it comes to training and evaluation, it‚Äôs super important to have a tool that makes life easier, something that lets us compare different experiments and track their performance (like losses, hyperparameters, metrics, etc.).</p>

<p>There are a bunch of tools out there for this, but we‚Äôre going to focus on one: <a href="https://mlflow.org/" target="_blank" rel="noopener noreferrer">MLFlow</a>. The goal of this post is to learn how to set up MLFlow in AWS, and we‚Äôre going to use Terraform for that. <a href="https://www.terraform.io/" target="_blank" rel="noopener noreferrer">Terraform</a> is awesome because (disclaimer: I hate its syntax) it lets you define all your infrastructure as code so you will never ever forget what button did you click to set up that AWS instance.</p>

<h2 id="why-mlflow">Why MLFlow?</h2>

<p>MLFlow is an open-source platform designed for managing the entire machine learning lifecycle. It‚Äôs great for tracking experiments, managing models, and even deploying them into production. Personally, I have primarily used MLFlow for tracking experiments. I became quite familiar with it in my previous job and really appreciated its features, especially the ability to create custom visualizations and share experiment links with colleagues. These links can also be embedded in reports, which is super handy.</p>

<p>I really missed using it and was eager to set it up in my current job. But, as often happens, more pressing tasks always seemed to take priority. To be fair, for basic tracking needs, you can get by with TensorBoard. It‚Äôs a straightforward tool that provides all the essentials. However, it starts to feel a bit overwhelming when you‚Äôre juggling a large number of experiments. Also, I did not have much experience with Terraform, just added a few lines here and there to existing configurations to set up some permissions and such. So, I decided to kill two birds with one stone and get into this project.</p>

<h2 id="what-do-we-need">What do we need?</h2>

<p>Before we dive in, let‚Äôs make sure we have got everything we need. We are going to use Terraform for setting up our infrastructure, so you‚Äôll need that installed. This post assumes you‚Äôve got Terraform ready and configured for your AWS account, so we‚Äôll skip that part and focus on what we need to add to our Terraform configuration. If you haven‚Äôt set up Terraform yet, no problem ‚Äì just follow <a href="https://learn.hashicorp.com/tutorials/terraform/aws-build?in=terraform/aws-get-started" target="_blank" rel="noopener noreferrer">this guide</a> for the setup. Also, I have deployed the MLFlow server in AWS EKS using <a href="https://fluxcd.io/" target="_blank" rel="noopener noreferrer">Flux</a>, a great tool for deploying services in Kubernetes. It‚Äôs not essential for this tutorial, though. Feel free to deploy the MLFlow server manually or with any other tool that you‚Äôre comfortable with.</p>

<p>Now, the MLFlow tracking server needs a few key components:</p>

<ul>
  <li>A <strong>database</strong> to store all your data</li>
  <li>A <strong>storage bucket</strong> for artifacts, like model checkpoints.</li>
  <li>A <strong>server</strong> to run the tracking server, which is essentially a Docker image with MLFlow installed.</li>
</ul>

<p>Let‚Äôs take it step by step and see how we can set up each of these components.</p>

<h2 id="database">Database</h2>

<p>MLFlow supports a bunch of different databases, but we are going to use Postgres ‚Äì it‚Äôs popular, and I‚Äôm quite familiar with it.We will use AWS RDS to set up our database and its security group. Here‚Äôs what to add to your Terraform configuration:</p>

<p>First, we create a random password without special characters:</p>

<div class="language-hcl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Creates a random password for the database</span>
<span class="nx">resource</span> <span class="s2">"random_password"</span> <span class="s2">"mlflow_db"</span> <span class="p">{</span>
  <span class="nx">length</span>  <span class="p">=</span> <span class="mi">16</span>
  <span class="nx">special</span> <span class="p">=</span> <span class="kc">false</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Next, we set up a security group for the database that allows TCP traffic on port 5432 (typical for PostgreSQL databases). We are limiting incoming traffic to only our VPN. If you are not looking to restrict traffic, you can skip this step.</p>

<div class="language-hcl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This specifies the security group for the database</span>
<span class="nx">module</span> <span class="s2">"mlflow_db_security_group"</span> <span class="p">{</span>
  <span class="nx">source</span>  <span class="p">=</span> <span class="s2">"terraform-aws-modules/security-group/aws"</span>
  <span class="nx">version</span> <span class="p">=</span> <span class="s2">"~&gt; 4"</span>

  <span class="nx">name</span>        <span class="p">=</span> <span class="s2">"mlflow-db-sg"</span>
  <span class="nx">description</span> <span class="p">=</span> <span class="s2">"RDS Aurora ingress security group"</span>
  <span class="nx">vpc_id</span>      <span class="p">=</span> <span class="nx">aws_vpc</span><span class="p">.</span><span class="nx">main</span><span class="p">.</span><span class="nx">id</span> <span class="c1"># You need to use you own vpc id here</span>

  <span class="nx">ingress_with_cidr_blocks</span> <span class="p">=</span> <span class="p">[</span>
    <span class="p">{</span>
      <span class="nx">from_port</span>   <span class="p">=</span> <span class="mi">5432</span>
      <span class="nx">to_port</span>     <span class="p">=</span> <span class="mi">5432</span>
      <span class="nx">protocol</span>    <span class="p">=</span> <span class="s2">"tcp"</span>
      <span class="nx">description</span> <span class="p">=</span> <span class="s2">"RDS Aurora access from within VPC"</span>
      <span class="nx">cidr_blocks</span> <span class="p">=</span> <span class="p">[</span><span class="nx">var</span><span class="p">.</span><span class="nx">your_vpn</span><span class="p">]</span>
    <span class="p">},</span>
  <span class="p">]</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Finally, we create a RDS Aurora cluster for the MLFlow database. The setup is straightforward, but you can always refer to the <a href="https://registry.terraform.io/modules/terraform-aws-modules/rds-aurora/aws/latest" target="_blank" rel="noopener noreferrer">official documentation</a> for more details. Notice how we use the earlier created password and security group to restrict access. We are creating a single writer instance, assuming limited traffic. If you expect more, consider adding a read replica in the instances map.</p>

<div class="language-hcl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Database configuration</span>
<span class="nx">module</span> <span class="s2">"mlflow_cluster_db"</span> <span class="p">{</span>
  <span class="nx">source</span>         <span class="p">=</span> <span class="s2">"terraform-aws-modules/rds-aurora/aws"</span>
  <span class="nx">version</span>        <span class="p">=</span> <span class="s2">"6.2.0"</span>

  <span class="nx">name</span>           <span class="p">=</span> <span class="s2">"mlflow-db"</span>
  <span class="nx">engine</span>         <span class="p">=</span> <span class="s2">"aurora-postgresql"</span>
  <span class="nx">engine_version</span> <span class="p">=</span> <span class="s2">"14.5"</span>
  <span class="nx">instance_class</span> <span class="p">=</span> <span class="s2">"db.r5.large"</span>
  <span class="nx">instances</span> <span class="p">=</span> <span class="p">{</span>
    <span class="nx">one</span> <span class="p">=</span> <span class="p">{}</span>
  <span class="p">}</span>

  <span class="nx">database_name</span>          <span class="p">=</span> <span class="s2">"mlflow"</span>
  <span class="nx">master_username</span>        <span class="p">=</span> <span class="s2">"mlflow"</span>
  <span class="nx">create_random_password</span> <span class="p">=</span> <span class="kc">false</span>
  <span class="nx">master_password</span>        <span class="p">=</span> <span class="nx">random_password</span><span class="p">.</span><span class="nx">mlflow_db</span><span class="p">.</span><span class="nx">result</span>

  <span class="nx">create_security_group</span>  <span class="p">=</span> <span class="kc">false</span>
  <span class="nx">subnets</span>                <span class="p">=</span> <span class="nx">local</span><span class="p">.</span><span class="nx">subnets_ids_database</span>
  <span class="nx">vpc_security_group_ids</span> <span class="p">=</span> <span class="p">[</span><span class="nx">module</span><span class="p">.</span><span class="nx">mlflow_db_security_group</span><span class="p">.</span><span class="nx">security_group_id</span><span class="p">]</span>

  <span class="nx">storage_encrypted</span>   <span class="p">=</span> <span class="kc">true</span>
  <span class="nx">apply_immediately</span>   <span class="p">=</span> <span class="kc">true</span>
  <span class="nx">monitoring_interval</span> <span class="p">=</span> <span class="mi">10</span>

  <span class="nx">enabled_cloudwatch_logs_exports</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"postgresql"</span><span class="p">]</span>

  <span class="nx">tags</span> <span class="p">=</span> <span class="p">{</span>
    <span class="nx">Environment</span> <span class="p">=</span> <span class="s2">"dev"</span>
    <span class="nx">Terraform</span>   <span class="p">=</span> <span class="s2">"true"</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>That is about it for the database setup. Lastly, we need to store the DB user, password, and endpoint for later connection. I will use AWS Secrets Manager for this. How you manage secrets may vary; the only thing you really want to avoid is to store them in source code. You could also use AWS Parameter Store or Hashicorp Vault, for example. Here‚Äôs how to store the secrets in AWS Secrets Manager:</p>

<div class="language-hcl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">resource</span> <span class="s2">"aws_secretsmanager_secret"</span> <span class="s2">"mlflow_db_master_username_id"</span> <span class="p">{</span>
  <span class="nx">name</span>  <span class="p">=</span> <span class="s2">"mlflow-username"</span>
<span class="p">}</span>

<span class="nx">resource</span> <span class="s2">"aws_secretsmanager_secret_version"</span> <span class="s2">"mlflow_db_master_username"</span> <span class="p">{</span>
  <span class="nx">secret_id</span>     <span class="p">=</span> <span class="nx">aws_secretsmanager_secret</span><span class="p">.</span><span class="nx">mlflow_db_master_username_id</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nx">id</span>
  <span class="nx">secret_string</span> <span class="p">=</span> <span class="nx">module</span><span class="p">.</span><span class="nx">mlflow_cluster_db</span><span class="p">.</span><span class="nx">cluster_master_username</span>
<span class="p">}</span>

<span class="nx">resource</span> <span class="s2">"aws_secretsmanager_secret"</span> <span class="s2">"mlflow_db_master_password_id"</span> <span class="p">{</span>
  <span class="nx">name</span>  <span class="p">=</span> <span class="s2">"mlflow-password"</span>
<span class="p">}</span>

<span class="nx">resource</span> <span class="s2">"aws_secretsmanager_secret_version"</span> <span class="s2">"mlflow_db_master_password"</span> <span class="p">{</span>
  <span class="nx">secret_id</span>     <span class="p">=</span> <span class="nx">aws_secretsmanager_secret</span><span class="p">.</span><span class="nx">mlflow_db_master_password_id</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nx">id</span>
  <span class="nx">secret_string</span> <span class="p">=</span> <span class="nx">module</span><span class="p">.</span><span class="nx">mlflow_cluster_db</span><span class="p">.</span><span class="nx">cluster_master_password</span>
<span class="p">}</span>

<span class="nx">resource</span> <span class="s2">"aws_secretsmanager_secret"</span> <span class="s2">"mlflow_db_endpoint_id"</span> <span class="p">{</span>
  <span class="nx">name</span>  <span class="p">=</span> <span class="s2">"mlflow-db-writer-endpoint"</span>
<span class="p">}</span>

<span class="nx">resource</span> <span class="s2">"aws_secretsmanager_secret_version"</span> <span class="s2">"mlflow_db_endpoint"</span> <span class="p">{</span>
  <span class="nx">secret_id</span>     <span class="p">=</span> <span class="nx">aws_secretsmanager_secret</span><span class="p">.</span><span class="nx">mlflow_db_endpoint_id</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nx">id</span>
  <span class="nx">secret_string</span> <span class="p">=</span> <span class="nx">module</span><span class="p">.</span><span class="nx">mlflow_cluster_db</span><span class="p">.</span><span class="nx">cluster_endpoint</span>
<span class="p">}</span>
</code></pre></div></div>

<h2 id="storage-bucket-and-iam-role">Storage bucket and IAM Role</h2>

<p>Next up is setting up a storage bucket for all the artifacts, and we‚Äôre going to use AWS S3 for this. Here‚Äôs the Terraform configuration needed:</p>

<div class="language-hcl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">resource</span> <span class="s2">"aws_s3_bucket"</span> <span class="s2">"mlflow_artifacts_bucket"</span> <span class="p">{</span>
  <span class="nx">bucket</span> <span class="p">=</span> <span class="s2">"mlflow-artifacts-bucket"</span>
<span class="p">}</span>

<span class="nx">resource</span> <span class="s2">"aws_s3_bucket_ownership_controls"</span> <span class="s2">"mlflow_bucket_ownership"</span> <span class="p">{</span>
  <span class="nx">bucket</span> <span class="p">=</span> <span class="nx">aws_s3_bucket</span><span class="p">.</span><span class="nx">mlflow_artifacts_bucket</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nx">id</span>
  <span class="nx">rule</span> <span class="p">{</span>
    <span class="nx">object_ownership</span> <span class="p">=</span> <span class="s2">"BucketOwnerPreferred"</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="nx">resource</span> <span class="s2">"aws_s3_bucket_acl"</span> <span class="s2">"mlflow_bucket_acl"</span> <span class="p">{</span>
  <span class="nx">depends_on</span> <span class="p">=</span> <span class="p">[</span><span class="nx">aws_s3_bucket_ownership_controls</span><span class="p">.</span><span class="nx">mlflow_bucket_ownership</span><span class="p">]</span>

  <span class="nx">bucket</span> <span class="p">=</span> <span class="nx">aws_s3_bucket</span><span class="p">.</span><span class="nx">mlflow_artifacts_bucket</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nx">id</span>
  <span class="nx">acl</span>    <span class="p">=</span> <span class="s2">"private"</span>
<span class="p">}</span>
</code></pre></div></div>

<p>This configuration creates an S3 bucket named ‚Äòmlflow-artifacts-bucket‚Äô. It also sets up ownership controls, ensuring new objects uploaded without an ACL are owned by the bucket owner. Additionally, it enforces a private ACL for the bucket, securing the stored data.</p>

<p>Now, MLFlow needs to access this bucket from EKS, so we‚Äôll create an IAM Role for Service Accounts. Here‚Äôs how to do it:</p>

<div class="language-hcl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">resource</span> <span class="s2">"aws_iam_role"</span> <span class="s2">"mlflow-role"</span> <span class="p">{</span>
  <span class="nx">name</span> <span class="p">=</span> <span class="s2">"mlflow-role"</span>

  <span class="nx">assume_role_policy</span> <span class="p">=</span> <span class="nx">jsonencode</span><span class="p">({</span>
    <span class="nx">Version</span> <span class="p">=</span> <span class="s2">"2012-10-17"</span><span class="p">,</span>
    <span class="nx">Statement</span> <span class="p">=</span> <span class="p">[</span>
      <span class="p">{</span>
        <span class="nx">Action</span> <span class="p">=</span> <span class="s2">"sts:AssumeRole"</span><span class="p">,</span>
        <span class="nx">Effect</span> <span class="p">=</span> <span class="s2">"Allow"</span><span class="p">,</span>
        <span class="nx">Principal</span> <span class="p">=</span> <span class="p">{</span>
          <span class="nx">Service</span> <span class="p">=</span> <span class="s2">"ec2.amazonaws.com"</span> 
        <span class="p">},</span>
      <span class="p">},</span>
    <span class="p">],</span>
  <span class="p">})</span>
<span class="p">}</span>

<span class="c1"># Custom Policy for Specific S3 Bucket Access</span>
<span class="nx">resource</span> <span class="s2">"aws_iam_policy"</span> <span class="s2">"mlflow_s3_policy"</span> <span class="p">{</span>
  <span class="nx">name</span>   <span class="p">=</span> <span class="s2">"mlflow_s3_policy"</span>
  <span class="nx">policy</span> <span class="p">=</span> <span class="nx">jsonencode</span><span class="p">({</span>
    <span class="nx">Version</span> <span class="p">=</span> <span class="s2">"2012-10-17"</span><span class="p">,</span>
    <span class="nx">Statement</span> <span class="p">=</span> <span class="p">[</span>
      <span class="p">{</span>
        <span class="nx">Action</span> <span class="p">=</span> <span class="p">[</span>
          <span class="s2">"s3:GetObject"</span><span class="p">,</span>
          <span class="s2">"s3:GetObject*"</span><span class="p">,</span>
          <span class="s2">"s3:ListBucket"</span><span class="p">,</span>
          <span class="s2">"s3:PutObject"</span><span class="p">,</span>
          <span class="s2">"s3:DeleteObject"</span>
        <span class="p">],</span>
        <span class="nx">Effect</span>   <span class="p">=</span> <span class="s2">"Allow"</span><span class="p">,</span>
        <span class="nx">Resource</span> <span class="p">=</span> <span class="p">[</span>
          <span class="s2">"arn:aws:s3:::mlflow-artifacts-bucket"</span><span class="p">,</span>
          <span class="s2">"arn:aws:s3:::mlflow-artifacts-bucket/*"</span>
        <span class="p">]</span>
      <span class="p">}</span>
    <span class="p">]</span>
  <span class="p">})</span>
<span class="p">}</span>

<span class="c1"># Attach Custom Policy to the Role</span>
<span class="nx">resource</span> <span class="s2">"aws_iam_role_policy_attachment"</span> <span class="s2">"s3_policy_attachment"</span> <span class="p">{</span>
  <span class="nx">role</span>       <span class="p">=</span> <span class="nx">aws_iam_role</span><span class="p">.</span><span class="nx">mlflow-role</span><span class="p">.</span><span class="nx">name</span>
  <span class="nx">policy_arn</span> <span class="p">=</span> <span class="nx">aws_iam_policy</span><span class="p">.</span><span class="nx">mlflow_s3_policy</span><span class="p">.</span><span class="nx">arn</span>
  <span class="nx">depends_on</span> <span class="p">=</span> <span class="p">[</span><span class="nx">aws_iam_policy</span><span class="p">.</span><span class="nx">mlflow_s3_policy</span><span class="p">]</span>
<span class="p">}</span>
</code></pre></div></div>

<p>This code sets up a new role, <code class="language-plaintext highlighter-rouge">mlflow-role</code>, and attaches a custom policy, <code class="language-plaintext highlighter-rouge">mlflow_s3_policy</code>, to it. This policy grants specific permissions to access the S3 bucket we created earlier.</p>

<h2 id="docker">Docker</h2>

<p>Before we proceed, it‚Äôs important to note that the official MLFlow Docker image doesn‚Äôt include the necessary libraries for connecting with AWS S3 and Postgres. To address this, we‚Äôll need to create a custom image that includes these libraries. This is done by crafting a Dockerfile like the one below:</p>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ghcr.io/mlflow/mlflow:v2.7.1</span>

<span class="k">RUN </span>apt-get <span class="nt">-y</span> update <span class="o">&amp;&amp;</span> <span class="se">\
</span>    apt-get <span class="nt">-y</span> <span class="nb">install </span>python3-dev build-essential pkg-config <span class="o">&amp;&amp;</span> <span class="se">\
</span>    pip <span class="nb">install</span> <span class="nt">--upgrade</span> pip <span class="o">&amp;&amp;</span> <span class="se">\
</span>    pip <span class="nb">install </span>psycopg2-binary boto3

<span class="k">CMD</span><span class="s"> ["bash"]</span>
</code></pre></div></div>

<p>This Dockerfile starts with the official MLFlow image and adds the required libraries. After crafting this file, you can build the image and push it to your ECR repository, or any other repository you prefer to use.</p>

<h2 id="deployment">Deployment</h2>

<p>Finally, we‚Äôll set up the server to run the MLflow tracking server on AWS EKS. We typically use Flux for deploying services in Kubernetes, so we‚Äôll create a <code class="language-plaintext highlighter-rouge">kustomization.yaml</code> file containing all the necessary resources. Alternatively, you could also deploy using Terraform, although I am less familiar with this method as we primarily utilize Flux for our deployments.</p>

<p>We will begin with defining secrets. Our secrets are stored in AWS Secrets Manager and accessed from Kubernetes. We use the <a href="https://github.com/external-secrets/kubernetes-external-secrets" target="_blank" rel="noopener noreferrer">External Secrets</a> package for simplicity. Here is what our <code class="language-plaintext highlighter-rouge">secrets.yaml</code> file looks like:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s2">"</span><span class="s">kubernetes-client.io/v1"</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ExternalSecret</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mlflow-secrets</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">monitoring</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">backendType</span><span class="pi">:</span> <span class="s">secretsManager</span>
  <span class="na">data</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span>  <span class="s">mlflow-username</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">AWS_SECRET_MLFLOW_USERNAME</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">mlflow-password</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">AWS_SECRET_MLFLOW_PASSWORD</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">mlflow-db-writer-endpoint</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">AWS_SECRET_MLFLOW_HOST</span>
</code></pre></div></div>

<p>Next, we need a service account for our deployment, defined in <code class="language-plaintext highlighter-rouge">service-account.yaml</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ServiceAccount</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mlflow-sa</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">eks.amazonaws.com/role-arn</span><span class="pi">:</span> <span class="s">arn:aws:iam::941819254007:role/mlflow-role</span>
</code></pre></div></div>

<p>This configuration uses the earlier created IAM role for accessing the S3 bucket.</p>

<p>We also need a service for our deployment, specified in <code class="language-plaintext highlighter-rouge">service.yaml</code> file:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mlflow</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">8080</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">http</span>
      <span class="na">targetPort</span><span class="pi">:</span> <span class="s">http</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app.kubernetes.io/name</span><span class="pi">:</span> <span class="s">mlflow</span>
</code></pre></div></div>

<p>This is a very simple service definition that just exposes port 8080 and targets all pods with the label <code class="language-plaintext highlighter-rouge">app.kubernetes.io/name: mlflow</code>.</p>

<p>Next, we need an ingress to expose our service to the outside world. We use AWS ALB Ingress Controller for this. Here‚Äôs what our <code class="language-plaintext highlighter-rouge">ingress.yaml</code> file looks like:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mlflow</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">kubernetes.io/ingress.class</span><span class="pi">:</span> <span class="s">alb</span>
    <span class="na">alb.ingress.kubernetes.io/scheme</span><span class="pi">:</span> <span class="s">internal</span>
    <span class="na">alb.ingress.kubernetes.io/target-type</span><span class="pi">:</span> <span class="s">ip</span>
    <span class="na">alb.ingress.kubernetes.io/ssl-redirect</span><span class="pi">:</span> <span class="s2">"</span><span class="s">443"</span>
    <span class="na">alb.ingress.kubernetes.io/listen-ports</span><span class="pi">:</span> <span class="s1">'</span><span class="s">[{"HTTPS":</span><span class="nv"> </span><span class="s">443}]'</span>
    <span class="na">alb.ingress.kubernetes.io/healthcheck-path</span><span class="pi">:</span> <span class="s">/</span>

<span class="na">spec</span><span class="pi">:</span>
  <span class="na">rules</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s">host-url</span>
    <span class="na">http</span><span class="pi">:</span>
      <span class="na">paths</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/</span>
          <span class="na">pathType</span><span class="pi">:</span> <span class="s">Prefix</span>
          <span class="na">backend</span><span class="pi">:</span>
            <span class="na">service</span><span class="pi">:</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">mlflow</span>
              <span class="na">port</span><span class="pi">:</span>
                <span class="na">name</span><span class="pi">:</span> <span class="s">http</span>
</code></pre></div></div>

<p>This Ingress configuration sets up access for our MLFlow tracking server. It is very simple, the configuration ensures SSL redirection to HTTPS on port 443 and that is is only accessible from within the VPC (internal). The rule specified routes traffic for the <code class="language-plaintext highlighter-rouge">host-url</code> to the MLflow service on the HTTP port.</p>

<p>Lastly, the deployment itself, outlined in <code class="language-plaintext highlighter-rouge">deployment.yaml</code> file:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mlflow</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">monitoring</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">mlflow</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">mlflow</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">serviceAccount</span><span class="pi">:</span> <span class="s">mlflow-sa</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">app</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">add-your-image-here</span>
        <span class="na">command</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s2">"</span><span class="s">mlflow"</span>
        <span class="pi">-</span> <span class="s2">"</span><span class="s">server"</span>
        <span class="pi">-</span> <span class="s2">"</span><span class="s">--host=0.0.0.0"</span>
        <span class="pi">-</span> <span class="s2">"</span><span class="s">--backend-store-uri=postgresql://$(AWS_SECRET_MLFLOW_USERNAME):$(AWS_SECRET_MLFLOW_PASSWORD)@$(AWS_SECRET_MLFLOW_HOST):5432/mlflow"</span>
        <span class="pi">-</span> <span class="s2">"</span><span class="s">--default-artifact-root=s3://mlflow-artifacts-bucket"</span>
        <span class="pi">-</span> <span class="s2">"</span><span class="s">--port=8080"</span>
        <span class="na">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">http</span>
          <span class="na">containerPort</span><span class="pi">:</span> <span class="m">8080</span>
        <span class="na">envFrom</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">secretRef</span><span class="pi">:</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">mlflow-secrets</span>
        <span class="na">readinessProbe</span><span class="pi">:</span>
          <span class="na">httpGet</span><span class="pi">:</span>
            <span class="na">path</span><span class="pi">:</span> <span class="s">/</span>
            <span class="na">port</span><span class="pi">:</span> <span class="m">8080</span>
        <span class="na">startupProbe</span><span class="pi">:</span>
          <span class="na">initialDelaySeconds</span><span class="pi">:</span> <span class="m">10</span>
          <span class="na">httpGet</span><span class="pi">:</span>
            <span class="na">path</span><span class="pi">:</span> <span class="s">/</span>
            <span class="na">port</span><span class="pi">:</span> <span class="m">8080</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1G</span>
            <span class="na">cpu</span><span class="pi">:</span> <span class="s">500m</span>
          <span class="na">limits</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1G</span>
            <span class="na">cpu</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1"</span>
</code></pre></div></div>

<p>Key takeaways from this setup:</p>

<ol>
  <li>We deploy a single replica of the MLFlow pod.</li>
  <li>Replace <code class="language-plaintext highlighter-rouge">add-your-image-here</code> with the image we created earlier.</li>
  <li>Secrets created earlier are passed as environment variables in the <code class="language-plaintext highlighter-rouge">envFrom</code> section.</li>
  <li>Entrypoint is set to <code class="language-plaintext highlighter-rouge">mlflow server</code> and we pass the required arguments to connect to the database and the S3 bucket.</li>
  <li>Readiness and startup probes ensure the pod is fully operational before receiving requests.</li>
</ol>

<p>And that‚Äôs it! With these steps, our MLFlow server is ready to go and can start tracking experiments.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this post we have seen how to set up MLFlow in AWS using Terraform. We have seen how to set up a Postgres database, an S3 bucket, and a Kubernetes deployment for the MLFlow server. We have also seen how to create a custom Docker image that includes the required libraries to connect to the database and the S3 bucket.</p>

<p><em>Any ideas for future posts or is there something you would like to comment? Please feel free to reach out via <a href="https://twitter.com/mmeendez8" target="_blank" rel="noopener noreferrer">Twitter</a> or <a href="https://github.com/mmeendez8" target="_blank" rel="noopener noreferrer">Github</a></em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Explore this step-by-step guide on deploying MLFlow in AWS using Terraform. Learn how to effectively manage your machine learning lifecycle, set up a Postgres database, create a secure S3 bucket, and customize a MLFlow Docker image. Improve you Machine Learning experiment tracking and model management in the cloud.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mmeendez8.github.io/assets/images/fullsize/posts/2023-11-22-deploy-mlflow-terraform/thumbnail.jpg" /><media:content medium="image" url="https://mmeendez8.github.io/assets/images/fullsize/posts/2023-11-22-deploy-mlflow-terraform/thumbnail.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Multi-Object Tracking by Detection: A Comprehensive Guide</title><link href="https://mmeendez8.github.io/2023/11/08/tracking-by-detection-overview.html" rel="alternate" type="text/html" title="Multi-Object Tracking by Detection: A Comprehensive Guide" /><published>2023-11-08T00:00:00+00:00</published><updated>2023-11-08T00:00:00+00:00</updated><id>https://mmeendez8.github.io/2023/11/08/tracking-by-detection-overview</id><content type="html" xml:base="https://mmeendez8.github.io/2023/11/08/tracking-by-detection-overview.html"><![CDATA[<h2 id="table-of-contents">Table of Contents</h2>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th>¬†</th>
        <th>Year</th>
        <th>Appearence Features</th>
        <th>Camera Compensation</th>
        <th>HOTA MOT20</th>
        <th>Extra data</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><a href="#sort">SORT</a></td>
        <td>2016</td>
        <td>‚ùå</td>
        <td>‚ùå</td>
        <td>¬†</td>
        <td>‚ùå</td>
      </tr>
      <tr>
        <td><a href="#deepsort">DeepSORT</a></td>
        <td>2017</td>
        <td>‚úÖ</td>
        <td>‚ùå</td>
        <td>¬†</td>
        <td>‚úÖ</td>
      </tr>
      <tr>
        <td><a href="#bytetrack">ByteTrack</a></td>
        <td>2021</td>
        <td>‚úÖ</td>
        <td>‚ùå</td>
        <td>61.3</td>
        <td>‚úÖ</td>
      </tr>
      <tr>
        <td><a href="#bot-sort">BoT-SORT</a></td>
        <td>2022</td>
        <td>‚úÖ</td>
        <td>‚úÖ</td>
        <td>63.3</td>
        <td>‚úÖ</td>
      </tr>
      <tr>
        <td><a href="#smiletrack">SMILEtrack</a></td>
        <td>2022</td>
        <td>‚úÖ</td>
        <td>‚úÖ  (?)</td>
        <td>63.4</td>
        <td>‚úÖ</td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="introduction">Introduction</h2>

<p>Tracking by detection is an object tracking approach that first detects objects in each frame of a video and then associates the detections across frames. This process involves matching detections by analyzing their location, appearance, or motion characteristics. Tracking by detection has become the most popular method for addressing object tracking due to the rapid development of reliable object detectors.</p>

<p>The intention of this blog is to keep myself updated with the bibliography of tracking by detection methods. My intention is to regularly update this blog with new information and resources I find interesting.
I have included the SORT and DeepSORT papers in the list, despite being older methods, as they laid the groundwork for many of the techniques covered here.</p>

<h2 id="sort">SORT</h2>

<p>It is a very good and simple work from 2016 that quickly became a standard in the field. The author‚Äôs main goal was to create the fastest possible tracker relying on the quality of the object detector predictions. Appearance features of the objects are not used; the system relies solely on bounding box position and size.</p>

<p>They employ two classical methods:</p>

<ul>
  <li>
    <p><strong>Kalman Filter:</strong> is in charge of handling motion prediction, this is, figuring out where a track is going to move in the next frame given previous states. Track states are modeled with six different variables:</p>

\[\mathbf{x} = [u,v,s,r,\dot{u},\dot{v},\dot{s}]^T,\]

    <p>These are the center of the target bounding box ($u, v$), the scales and aspect ratio of it ($s, r$) and their velocity components ($\dot{u},\dot{v},\dot{s}$).</p>
  </li>
  <li>
    <p><strong>Hungarian method:</strong> used in the data association step to match new predictions with tracks based on IoU metric.</p>
  </li>
</ul>

<div class="post-center-image">
    <a href="/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/sort.jpg">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/sort-800-ef8ea65f7.jpg" alt="SORT architecture diagram" srcset="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/sort-400-0129c6d0c.webp 400w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/sort-600-0129c6d0c.webp 600w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/sort-800-0129c6d0c.webp 800w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/sort-1000-0129c6d0c.webp 1000w" sizes="(max-width: 767px) 100vw, 80vw" width="1352" height="739" />
</a>

</div>

<p class="image-caption"><em>SORT architecture diagram</em></p>

<ol>
  <li>An object detector returns bounding boxes for frame 0.</li>
  <li>In T=0, a new track is created for each of the predicted bounding boxes</li>
  <li>KF will predict a new position for each of the tracks</li>
  <li>Object detector returns bounding boxes for frame 1</li>
  <li>These bounding boxes are associated with tracks positions predicted by KF</li>
  <li>New tracks are created for unmatched bounding boxes</li>
  <li>Unmatched tracks can be terminated if they are not matched to any detection for $T_{Lost}$ frames.</li>
  <li>Matched tracks and new tracks are passed to the next time step</li>
  <li>Back to 3</li>
</ol>

<h2 id="deepsort">DeepSORT</h2>

<p>DeepSORT is an extension of SORT that uses appearance features. It enhances SORT by adding a simple CNN extension that extracts appearance features from bounding boxes, improving object tracking, especially during occlusions. An object can be re-identified using appearance similarity after being occluded for a long period of time</p>

<p>Each track maintains a gallery of the last \(n\) appearance descriptors, enabling cosine distance calculations between new detections and descriptors. Track age, determined by frames since the last association, plays a crucial role in the association process. DeepSORT adopts a cascade approach, prioritizing tracks with lower ages over a single-step association between predicted Kalman states and new measurements.</p>

<div class="post-center-image">
    <a href="/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/deepsort.jpg">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/deepsort-800-58a403e1a.jpg" alt="DeepSORT architecture diagram" srcset="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/deepsort-400-d49ec61a3.webp 400w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/deepsort-600-d49ec61a3.webp 600w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/deepsort-800-d49ec61a3.webp 800w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/deepsort-1000-d49ec61a3.webp 1000w" sizes="(max-width: 767px) 100vw, 80vw" width="1378" height="725" />
</a>

</div>

<p class="image-caption"><em>DeepSORT architecture diagram</em></p>

<p>There is a small modification on the Kalman Filter prediction step that is included in the <a href="https://github.com/nwojke/deep_sort/blob/master/deep_sort/kalman_filter.py#L108" target="_blank" rel="noopener noreferrer">code</a> but not mentioned in the original paper. The matrices \(Q\), \(R\) of the Kalman Filter were chosen in SORT to be time indepent, however in DeepSORT it was suggested to choose \(Q%\), \(R\) as functions of the scale of the bounding box. This can be due to the scale is less likely to change over time than other features and it can be also be used to compensate for changes in camera‚Äôs viewpoint.</p>

<p>The cascade association step would look like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">track_age</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">maximum_age</span><span class="p">):</span>
    <span class="n">tracks_to_associate</span> <span class="o">=</span> <span class="nf">get_tracks_with_age</span><span class="p">(</span><span class="n">tracks</span><span class="p">,</span> <span class="n">track_age</span><span class="p">)</span>
    <span class="nf">associate</span><span class="p">(</span><span class="n">tracks_to_associate</span><span class="p">,</span> <span class="n">detections</span><span class="p">)</span>
    <span class="nf">remove_associated_detections</span><span class="p">(</span><span class="n">detections</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="bytetrack">ByteTrack</h2>

<p>ByteTrack is a recent object tracking algorithm that proposes a simple but effective optimization for the data association step. Most methods filter out detections with low confidence scores. This is because low-confidence detections are more likely to be false positives, or to correspond to objects that are not present in the scene. However, this can lead to problems when tracking objects that are partially occluded or that undergo significant appearance changes.</p>

<p>ByteTrack addresses this problem by using all detections, regardless of their confidence score. The algorithm works in two steps:</p>

<ol>
  <li><strong>High-confidence detections</strong>: High-confidence detections are associated with tracks using intersection-over-union (IoU) or appearance features. Both approaches are evaluated in the results section of the paper.</li>
  <li><strong>Low-confidence detections</strong>: Low-confidence detections are associated with tracks using only IoU. This is because low-confidence detections are more likely to be spurious or inaccurate, so it is important to be more conservative when associating them with tracks.</li>
</ol>

<div class="post-center-image">
    <a href="/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/bytetrack.jpg">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/bytetrack-800-eb704f5e3.jpg" alt="ByteTrack architecture diagram" srcset="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/bytetrack-400-1a0b7d0b3.webp 400w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/bytetrack-600-1a0b7d0b3.webp 600w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/bytetrack-800-1a0b7d0b3.webp 800w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/bytetrack-1000-1a0b7d0b3.webp 1000w" sizes="(max-width: 767px) 100vw, 80vw" width="1311" height="762" />
</a>

</div>

<p class="image-caption"><em>ByteTrack architecture diagram</em></p>

<p>The ByteTrack algorithm has been shown to be very effective and it is currently among the top-performing methods on the <a href="https://paperswithcode.com/sota/multi-object-tracking-on-mot20-1" target="_blank" rel="noopener noreferrer">MOT Challenge leaderboard</a>.</p>

<h2 id="bot-sort">BoT-SORT</h2>

<p>I personally love the BoT-SORT paper. It is build upon ByteTrack and it combines three different ideas that work very well together. These are:</p>

<ol>
  <li>
    <p><strong>Kalman Filter update</strong>: SORT introduced a way of modelling the track state vector using a seven-tuple \(\mathbf{x} = [x_c,y_c,a,h,\dot{x_c},\dot{y_c},\dot{s}]^T\). BoT-SORT proposes to replace the scale and aspect ratio of the bounding box  (\(s\), \(a\)) with the widht and height (\(w\), \(h\)) to create an eight-tuple:</p>

\[\mathbf{x} = [x_c,y_c,w,h,\dot{x_c},\dot{y_c},\dot{w}, \dot{h}]^T\]

    <p>They also choose Q, R matrices from the Kalman Filter as functions of the bounding box width and height. Recall that in DeepSORT, only the scale of the bounding box influenced on the Q, R matrices (see section 3.1 of <a href="https://arxiv.org/pdf/2206.14651v2.pdf" target="_blank" rel="noopener noreferrer">BoT-SORT paper</a> for more details).</p>
  </li>
  <li>
    <p><strong>Camera Motion Compensation</strong>: In dynamic camera situations, objects that are static can appear to move, and objects that are moving can appear to be static. The Kalman Filter does not take camera motion into account for its predictions, so BoT-SORT proposes to incorporate this knowledge. To do this, they use the global motion compensation technique (GMC) from the OpenCV Video Stabilization module. This technique extracts keypoints from consecutive frames and computes the homography matrix between the matching pairs. This matrix can then be used to transform the prediction bounding box from the coordinate system of frame \(k ‚àí 1\) to the coordinates of the next frame \(k\) (see section 3.2 of <a href="https://arxiv.org/pdf/2206.14651v2.pdf" target="_blank" rel="noopener noreferrer">BoT-SORT paper</a> to a full formulation on how incorporate the homography matrix in the prediction step).</p>

    <div class="post-center-image">
     <a href="/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/cmc.png">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/cmc-800-3538f10c9.png" alt="Camera movement example" srcset="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/cmc-400-ed5e82503.webp 400w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/cmc-600-ed5e82503.webp 600w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/cmc-800-ed5e82503.webp 800w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/cmc-1000-ed5e82503.webp 1000w" sizes="(max-width: 767px) 100vw, 80vw" width="1152" height="1296" />
</a>

 </div>

    <p class="image-caption"><em>Player is static on the pitch while throwing the ball but location on the image changes due to camera movement.</em></p>
  </li>
  <li>
    <ol>
      <li><strong>IoU - ReID Fusion</strong>:  BoT-SORT proposes a new way of solving the association step by combining motion and appearance information. The cost matrix elements are computed as follows:</li>
    </ol>

\[\hat{d}^{cos}_{i,j} = 
 \begin{equation}
 \begin{cases}
 0.5 \cdot {d}^{cos}_{i,j}, ({d}^{cos}_{i,j} &lt; \theta_{emb}) \hat{} ({d}^{iou}_{i,j} &lt; \theta_{iou})\\
 1, \text{otherwise}
 \end{cases}
 \end{equation}\]

\[C_{i,j} = min(d^{iou}_{i,j}, \hat{d}^{cos}_{i,j})\]

    <p>The appearence distance is recomputed as shown in the first equation. The idea is to filter out pairs with large iou or large appearance distance (two different thresholds are used here).  Then, the cost matrix element is updated as the minimum between the IoU and the new appearance distance. This method seems to be handcrafted, and the authors likely spent a significant amount of time evaluating different thresholds on the MOT17 dataset to arrive at this formulation. Note thresholds are callibrated using MOT17 validation set.</p>
  </li>
</ol>

<div class="post-center-image">
    <a href="/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/botsort.jpg">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/botsort-800-ed3f0f9ff.jpg" alt="BoT-SORT architecture diagram" srcset="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/botsort-400-f1cf4e2c5.webp 400w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/botsort-600-f1cf4e2c5.webp 600w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/botsort-800-f1cf4e2c5.webp 800w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/botsort-1000-f1cf4e2c5.webp 1000w" sizes="(max-width: 767px) 100vw, 80vw" width="1329" height="752" />
</a>

</div>

<p class="image-caption"><em>ByteTrack architecture diagram</em></p>

<h2 id="smiletrack">SMILEtrack</h2>

<p>This method currently holds the title of being the State-of-the-Art (SOTA) in the MOT17 and MOT20 datasets. It builds upon ByteTrack but throws in a handful of fresh ideas designed to give appearance features more importance.</p>

<p>I spent a couple hours trying to understand the paper but I have to admit it felt very confusing to me, so I went straight to the <a href="https://github.com/pingyang1117/SMILEtrack_Official" target="_blank" rel="noopener noreferrer">code</a>. Things got even trickier there; I noticed quite a few things that didn‚Äôt align with what was mentioned in the paper. As a results, so I opened an <a href="https://github.com/pingyang1117/SMILEtrack_Official/issues/3" target="_blank" rel="noopener noreferrer">issue</a> on the project‚Äôs GitHub repository. I‚Äôll update this section once I hear back from the authors.</p>

<!-- Let's see what they are:

1. **Similarity Learning Module (SLM)**: It is a Siamese network that computes appearence similarity between two objects using a Patch Self-Attention (PSA) block. Think about it as a boosted feature descriptor that incorporates attention mechanism following ViT style.

2. **Similarity Matching Cascade (SMC)**: Very similar to ByteTrack, it splits the data association step in two parts depending on detection scores. First, high confidence detections are tried to be matched with the tracks, for then proceeding with the low confidence ones. In both cases, IoU and appearence features are used to compute the cost matrix. The key addition is a new **GATE function** that is used right after the high confidence association. Unmatched objects with high scores might find matches in subsequent frames due to occlusions or lighting changes. When an object passes this GATE function, a new track is created for it.

Idea is, if IoU is high but they don't look alike, probably occlusion. If they don't match but they have some past track that looks alike, create a new track for it (this i don't understand very well)
There is camera motion correction in the code but not in the paper!!!!
 -->

<h2 id="references">References</h2>

<ul>
  <li>[<a href="https://arxiv.org/pdf/1602.00763.pdf">1</a>] Bewley, A., Ge, Z., Ott, L., Ramos, F., &amp; Upcroft, B. (2016, September). Simple online and realtime tracking. In 2016 IEEE international conference on image processing (ICIP) (pp. 3464-3468). IEEE.</li>
  <li>[<a href="https://arxiv.org/pdf/1703.07402.pdf">2</a>] Wojke, N., Bewley, A., &amp; Paulus, D. (2017, September). Simple online and realtime tracking with a deep association metric. In 2017 IEEE international conference on image processing (ICIP) (pp. 3645-3649). IEEE.</li>
  <li>[<a href="https://arxiv.org/pdf/2110.06864.pdf">3</a>] Zhang, Y., Sun, P., Jiang, Y., Yu, D., Weng, F., Yuan, Z., ‚Ä¶ &amp; Wang, X. (2022, October). Bytetrack: Multi-object tracking by associating every detection box. In European Conference on Computer Vision (pp. 1-21). Cham: Springer Nature Switzerland.</li>
  <li>[<a href="https://arxiv.org/pdf/2206.14651.pdf">4</a>] Aharon, N., Orfaig, R., &amp; Bobrovsky, B. Z. (2022). BoT-SORT: Robust associations multi-pedestrian tracking. arXiv preprint arXiv:2206.14651.</li>
  <li>[<a href="https://arxiv.org/pdf/2211.08824.pdf">5</a>] Wang, Y. H. (2022). SMILEtrack: SiMIlarity LEarning for Multiple Object Tracking. arXiv preprint arXiv:2211.08824.</li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Dive into the complexities of object tracking in computer vision with this detailed overview of tracking by detection. Discover the intricacies and advancements of key algorithms like SORT, DeepSORT, and ByteTrack, and learn to select the right one for your project. From foundational concepts to cutting-edge techniques, the performance and applications of multi-object tracking for real-world scenarios is dissected. SORT vs DeepSORT vs ByteTrack vs BoT-SORT vs SMILEtrack.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mmeendez8.github.io/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/thumbnail.png" /><media:content medium="image" url="https://mmeendez8.github.io/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/thumbnail.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Creating Better Data: AI &amp;amp; Homography Estimation</title><link href="https://mmeendez8.github.io/2023/09/22/homography-estimation.html" rel="alternate" type="text/html" title="Creating Better Data: AI &amp;amp; Homography Estimation" /><published>2023-09-22T00:00:00+00:00</published><updated>2023-09-22T00:00:00+00:00</updated><id>https://mmeendez8.github.io/2023/09/22/homography-estimation</id><content type="html" xml:base="https://mmeendez8.github.io/2023/09/22/homography-estimation.html"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Revolutionizing Sports Analytics with Artificial Intelligence: Discover how StatsBomb leverages cutting-edge AI and deep learning to redefine sports analytics using homography estimation, providing unparalleled insights into player performance and game strategies. Dive into the innovative world of data-driven sports analysis with StatsBomb's latest breakthroughs.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mmeendez8.github.io/assets/images/fullsize/posts/2023-09-22-homography-estimation/thumbnail.jpg" /><media:content medium="image" url="https://mmeendez8.github.io/assets/images/fullsize/posts/2023-09-22-homography-estimation/thumbnail.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Image Transmission for Computer Vision: A Comparison of Torchserve‚Äôs REST and gRPC</title><link href="https://mmeendez8.github.io/2023/02/24/torchserve-grpc.html" rel="alternate" type="text/html" title="Image Transmission for Computer Vision: A Comparison of Torchserve‚Äôs REST and gRPC" /><published>2023-02-24T00:00:00+00:00</published><updated>2023-02-24T00:00:00+00:00</updated><id>https://mmeendez8.github.io/2023/02/24/torchserve-grpc</id><content type="html" xml:base="https://mmeendez8.github.io/2023/02/24/torchserve-grpc.html"><![CDATA[<p>Special thanks to <a href="https://www.linkedin.com/in/jguzmanfd/">Javier Guzman</a> for working with me in completing the benchmarking discussed in this post.</p>

<p>In the past few weeks, we have been exploring the potential advantages of adopting gRPC to enhance the performance of our services. Although I have conducted extensive research on this topic, I have not been able to find relevant information that specifically addresses our use case, which involves transmitting images to a model server and receiving a response in the most efficient manner. While there are numerous benchmarks that demonstrate significant performance improvements when migrating from REST to gRPC using structured data, it has been challenging to locate a similar benchmark for image transmission‚Ä¶ And that is the main reason behind this post!</p>

<p>All the code for the different benchmarks can be found in <a href="https://github.com/mmeendez8/grpc_vs_rest" target="_blank" rel="noopener noreferrer">this Github repository</a>. You can find instructions in the README file. It‚Äôs important to note that our primary objective was to conduct this testing on our cloud infrastructure, where both the servers and clients were deployed on the same Kubernetes cluster. This allowed us to replicate a real-world scenario as closely as possible.</p>

<h2 id="some-thoughts-on-grpc">Some thoughts on gRPC</h2>

<p>When you start reading about gRPC, you soon realize that it involves two main features that can really help you to speed up your system communications.</p>

<h3 id="http2">HTTP2</h3>

<p>gRPC is built on the HTTP/2 protocol, which was specifically designed to address the latency issues of its predecessor, HTTP/1.1. There are two key features of HTTP/2 that are particularly relevant to our benchmarking efforts:</p>

<ul>
  <li>
    <p><strong>Multiplexed streams</strong>: With HTTP/2, multiple requests and responses can be transmitted over a single connection. While HTTP/1.1 can also reuse connections through pooling, the ability to multiplex streams becomes more important as the number of servers increases. When multiple HTTP requests are performed in a very short span of time, HTTP/1.1 has no way to share those connections. Therefore, it will create new connections to the content server for each HTTP request (see <a href="https://blog.codavel.com/http2-multiplexing" target="_blank" rel="noopener noreferrer">here</a> for a extended explanation)</p>
  </li>
  <li>
    <p><strong>Binary protocol</strong>: Unlike HTTP/1.1, which is text-based, HTTP/2 uses a binary protocol which facilitates more efficient parsing. This can have a significant impact on performance, particularly when dealing with large datasets such as images.</p>
  </li>
</ul>

<h3 id="protobuf">Protobuf</h3>

<p>Protocol Buffers, also known as Protobuf, is a language-agnostic binary serialization format developed by Google. It is used for efficient data <strong>serialization of structured data</strong> and communication between applications. It is faster than JSON for two reasons:</p>

<ul>
  <li><strong>Messages are shorter</strong>. In Protobuf messages do not contain any metadata or extra information such as field names and data types. This is not needed since the data schema has been strictly predefined in the <code class="language-plaintext highlighter-rouge">.proto</code> file. It also uses a compact binary representation, variable-length encoding, which means that the number of bytes required to represent a value depends on its size.</li>
  <li><strong>Serialization is faster</strong>. Converting messages to and from bytes is faster than in JSON because of its binary format and predefined schema. Decoding can be optimized and parallelized.</li>
</ul>

<p>In <a href="https://nilsmagnus.github.io/post/proto-json-sizes/" target="_blank" rel="noopener noreferrer">this post</a> you can see a good comparison of Protobuf vs JSON sizes for structured data. TLDR: Protobuf is always smaller than gzipped json but seems to lose its clear advantage when mesage sizes are large.</p>

<h3 id="how-does-this-apply-to-images">How does this apply to images?</h3>

<p>Structured data is text that has been predefined and formatted to a set structure. Protobuf can take advantage of the schema definitions of the data to speed up serialization and compression size. However, images do not fall under the category of structured text. Basically if you want to convert an image to bytes in an efficient manner and without losing information you have to use specific handcrafted methods that have been carefully designed for this, such as JPEG, PNG‚Ä¶ In other words, Protobuf is not going to help you here since compression and serialization will depend on your image library. See this example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># create random 100x100 rgb image
</span><span class="n">image</span> <span class="o">=</span> <span class="n">numpy</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="mi">255</span>
<span class="c1"># serialize image to jpg using opencv
</span><span class="n">encoded_image</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">imencode</span><span class="p">(</span><span class="sh">"</span><span class="s">.jpg</span><span class="sh">"</span><span class="p">,</span> <span class="n">image</span><span class="p">)[</span><span class="mi">1</span><span class="p">].</span><span class="nf">tobytes</span><span class="p">()</span>
<span class="c1"># fake send with grpc
</span><span class="n">grpc</span><span class="p">.</span><span class="nf">send</span><span class="p">(</span><span class="n">encoded_image</span><span class="p">)</span>
</code></pre></div></div>

<p>The key feature here is that Protobuf is not really helping. Given that it is one of key points of gRPC, differences between REST and gRPC cannot be that high here‚Ä¶ Let‚Äôs check this with real numbers:</p>

<h2 id="1-base-benchmark">1. Base benchmark</h2>

<p>First thing we wanted to do is check if we were able to reproduce those benchmarks we found on the web. The idea is simple, create two equivalent REST and gRPC servers and measure the time they take to process and respond to different requests.
The gRPC server has been implemented using <a href="https://grpc.io/docs/languages/python/basics/" target="_blank" rel="noopener noreferrer">python grpc library</a> and we have used <a href="https://fastapi.tiangolo.com/" target="_blank" rel="noopener noreferrer">FastAPI</a> for the REST one.</p>

<div class="post-center-image">
<a href="/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/cat_bytes.png">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/cat_bytes-768-253323f5f.png" alt="Cat being compressed to bytes" srcset="/generated/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/cat_bytes-400-b0be83dae.webp 400w, /generated/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/cat_bytes-600-b0be83dae.webp 600w, /generated/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/cat_bytes-768-b0be83dae.webp 768w" sizes="(max-width: 767px) 100vw, 80vw" width="768" height="768" />
</a>

</div>

<p class="image-caption"><em>This is what Stable Diffusion creates with the prompt ‚Äúan image of a cat is being encoded into a chunk of bytes‚Äù</em></p>

<p>We decided to measure three different requests and using a single response for all of them. The gRPC <code class="language-plaintext highlighter-rouge">.proto</code> file for those requests looks like the following:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BasicRequest</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Structured data request, we expect to match online benchmarks with this
    </span><span class="sh">"""</span>
    <span class="n">field1</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">field2</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">field3</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">field4</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>

<span class="k">class</span> <span class="nc">ImageBase64Request</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Encode image as a string using Base64 encoding. 
    This is a very bad solution (but simple to do) that should always be avoided
    </span><span class="sh">"""</span>
    <span class="n">image</span><span class="p">:</span> <span class="nb">str</span>

<span class="k">class</span> <span class="nc">ImageBinaryRequest</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Contains an image encoded as bytes.
    </span><span class="sh">"""</span>
    <span class="n">image</span><span class="p">:</span> <span class="nb">bytes</span>

<span class="k">class</span> <span class="nc">BasicResponse</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">prediction1</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span>
    <span class="n">prediction2</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>
</code></pre></div></div>

<p>Note REST‚Äôs requests and responses are identical to these so we can make a fair comparison.</p>

<p>Our client does a very simple thing, sends twenty concurrent requests to each server and waits for a response. It repeats this ten times for then computing the average time it took. Pseudocode for the client it is shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">start</span><span class="o">=</span> <span class="nf">time</span><span class="p">()</span>
    <span class="nf">send_concurrent_request_to_specific_server</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">times</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="nf">start</span><span class="p">())</span>

<span class="n">average_time</span> <span class="o">=</span> <span class="nf">mean</span><span class="p">(</span><span class="n">times</span><span class="p">)</span>
</code></pre></div></div>

<p>We tested this for three different image sizes. Results are collected below:</p>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th>¬†</th>
        <th>Basic (0.001 MB)</th>
        <th>B64 (0.306 MB)</th>
        <th>Binary (0.229 MB)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>REST</td>
        <td>0.0723</td>
        <td>0.0943</td>
        <td>0.0572</td>
      </tr>
      <tr>
        <td>gRPC</td>
        <td>0.0093 (x7.7)</td>
        <td>0.0179 (x5.2)</td>
        <td>0.0120 (x4.7)</td>
      </tr>
    </tbody>
  </table>

  <p>Table 1. Results for small images: 360x640</p>

</div>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th>¬†</th>
        <th>Basic (0.001 MB)</th>
        <th>B64 (1.160 MB)</th>
        <th>Binary (0.870 MB)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>REST</td>
        <td>0.0611</td>
        <td>0.2350</td>
        <td>0.0872</td>
      </tr>
      <tr>
        <td>gRPC</td>
        <td>0.0090 (x6.7)</td>
        <td>0.0926 (x2.5)</td>
        <td>0.0570 (x1.5)</td>
      </tr>
    </tbody>
  </table>

  <p>Table 2. Results for medium images: 720x1280</p>

</div>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th>¬†</th>
        <th>Basic (0.001 MB)</th>
        <th>B64 (3.094 MB)</th>
        <th>Binary (2.320 MB)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>REST</td>
        <td>0.0583</td>
        <td>0.8056</td>
        <td>0.1909 (x1.03)</td>
      </tr>
      <tr>
        <td>gRPC</td>
        <td>0.0097 (x6)</td>
        <td>0.2793 (x2.9)</td>
        <td>0.1974</td>
      </tr>
    </tbody>
  </table>

  <p>Table 3. Results for large images: 1080x1920</p>

</div>

<p>We can extract some conclussion from previous tables:</p>

<ol>
  <li>gRPC achieves around a x6 improvement with respect to REST for structured data (Basic column). The results <a href="https://medium.com/@EmperorRXF/evaluating-performance-of-rest-vs-grpc-1b8bdf0b22da#:~:text=gRPC%20is%20roughly%207%20times,of%20HTTP%2F2%20by%20gRPC." target="_blank" rel="noopener noreferrer">match online benchmarks</a> and we know this is because we are taking advantage of Protobuf serialization and HTTP2 protocol.</li>
  <li>For Base64 and Binary we observe a relation between image size and gRPC performance. As the image size increase, the difference between REST and gRPC are smaller.</li>
  <li>In the Base64 case, gRPC helps to serialize faster and in a more optimal way the string. We know from <a href="https://nilsmagnus.github.io/post/proto-json-sizes/" target="_blank" rel="noopener noreferrer">this post</a> that Protobuf loses its advantage when message size increases.</li>
  <li>Binary is a special case as we know we are not getting any advantage from using Protobuf for our serialization and message size (this is determined by the image format we chose). On the contrary it is harming our performance. There is still  some encoding going on in Protobuf, since it needs to format our chunk of image bytes inside the Protobuf message format. This little thing might be making REST as good as gRPC for large images!</li>
</ol>

<h2 id="torchserve-benchmark">Torchserve benchmark</h2>

<p>I have been using <a href="https://pytorch.org/serve/">TorchServe</a> for a while now and I am quite happy with it. It provides all the flexibility I need and it is quite simple to set up. Model handlers allow you to customize every detail for your specific model without really worrying about other complex things such as batching and queing requests. 
I do not intend to give an overview of TorchServe or make a comparison of its advantages compared to other inference servers, I will leave that for a plausible future post.</p>

<p>The documentation for Torchserve‚Äôs <a href="https://pytorch.org/serve/grpc_api.html" target="_blank" rel="noopener noreferrer">gRPC API</a> could be improved, as it currently requires users to download the official repository to generate a Python gRPC client stub from the proto files. However, I have attached these files to the repository, so you can easily run the benchmark without having to worry about this step.</p>

<p>The experiment is very similar to the previous one, sending 20 concurrent request and repeating that 10 times to measure the average time. I am going to use one of the pytorch vision model examples, <a href="https://pytorch.org/hub/pytorch_vision_densenet/" target="_blank" rel="noopener noreferrer">densenet161</a>. The model is not important here since we do not really care about inference results. Let‚Äôs see some results:</p>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th>¬†</th>
        <th>B64 (0.306 MB)</th>
        <th>Binary (0.229)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>REST</td>
        <td>0.884</td>
        <td>0.628</td>
      </tr>
      <tr>
        <td>gRPC</td>
        <td>X</td>
        <td>0.645</td>
      </tr>
    </tbody>
  </table>

  <p>Table 4. Results for small images: 360x640</p>

</div>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th>¬†</th>
        <th>B64 (0.306 MB)</th>
        <th>Binary (0.229)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>REST</td>
        <td>1.262</td>
        <td>0.946</td>
      </tr>
      <tr>
        <td>gRPC</td>
        <td>X</td>
        <td>0.927</td>
      </tr>
    </tbody>
  </table>

  <p>Table 5. Results for medium images: 720x1280</p>

</div>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th>¬†</th>
        <th>B64 (0.306 MB)</th>
        <th>Binary (0.229)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>REST</td>
        <td>2.188</td>
        <td>1.384</td>
      </tr>
      <tr>
        <td>gRPC</td>
        <td>X</td>
        <td>1.422</td>
      </tr>
    </tbody>
  </table>

  <p>Table 6. Results for large images: 1080x1920</p>

</div>

<p>Note there are not results for B64 gRPC since this is not allowed by Torchserve schema definition.</p>

<p>Translating the insights gained from benchmarking with the base servers can be challenging. The tables indicate that Base64 encoding should be avoided and that there are no significant performance differences between using gRPC and REST.</p>

<p>Two factors contribute to the similar performance results for gRPC and REST. Firstly, the model‚Äôs inference time is considerably longer than the networking time, making it difficult to discern the small gains obtained by changing the transmission protocol. For example, sending 20 large images concurrently in the simple base case (Table 3) took roughly 0.19s, whereas we are now spending approximately 1.4 seconds (Table 6), highlighting the significant impact of model inference time on the comparison.</p>

<p>Secondly, the Torchserve implementation plays a role in these results. It has been observed that Torchserve‚Äôs <code class="language-plaintext highlighter-rouge">.proto</code> definition for <a href="https://github.com/pytorch/serve/blob/master/frontend/server/src/main/resources/proto/inference.proto#L20-L23" target="_blank" rel="noopener noreferrer">prediction response</a> is too generic and it cannot be personalized with your model specifics.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">message</span> <span class="n">PredictionResponse</span> <span class="p">{</span>
    <span class="o">//</span> <span class="n">Response</span> <span class="n">content</span> <span class="k">for</span> <span class="n">prediction</span>
    <span class="nb">bytes</span> <span class="n">prediction</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>This means that your response will be converted to a chunk of bytes so you would not be getting any advantage from Protobuf serialization (similar to what happens with images). For example if our model returns three lists of bounding boxes, class and scores, the <code class="language-plaintext highlighter-rouge">.proto</code> file for our response could be something like:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">message</span> <span class="n">PredictionResponse</span> <span class="p">{</span>
    <span class="n">repeated</span> <span class="nb">float</span> <span class="n">scores</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
    <span class="n">repeated</span> <span class="n">int32</span> <span class="n">scores</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
    <span class="n">repeated</span> <span class="n">repeated</span> <span class="n">int32</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The differences between this response and the one provided by Torchserve are clear. You do not get any of the Protobuf advantage since the Torchserve schema definition is too general. A better or more customizable definition such as the one provided by Tfserving, of the <code class="language-plaintext highlighter-rouge">.proto</code> file could help boost performance.</p>

<h2 id="conclusion">Conclusion</h2>

<p>If you‚Äôre using Torchserve to serve computer vision models, it‚Äôs recommended to steer clear of gRPC. Our findings show that there are no performance benefits to using gRPC. Moreover, it adds code complexity while hindering debugging due to its non-human-readable messages. Since REST is more commonly used, most developers are already familiar with it. Switching to gRPC in this scenario comes with a learning curve that doesn‚Äôt offer any significant advantages.</p>

<p><em>Any ideas for future posts or is there something you would like to comment? Please feel free to reach out via <a href="https://twitter.com/mmeendez8" target="_blank" rel="noopener noreferrer">Twitter</a> or <a href="https://github.com/mmeendez8" target="_blank" rel="noopener noreferrer">Github</a></em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[This post compares the performance of Torchserve's REST and gRPC communication protocols for transmitting images to a computer vision deep learning model. We conducted benchmarks for structured data, Base64 encoded images, and binary image transmission. The goal is to help practitioners make informed decisions when choosing the right communication protocol for their specific use case, taking into account factors such as ease of implementation and familiarity with the technology.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mmeendez8.github.io/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/thumbnail.jpg" /><media:content medium="image" url="https://mmeendez8.github.io/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/thumbnail.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">CNNs &amp;amp; Transformers Explainability: What do they see?</title><link href="https://mmeendez8.github.io/2021/12/09/cnn-vs-transformers.html" rel="alternate" type="text/html" title="CNNs &amp;amp; Transformers Explainability: What do they see?" /><published>2021-12-09T00:00:00+00:00</published><updated>2021-12-09T00:00:00+00:00</updated><id>https://mmeendez8.github.io/2021/12/09/cnn-vs-transformers</id><content type="html" xml:base="https://mmeendez8.github.io/2021/12/09/cnn-vs-transformers.html"><![CDATA[<p><em>I created a small demo in <a href="https://huggingface.co/spaces/mmeendez/cnn_transformer_explainability" target="_blank" rel="noopener noreferrer">Hugging Face Spaces</a> to play with the code</em></p>

<p>A couple of weeks ago I saw a very interesting <a href="https://pytorch.org/blog/FX-feature-extraction-torchvision/" target="_blank" rel="noopener noreferrer">post by Alexander Soare and Francisco Massa</a> on Pytorch Blog. The authors explained that the latest version of Torchivision  <a href="https://github.com/pytorch/vision/releases/tag/v0.11.0" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">v0.11.0</code></a> included a new utility that allows us to access intermediate transformations of an input during the step-forward of a PyTorch module. That is, we don‚Äôt need more complex code to get the intermediate activations of a model, we can simply point to a specific layer and get its results. The article analyzes the different methods that were used to carry out this task, comparing their advantages and disadvantages. This is a remarkably clear post (as generally on the Pytorch blog) that not only explains you how this new feature works, but also provides insight into the other common methods.</p>

<p>So ‚Ä¶ I couldn‚Äôt resist, I really wanted to try this and see how it works! I‚Äôve been thinking about the differences between Transformer and CNN when classifying images and was wondering if I could compare them. So I rechecked the Class Activation Map paper<a href="https://arxiv.org/pdf/1512.04150.pdf" target="_blank" rel="noopener noreferrer">[1]</a> from 2015. This is a classic job that shows how to paint activation maps from your last conv layer, conditioned on your model output label. For the case of transformers, I based my experiments on Attention Flow <a href="https://arxiv.org/pdf/2005.00928.pdf" target="_blank" rel="noopener noreferrer">[2]</a> which seems to be the standard method in the community.</p>

<p>This post was created with the intention of improving my knowledge on ViT, TorchVision and model‚Äôs explainability. I do not pretend to compare ResNet against ViT since they have been trained with different datasets. ViT was pre-trained on ImageNet-21k and finetuned on ImageNet whileas ResNet50 was only trained on ImageNet.</p>

<p>Now, let‚Äôs see how to implement both methods and visualize some results!</p>

<h2 id="class-activation-map---resnet">Class Activation Map - ResNet</h2>

<h3 id="a-small-review">A small review</h3>

<p>In <a href="https://arxiv.org/pdf/1512.04150.pdf" target="_blank" rel="noopener noreferrer">[1]</a> authors propose a way to relate last layer activations to the input image. Conv layers apply a set of filters to the input data and they return the stacked filter responses. In this paper authors show how each of this stacked responses contribute to decide the output label. The trick is very simple, they propose to add a Global Average Pooling (GAP) layer over each of the 2D features outputted from the last convolutional layer. Thanks to this, we can figure out how much is each filter contributing to the final classification of the image. As usually an image is worth a thousand words, so have a look at the figure below extracted from the paper:</p>

<p><a href="/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/gap.jpg">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/gap-800-8518703f0.jpg" alt="CNNs &amp; Transformers Explainability: What do they see?" srcset="/generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/gap-400-c0f0bfcc7.webp 400w, /generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/gap-600-c0f0bfcc7.webp 600w, /generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/gap-800-c0f0bfcc7.webp 800w, /generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/gap-1000-c0f0bfcc7.webp 1000w" sizes="(max-width: 767px) 100vw, 80vw" width="1002" height="476" />
</a></p>

<p>See how the GAP layer reduces each of the filter outputs to a single averaged element. Then, we will have a vector of size <code class="language-plaintext highlighter-rouge">n_filters</code> that will be multiplied by a linear layer which weights will be a matrix of size <code class="language-plaintext highlighter-rouge">n_filters</code> x <code class="language-plaintext highlighter-rouge">n_classes</code>. Once you know the classification output, you can ‚Äúisolate‚Äù the weight vector related with that class and multiply it by the activations. In math notation this would be expressed by:</p>

\[M_c(x,y) = \sum_{k}w_k^c f_k(x,y)\]

<p>where \(k\) represents the number of filters in the last conv layer, \(w_k^c\)  are the linear layer weights and \(f_k(x,y)\) represents the 2D stacked filter responses.</p>

<p>This paper was publised in 2015 and at that time popular architectures did not have GAP layers so they have to be finetuned with these extra layers‚Ä¶ But we are going to use a ResNet architecture which already has a GAP layer at the end! You can check <a href="https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L203" target="_blank" rel="noopener noreferrer">here</a> torchvision implementation of ResNets to be sure of this.</p>

<p>There have been multiple works that have evolved CAM idea, you can check a few implementions of them in <a href="https://github.com/frgfm/torch-cam" target="_blank" rel="noopener noreferrer">torch-cam</a> repo.</p>

<h3 id="code">Code</h3>

<p>First of all we need to get the pretrained ResNet50 model from torchvision and put it in eval model. Then we can get extract the features we need by specifying their names. We can check all the names of the layers with <code class="language-plaintext highlighter-rouge">get_graph_node_names</code> function. In this case I need to extract last conv layer activation, this is <code class="language-plaintext highlighter-rouge">layer4</code>. One of the advantages of using the new feature extractor is that it would automatically mark the <code class="language-plaintext highlighter-rouge">layer4</code> as a leaf of the computation graph, so following layers would not be computed (and that‚Äôs awesome!). Unfortunately, we also need to get the classification output of the network so we are not really getting the full power of the feature_extractor. Let‚Äôs code this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">models</span>
<span class="kn">from</span> <span class="n">torchvision.models.feature_extraction</span> <span class="kn">import</span> <span class="n">create_feature_extractor</span>

<span class="n">resnet50</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="nf">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">)</span>
<span class="n">resnet50</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

<span class="n">feature_extractor</span> <span class="o">=</span> <span class="nf">create_feature_extractor</span><span class="p">(</span><span class="n">resnet50</span><span class="p">,</span> <span class="n">return_nodes</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">layer4</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">fc</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div>

<p>For computing the CAM we just need to apply the previous formula. First we need to get linear layer weight matrix, select the row that relates with the predicted output class and multiply it by the extracted features, then we can apply min-max normalization so that the CAM is between 0 and 1.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fc_layer_weights</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">.</span><span class="n">fc</span><span class="p">.</span><span class="n">weight</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="c1"># Extract features and remove batch dim
</span>    <span class="n">output</span> <span class="o">=</span> <span class="nf">feature_extractor</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">)</span>
    <span class="n">cnn_features</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="sh">"</span><span class="s">layer4</span><span class="sh">"</span><span class="p">].</span><span class="nf">squeeze</span><span class="p">()</span>
    <span class="n">class_id</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="sh">"</span><span class="s">fc</span><span class="sh">"</span><span class="p">].</span><span class="nf">argmax</span><span class="p">()</span>

    <span class="c1"># Linear combination of class weights and cnn features
</span>    <span class="n">cam</span> <span class="o">=</span> <span class="n">fc_layer_weights</span><span class="p">[</span><span class="n">class_id</span><span class="p">].</span><span class="nf">matmul</span><span class="p">(</span><span class="n">cnn_features</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># Reshape back to 2D
</span><span class="n">cam</span> <span class="o">=</span> <span class="n">cam</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">cnn_features</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cnn_features</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</code></pre></div></div>

<p>That‚Äôs all! Just a few lines, let‚Äôs see a few simple examples:</p>

<div class="post-center-image">
<a href="/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/CAMR.jpg">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/CAMR-800-333ec12b6.jpg" alt="CAM" srcset="/generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/CAMR-400-d5f6fcf29.webp 400w, /generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/CAMR-600-d5f6fcf29.webp 600w, /generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/CAMR-800-d5f6fcf29.webp 800w, /generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/CAMR-1000-d5f6fcf29.webp 1000w" sizes="(max-width: 767px) 100vw, 80vw" width="1100" height="200" />
</a>

</div>

<h2 id="vit-attention-map">ViT Attention Map</h2>

<h3 id="another-brief-review">Another brief review</h3>

<p>ViT paper<a href="https://arxiv.org/pdf/2010.11929.pdf" target="_blank" rel="noopener noreferrer">[3]</a> was publised at the end of 2020 and it has already become a reference in the field. There are an incredible large number of works<a href="https://arxiv.org/abs/2101.01169" target="_blank" rel="noopener noreferrer">[4]</a> that have used it as a baseline to build new methods upon its ideas. The authors found a simple way to treat images as sequences so they can feed them to a Transformer encoder, simply divide them into fixed-size patches.</p>

<div class="post-center-image">
<a href="/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/vit.jpg">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/vit-800-da40c3cdb.jpg" alt="Vision Transformer" srcset="/generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/vit-400-23824f5be.webp 400w, /generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/vit-600-23824f5be.webp 600w, /generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/vit-800-23824f5be.webp 800w, /generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/vit-879-23824f5be.webp 879w" sizes="(max-width: 767px) 100vw, 80vw" width="879" height="470" />
</a>

</div>

<p>The attentions mechanism allows us to figure out what parts or patches of the image are key for the classification result. This will allow us to interpret model‚Äôs decision.</p>

\[Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\]

<p>In the attention formula, the dot product between the query and the key represents the raw attention scores. I like to imagine this as a similarity matrix, where each position represents how ‚Äúsimilar‚Äù the query and key embeddings are. So when both vectors are not aligned the dot product will tend to zero.</p>

<p>At the very first attention layer, the input vectors are the linear projections of the flattened patches:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pseudocode simplification from HF implementation
</span><span class="n">patch_embeddings</span> <span class="o">=</span> <span class="nc">PatchEmbeddings</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">patch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">flat_patch_embeddings</span> <span class="o">=</span> <span class="nc">Flatten</span><span class="p">(</span><span class="n">patch_embeddings</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">linear_projections</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">patch_embeddings</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">embedding_size</span><span class="p">)</span>
</code></pre></div></div>

<p>So it would be very easy to visualize attention weights at this very first layer because they directly relate to the image embeddings. This task becomes harder when we stack multiple Transformer layers (there are 12 layers in ViT). In <a href="https://arxiv.org/abs/2005.00928" target="_blank" rel="noopener noreferrer">[2]</a> two different methods are proposed with the aim of easing this task, Attention Rollout and Attention Flow. We are going to use the first of them because of its simplicity.</p>

<h3 id="attention-rollout">Attention Rollout</h3>

<p>We can model the information flow as a graph where input patches and hidden embeddings are the nodes and the edges represent the attentions from the nodes in one layer to the next layer. These edges are weighted by the attention weights which determine the amount of information that is passed from one layer to the next. Hence, if we want to compute the attention that a node at layer \(i\) receives from all previous layer nodes, we can simply multiply the attention weights matrices from the input layer until our target \(i\). Check the following animation to see how this works:</p>

<div class="post-center-image">
<a href="/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/attention_rollout.jpg">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/attention_rollout-774-2f6a36848.jpg" alt="Attention Rollout" srcset="/generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/attention_rollout-400-d83342cea.webp 400w, /generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/attention_rollout-600-d83342cea.webp 600w, /generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/attention_rollout-774-d83342cea.webp 774w" sizes="(max-width: 767px) 100vw, 80vw" width="774" height="658" />
</a>

</div>

<p class="image-caption"><em>Attention rollout simulation obtained from <a href="https://samiraabnar.github.io/articles/2020-04/attention_flow" target="_blank" rel="noopener noreferrer">Samira Abnar‚Äôs blog</a></em></p>

<p>This is super straight-forward and easy to understand but we are missing the influence of residual connections. Paper authors handle this in a very elegant way, they realize that the output at layer \(V_{l+1}\) depends on the previous output and the attention weights: \(V_{l+1} = V_{l} + W_{att}V_l\), where \(W_{att}\) is the attention matrix. This can also be expressed as \(V_{l+1} = (W_{att} + I)V_l\). Thus, re-normalizing the weights, the raw attention updated by residual connections can be expressed as: \(A = 0.5W_{att} + 0.5I\).</p>

<p>Note I have seen other implementations of this method that instead of averaging the attention between the different heads of each layer, use min or max operator since it seems to work better in practice (see <a href="https://github.com/jacobgil/vit-explain" target="_blank" rel="noopener noreferrer">this implementation</a>)</p>

<h3 id="code-1">Code</h3>

<p>First of all we need to setup our ViT model, unfortunately at the moment of writing this post we cannot use Torchvision‚Äôs ViT because it is not included in latest version <code class="language-plaintext highlighter-rouge">0.11.1</code> (it has been recently added see <a href="https://github.com/pytorch/vision/pull/4594" target="_blank" rel="noopener noreferrer">this PR</a>). For this reason, we cannot use the new feature extractor and we need to find another implementation. I will use Hugging Face library because it is simple and allows me get all attention matrices directly.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">ViTForImageClassification</span>
<span class="n">vit</span> <span class="o">=</span> <span class="n">ViTForImageClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">google/vit-base-patch16-224</span><span class="sh">"</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">)</span>
<span class="n">vit</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
</code></pre></div></div>

<p>You can check the <a href="https://huggingface.co/docs/transformers/model_doc/vit#transformers.ViTModel" target="_blank" rel="noopener noreferrer">official documentation</a> to see how we can use <code class="language-plaintext highlighter-rouge">output_attentions</code> parameter to get the attentions tensors of all attention layers. Attention rollout code would consist on:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Inference
</span><span class="n">result</span> <span class="o">=</span> <span class="nf">vit</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># Stack all layers attention
</span><span class="n">attention_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">1</span><span class="p">]).</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Average the attention at each layer over all heads
</span><span class="n">attention_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Add residual and re-normalize
</span><span class="n">residual</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)).</span><span class="nf">to</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">)</span>
<span class="n">attention_probs</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">attention_probs</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">residual</span>

<span class="c1"># Normalize by layer
</span><span class="n">attention_probs</span> <span class="o">=</span> <span class="n">attention_probs</span> <span class="o">/</span> <span class="n">attention_probs</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Compute rollout
</span><span class="n">attention_rollout</span> <span class="o">=</span> <span class="n">attention_probs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">attention_probs</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
    <span class="n">attention_rollout</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">attention_rollout</span><span class="p">)</span>

<span class="c1"># Attentions between CLS token and patches
</span><span class="n">mask</span> <span class="o">=</span> <span class="n">attention_rollout</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span>
<span class="c1"># Reshape back to 2D
</span><span class="n">mask_size</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">mask</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">mask_size</span><span class="p">,</span> <span class="n">mask_size</span><span class="p">)</span>
</code></pre></div></div>

<p>Pretty simple, let‚Äôs see a few examples:</p>

<div class="post-center-image">
<a href="/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/ROLLOUTR.jpg">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/ROLLOUTR-800-62565fb71.jpg" alt="Rollout Transform" srcset="/generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/ROLLOUTR-400-c56709655.webp 400w, /generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/ROLLOUTR-600-c56709655.webp 600w, /generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/ROLLOUTR-800-c56709655.webp 800w, /generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/ROLLOUTR-1000-c56709655.webp 1000w" sizes="(max-width: 767px) 100vw, 80vw" width="1100" height="200" />
</a>

</div>

<p>There seems to be a larger noise when we comparing these results wrt CAM ones. One plausible option to reduce this effect is to filter very low attentions and keep only the strongest ones. I will stick with the original implementation but you find about this in <a href="https://github.com/jacobgil/vit-explain" target="_blank" rel="noopener noreferrer">this repo</a>.</p>

<h2 id="conclusion">Conclusion</h2>

<p>We have covered two important methods that can give us some intuition on how CNNs and Transformers work internally. A few key aspects that we must keep in mind:</p>

<ul>
  <li>
    <p>The idea behind this post was to improve my understanding of the ViT architecture, TorchVision new features, GAP and Attention Rollout. This should not be used as a comparison between ResNet and ViT, since ViT was pre-trained on ImageNet-21k and finetuned on ImageNet whileas ResNet50 was only trained on ImageNet.</p>
  </li>
  <li>
    <p>CAM does not generalize to models without global average pooling. You would need to retrain your model with a GAP layer or use a different method. <a href="https://github.com/frgfm/torch-cam" target="_blank" rel="noopener noreferrer">Here</a> you can check some different implementations.</p>
  </li>
  <li>
    <p>I have used Hugging Face‚Äôs ViT implementation since it is not yet available on latest Torchvision version.</p>
  </li>
  <li>
    <p>Do not forget to check the <a href="https://huggingface.co/spaces/mmeendez/cnn_transformer_explainability" target="_blank" rel="noopener noreferrer">Hugging Face Space</a> I created for this post!</p>
  </li>
</ul>

<h2 id="references">References</h2>

<ul>
  <li>[1] Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., &amp; Torralba, A. (2016). Learning deep features for discriminative localization. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2921-2929).</li>
  <li>[2] Abnar, S., &amp; Zuidema, W. (2020). Quantifying attention flow in transformers. arXiv preprint arXiv:2005.00928.</li>
  <li>[3] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ‚Ä¶ &amp; Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.</li>
  <li>[4] Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., &amp; Shah, M. (2021). Transformers in vision: A survey. arXiv preprint arXiv:2101.01169.</li>
</ul>

<p><em>Any ideas for future posts or is there something you would like to comment? Please feel free to reach out via <a href="https://twitter.com/mmeendez8" target="_blank" rel="noopener noreferrer">Twitter</a> or <a href="https://github.com/mmeendez8" target="_blank" rel="noopener noreferrer">Github</a></em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[A Hugging Face Space to compare ResNet Class Activation Map to Vit Attention Rollout]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mmeendez8.github.io/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/thumbnail.jpg" /><media:content medium="image" url="https://mmeendez8.github.io/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/thumbnail.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Using Ansible and Python to monitor my servers</title><link href="https://mmeendez8.github.io/2021/09/23/ansible-conda.html" rel="alternate" type="text/html" title="Using Ansible and Python to monitor my servers" /><published>2021-09-23T00:00:00+00:00</published><updated>2021-09-23T00:00:00+00:00</updated><id>https://mmeendez8.github.io/2021/09/23/ansible-conda</id><content type="html" xml:base="https://mmeendez8.github.io/2021/09/23/ansible-conda.html"><![CDATA[<p>I have been in charge of my team‚Äôs computational servers for about a year. I am far from being an expert in this field and most of the things I know are related to having been using Linux for so many years. I needed to automate some things that were wasting me a lot of time, so I decided that instead of learning and configuring a new monitoring tool, I would use this opportunity to create my own. This may not be the best decision for you, but in my case I had a clear vision of what I needed and the amount of work it would take to do it (and I also wanted to play a little bit with all this stuff).</p>

<p>Anyway, the best advice I can give and the one I learned by heart is: when it comes to a shared infrastructure, better make sure you test everything thoroughly before touching things there or you will end up with a high rate of mail blaming you.</p>

<h2 id="why-ansible">Why Ansible?</h2>

<p>Well, Ansible‚Äôs moto is: Ansible is Simple IT Automation. That‚Äôs what I was looking for and after checking their documentations I realized that I wouldn‚Äôt need much more than 15 minutes to set it up and that‚Äôs pretty awesome. I read a few posts such <a href="https://mtyurt.net/post/2020/good-bad-parts-of-ansible-after-two-years.html" target="_blank" rel="noopener noreferrer">this one</a> that basically supported all my initial intuitions. I was also influenced by some of my devops friends who used to say good thing about it. Summarizing:</p>

<h3 id="advantages">Advantages</h3>

<ul>
  <li>Open source</li>
  <li>Written in Python!</li>
  <li>YAML based = Low barrier of entry</li>
  <li>Agentless = You don‚Äôt need to install any other software on the client systems (this is super)</li>
  <li>Many integrated modules (it hardly takes time to automate classic operations such as user creation, database operations ‚Ä¶)</li>
</ul>

<h3 id="drawbacks">Drawbacks</h3>

<p>In fact, I still didn‚Äôt find any drawbacks, although I did seek other people‚Äôs opinion, but things like lack of user interface or lack of Windows support don‚Äôt really matter to me.</p>

<h2 id="my-python-monitoring-library">My Python monitoring library</h2>

<p>My idea was to create a simple python library that could execute commands and communicate with our internal messaging application <a href="https://rocket.chat/" target="_blank" rel="noopener noreferrer">Rocket.chat</a>. In this way, you could receive private messages when something does not work as expected and also automate tasks such as sending welcome messages on user creation, cleaning system caches, updating packages‚Ä¶</p>

<p>Since we use conda in all our servers, the intention was to automatically create a new environment to install there my monitoring application and all its dependencies. I cannot share this application but it is quite a simple thing, I followed a <a href="https://charlesreid1.github.io/python-patterns-the-registry.html" target="_blank" rel="noopener noreferrer">register pattern</a> since I wanted to be able to sequentially add new functionalities that could get automatically registered in my application. I recommend you to use the <a href="https://github.com/explosion/catalogue" target="_blank" rel="noopener noreferrer">catalogue</a> library from explosion guys. The provide a super simple way to register your functions or classes using a decorator. What I did was to use my library main‚Äôs <code class="language-plaintext highlighter-rouge">__init__.py</code> file to declare the register:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">catalogue</span>

<span class="n">SERVICES</span> <span class="o">=</span> <span class="n">catalogue</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span><span class="sh">"</span><span class="s">my_services_app</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">services</span><span class="sh">"</span><span class="p">)</span>

</code></pre></div></div>

<p>And the simply register my ‚Äòservices‚Äô with:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">my_services_app</span> <span class="kn">import</span> <span class="n">SERVICES</span>
<span class="kn">from</span> <span class="n">my_services_app.services.base</span> <span class="kn">import</span> <span class="n">ServerStatus</span>


<span class="nd">@SERVICES.register</span><span class="p">(</span><span class="sh">"</span><span class="s">PingService</span><span class="sh">"</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">PingService</span><span class="p">(</span><span class="n">BaseService</span><span class="p">):</span>
    <span class="bp">...</span>
</code></pre></div></div>

<p>That‚Äôs all, you now would be able to instantiate your <code class="language-plaintext highlighter-rouge">PingService</code> by:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">my_services_app</span> <span class="kn">import</span> <span class="n">SERVICES</span>
<span class="n">service</span> <span class="o">=</span> <span class="n">SERVICES</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">PingService</span><span class="sh">"</span><span class="p">)()</span>
</code></pre></div></div>

<p>So simple and so powerful! Also note that I have created a <code class="language-plaintext highlighter-rouge">BaseService</code> class, which basically has all the necessary funcionality to send messages to Rocket.chat.</p>

<h2 id="conda--ansible">Conda &amp; Ansible</h2>

<p>I created a super simple playbook that helps to clone the latest version of my code from Github, create a new conda environment if does not exists, install my Python monitoring library and also set up some of those services as cron jobs!</p>

<p>But first we need to create a Github Personal Access Token (PAT) to let ansible to login and clone the source code. You can check the official documentation <a href="https://docs.github.com/en/github/authenticating-to-github/keeping-your-account-and-data-secure/creating-a-personal-access-token" target="_blank" rel="noopener noreferrer">here</a>. Once we have our token we can encrypt it with ansible add an extra security layer to our deployment. You can simply do something like:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"gh_tokeen: [YOUR-TOKEN]"</span> <span class="o">&gt;</span> gh_token.enc
ansible-vault encrypt gh_token.enc
</code></pre></div></div>

<p>You should now have a <code class="language-plaintext highlighter-rouge">gh_token.enc</code> encrypted file and Ansible will be in charge of asking you for the password when running the playbook so it can decrypt it. Note that this is very handy since you can place encrypted content under source control and share it more safely.</p>

<p>Let‚Äôs now create our Ansible playbook to deploy our monitoring library and also deploy one of our services as a cron tass than runs once hourly:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nn">---</span>
<span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Create remote services</span>
  <span class="na">hosts</span><span class="pi">:</span> <span class="s">all</span>
  <span class="na">vars</span><span class="pi">:</span>
    <span class="na">conda_path</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">Conda installation path</span><span class="pi">]</span>
    <span class="na">conda_env</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">Conda environment name</span><span class="pi">]</span>
    <span class="na">repo_path</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">Path where clone the repo</span><span class="pi">]</span>

  <span class="na">tasks</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Clone and update repo</span>
    <span class="na">git</span><span class="pi">:</span>
      <span class="na">repo</span><span class="pi">:</span> <span class="s2">"</span><span class="s">https://@github.com/mmeendez8/myrepo.git"</span>
      <span class="na">dest</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>
      <span class="na">clone</span><span class="pi">:</span> <span class="s">yes</span>
      <span class="na">update</span><span class="pi">:</span> <span class="s">yes</span>
  
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Check conda environment exists</span>
    <span class="na">command</span><span class="pi">:</span>
      <span class="na">cmd</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/bin/activate</span><span class="nv"> </span><span class="s">"</span>
    <span class="na">register</span><span class="pi">:</span> <span class="s">env_output</span>
    <span class="na">ignore_errors</span><span class="pi">:</span> <span class="s">True</span>

  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Install conda environment if not exists</span>
    <span class="na">command</span><span class="pi">:</span> 
      <span class="na">cmd</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/bin/conda</span><span class="nv"> </span><span class="s">env</span><span class="nv"> </span><span class="s">create</span><span class="nv"> </span><span class="s">-f</span><span class="nv"> </span><span class="s">/conda.yaml"</span>
    <span class="na">when</span><span class="pi">:</span> <span class="s">env_output.failed == True</span>
  
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Install server_status package</span>
    <span class="na">shell</span><span class="pi">:</span> 
      <span class="na">cmd</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/envs//bin</span><span class="nv"> </span><span class="s">setup.py</span><span class="nv"> </span><span class="s">install"</span>
      <span class="na">chdir</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span>

  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Check status</span>
    <span class="na">ansible.builtin.cron</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">My</span><span class="nv"> </span><span class="s">Ping</span><span class="nv"> </span><span class="s">Service"</span>
    <span class="na">special_time</span><span class="pi">:</span> <span class="s">hourly</span>
    <span class="na">job</span><span class="pi">:</span> <span class="pi">&gt;</span>
        <span class="s">/envs//bin/server_status --service_name=PingService</span>
</code></pre></div></div>

<p>This code is basically plain english! There are a few things we can highlight. First, see how we use `` variable inside the github repository url. We need to tell Ansible that this variable is inside our encrypted file so we should run our playbook with:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ansible-playbook create_services.yml <span class="nt">-e</span> @gh_token.enc <span class="nt">--ask-vault-pass</span>
</code></pre></div></div>

<p>Also see how the ‚Äòcheck conda environmet‚Äô task registers in a variable the return of activate command so the conda installation task runs only when the environment does not exist.
You can execute this playbook as many times as you want since Ansible is pretty smart and will not create new cron tasks, it will identify this one was already created and update its values if necessary!</p>

<h2 id="conclusion">Conclusion</h2>

<p>There is a very simple way to create and deploy Python code on your computational servers without spending effort and time in complex configurations. We have learned to:</p>

<ul>
  <li>
    <p>Setup Ansible to deploy our library in all our servers</p>
  </li>
  <li>
    <p>Use conda to encapsulate our library dependencies</p>
  </li>
  <li>
    <p>Add a pattern registry to dinamically add modules to our Python library</p>
  </li>
  <li>
    <p>Hide our secrets with Ansible built-in encription</p>
  </li>
  <li>
    <p>Set cron jobs that will run automatically</p>
  </li>
</ul>

<p><em>Any ideas for future posts or is there something you would like to comment? Please feel free to reach out via <a href="https://twitter.com/mmeendez8" target="_blank" rel="noopener noreferrer">Twitter</a> or <a href="https://github.com/mmeendez8" target="_blank" rel="noopener noreferrer">Github</a></em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[The best way for monitoring your server is through your own code,]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mmeendez8.github.io/assets/images/fullsize/posts/2021-09-23-ansible-conda/thumbnail.jpg" /><media:content medium="image" url="https://mmeendez8.github.io/assets/images/fullsize/posts/2021-09-23-ansible-conda/thumbnail.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">DVC + MMdetection</title><link href="https://mmeendez8.github.io/2021/08/30/mmdet-dvc-tutorial.html" rel="alternate" type="text/html" title="DVC + MMdetection" /><published>2021-08-30T00:00:00+00:00</published><updated>2021-08-30T00:00:00+00:00</updated><id>https://mmeendez8.github.io/2021/08/30/mmdet-dvc-tutorial</id><content type="html" xml:base="https://mmeendez8.github.io/2021/08/30/mmdet-dvc-tutorial.html"><![CDATA[<p>I recently <a href="/2021/07/01/dvc-tutorial.html" target="_blank" rel="noopener noreferrer">published a post</a> where I showed how to use DVC to maintain versions of our datasets so we reduce data reproducibility problems to a minimum. This is the second part of the tutorial where we are going to see how we can combine the power of mmdetection framework and its huge <a href="https://github.com/open-mmlab/mmdetection/blob/master/docs/model_zoo.md" target="_blank" rel="noopener noreferrer">model zoo</a> with DVC for designing ML pipelines, versioning our models and monitor training progress.</p>

<p>It is quite a lot of content to cover, so I will be going through it step by step and trying to keep things as simple as possible. You can find all the code for this tutorial in my <a href="https://github.com/mmeendez8/mmdetection_dvc" target="_blank" rel="noopener noreferrer">Github</a>. So let‚Äôs start with it!</p>

<h2 id="1-setup-the-environment">1. Setup the environment</h2>

<p>We are gonna need a few packages to get our project up and running. I have created a <code class="language-plaintext highlighter-rouge">conda.yml</code> that you can find in the root of the repository, this is going to install pytorch and cudatoolkit since we are going to train our models using a GPU. You can create the environment by:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">env </span>create <span class="nt">-f</span> conda.yaml
conda activate mmdetection_dvc
</code></pre></div></div>

<h2 id="2-import-our-dataset">2. Import our dataset</h2>

<p>In the previous post we used a subset of the COCO dataset created by fast.ai. We push all data to a Google Drive remote storage using DVC and keep all metada files in a Github repository. We need now to import this dataset in our repo and that‚Äôs exactly what <a href="https://dvc.org/doc/command-reference/import" target="_blank" rel="noopener noreferrer">dvc import</a> can do for us!</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dvc init
dvc import <span class="s2">"git@github.com:mmeendez8/coco_sample.git"</span> <span class="s2">"data/"</span> <span class="nt">-o</span> <span class="s2">"data/"</span>
</code></pre></div></div>

<p>Note that we are importing <code class="language-plaintext highlighter-rouge">/data</code> folder of the remote repository into the <code class="language-plaintext highlighter-rouge">data/</code> directory of our project where all our data will be stored. This may take a while since we need to download all images and annotations from the remote gdrive storage. Let‚Äôs now publish the changes on git:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git add <span class="nb">.</span> 
git commit <span class="nt">-m</span> <span class="s2">"Import coco_sample dataset"</span> 
git push
</code></pre></div></div>

<p>Once it is downloaded, we can move between different <a href="https://github.com/mmeendez8/coco_sample/releases" target="_blank" rel="noopener noreferrer">versions of the dataset</a> with the <a href="https://dvc.org/doc/command-reference/update" target="_blank" rel="noopener noreferrer">dvc update</a> command. If we would go back to v1.0 of our dataset and push our changes to git:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dvc update <span class="nt">--rev</span> v1.0 data.dvc
git add data.dvc
git commit <span class="nt">-m</span> <span class="s2">"Get version 1.0 of coco_sample"</span>
git push
</code></pre></div></div>

<p>That‚Äôs it! We have imported our dataset and we know how to move between different versions so let‚Äôs create a script that will train our model!</p>

<h2 id="3-train-our-model">3. Train our model</h2>

<p><a href="https://github.com/open-mmlab/mmdetection" target="_blank" rel="noopener noreferrer">MMDetection</a> is an open source object detection toolbox based on PyTorch. It is a part of the OpenMMLab project and it is one of the most popular computer vision frameworks. I love it and I am an active contributor since it became my default framework for object detection last year.</p>

<p>They have an extense documentation which really helps first time users. In this post I will skip the very basics and focus on showing how easily can we train a RetinaNet object detector on our coco_sample dataset.</p>

<h3 id="31-model-config">3.1. Model config</h3>

<p>First thing we need to do is to find the config file for our model, so let‚Äôs explore mmdet model zoo and more specifically <a href="https://github.com/open-mmlab/mmdetection/tree/master/configs/retinanet" target="_blank" rel="noopener noreferrer">RetinaNet section</a>. There‚Äôs a bunch of different RetinaNet models there but let‚Äôs stick with the base config from the <a href="https://arxiv.org/pdf/1708.02002.pdf" target="_blank" rel="noopener noreferrer">original paper</a>. I have already downloaded this file to my repo and you can find it under <code class="language-plaintext highlighter-rouge">configs/retinanet_r50_fpn.py</code>. There are three main sections there:</p>

<ul>
  <li>The backbone definition, which in our case is a ResNet50. Its weights come from some torchvision checkpoint specified at:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">init_cfg</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="sh">'</span><span class="s">Pretrained</span><span class="sh">'</span><span class="p">,</span> <span class="n">checkpoint</span><span class="o">=</span><span class="sh">'</span><span class="s">torchvision://resnet50</span><span class="sh">'</span><span class="p">))</span><span class="err">`</span>
</code></pre></div></div>

<p>As a curious fact, I checked out <a href="https://pytorch.org/vision/stable/models.html" target="_blank" rel="noopener noreferrer">official torchvision documentation</a> and it seems this network has been trained with some dataset that is currently lost so there is no chance to reproduce this results‚Ä¶</p>

<center>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Just found this checking <a href="https://twitter.com/hashtag/torchvision?src=hash&amp;ref_src=twsrc%5Etfw">#torchvision</a> stable models, it seems they were trained on some volatile dataset üòÖ<br />cc <a href="https://twitter.com/DVCorg?ref_src=twsrc%5Etfw">@DVCorg</a> <a href="https://t.co/rR8ANSmucI">pic.twitter.com/rR8ANSmucI</a></p>&mdash; Miguel Mendez (@mmeendez8) <a href="https://twitter.com/mmeendez8/status/1418507102465765376?ref_src=twsrc%5Etfw">July 23, 2021</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>

<ul>
  <li>
    <p>The <a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener noreferrer">Feature Pyramid Network (FPN)</a> configuration which is useful to find features at different scale levels.</p>
  </li>
  <li>
    <p>The classification head and the regression head. They predict labels and bounding boxes regression parameters for each of the anchors of the model. I cannot really go deep how this model works and what anchors are but you should check our repo <a href="https://github.com/Gradiant/pyodi" target="_blank" rel="noopener noreferrer">pyodi</a> if you really want to understand all the details.</p>
  </li>
</ul>

<h3 id="32-dataset-config">3.2 Dataset config</h3>

<p>Mmdetection framework also uses config files for datasets. There we define our train and validation data and which types of transformation do we want to apply before images are feed into the network. Since our dataset follows COCO format, I just modified original <a href="https://github.com/open-mmlab/mmdetection/blob/master/configs/_base_/datasets/coco_detection.py" target="_blank" rel="noopener noreferrer">COCO_detection.py</a>. Note that:</p>

<ul>
  <li>I removed the test set since we are not going to use one for this tutorial.</li>
  <li>I added a <code class="language-plaintext highlighter-rouge">CLASSES</code> variable with our reduced set of labels.</li>
</ul>

<p>You can check the dataset config file in <code class="language-plaintext highlighter-rouge">configs/coco_sample.py</code></p>

<h3 id="33-train-configuration">3.3 Train configuration</h3>

<p>There are multiple training parameters we can configure using mmdetection. For this simple demo we are going to use the default scheduler (see <code class="language-plaintext highlighter-rouge">configs/scheduler.py</code>). It uses SGD and a dynamic learning rate policy and that‚Äôs mostly what we need to know for now.</p>

<p>Our runtime definition is under <code class="language-plaintext highlighter-rouge">configs/runtime.py</code> and we are going to specify a few interesting things there:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">checkpoint_config</code>: specifies the checkpoint saving frequency</li>
  <li><code class="language-plaintext highlighter-rouge">log_config</code>: allows us to select a specific logger for our training</li>
  <li><code class="language-plaintext highlighter-rouge">custom_hooks</code>: extra hooks that we can insert or create for retrieving or adding functionalities to our training</li>
  <li><code class="language-plaintext highlighter-rouge">workflow</code>: it defines training workflow, this is, how many training epochs do we want to run before a validation one.</li>
</ul>

<p>Since we are using DVC, we are also going to use the <a href="https://dvc.org/doc/dvclive" target="_blank" rel="noopener noreferrer">DVCLive</a> hook. DVCLive is an open-source Python library for monitoring the progress of metrics during training of machine learning models. It is a recent and super cool library with git integration and that‚Äôs all I need! See how simple is to add this hook:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">log_config</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">(</span>
    <span class="n">interval</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">hooks</span><span class="o">=</span><span class="p">[</span>
        <span class="nf">dict</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="sh">'</span><span class="s">TextLoggerHook</span><span class="sh">'</span><span class="p">),</span>
        <span class="nf">dict</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="sh">"</span><span class="s">DvcliveLoggerHook</span><span class="sh">"</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="sh">"</span><span class="s">training/metrics</span><span class="sh">"</span><span class="p">),</span>
    <span class="p">])</span>
</code></pre></div></div>

<h2 id="4-data-pipeline">4. Data pipeline</h2>

<p>Let‚Äôs create our first <a href="https://dvc.org/doc/start/data-pipelines" target="_blank" rel="noopener noreferrer">data pipeline</a>! Ideally (and following DVC docs) we should use dvc run commands so the pipeline gets automatically generated but‚Ä¶ I feel more comfortable creating a dvc.yaml and filling it myself.</p>

<h3 id="41-prepare-the-data">4.1 Prepare the data</h3>

<p>Our COCO annotations are missing a few fields because fast.ai guys considered them unnecessary (they actually are) so they removed all extra fields to reduce the final size of the json. That‚Äôs fair enough, but mmdetection needs them so I created a very simple script that will prepare the data for us, you can find it in <code class="language-plaintext highlighter-rouge">src/prepare_data.py</code>.</p>

<p>The first step of our data pipeline will prepare our annotation file and save the modified COCO files into prepare_data. You can simply add the following to your <code class="language-plaintext highlighter-rouge">dvc.yaml</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">stages</span><span class="pi">:</span>
  <span class="na">prepare_data</span><span class="pi">:</span>
    <span class="na">foreach</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">train</span>
      <span class="pi">-</span> <span class="s">val</span>
    <span class="na">do</span><span class="pi">:</span>
      <span class="na">cmd</span><span class="pi">:</span> <span class="s">python src/prepare_data.py</span>
        <span class="s">--coco_file data/coco_sample/annotations/split_${item}.json</span>
        <span class="s">--output_file processed_data/${item}_split_with_ann_id.json</span>
      <span class="na">deps</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">data/coco_sample/annotations/split_${item}.json</span>
      <span class="na">outs</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">processed_data/${item}_split_with_ann_id.json</span>
</code></pre></div></div>

<p>There‚Äôs a stage called called prepare_data that run a small for loop over values <code class="language-plaintext highlighter-rouge">[train, val]</code> and calls the prepare_data script. See how I have specified the original json files as dependencies and the new one as outputs so DVC knows how to track them.</p>

<p>You can now call run the pipeline with <code class="language-plaintext highlighter-rouge">dvc repro</code> and the new annotations file should appear!</p>

<h3 id="42-train-the-model">4.2 Train the model</h3>

<p>I have created a simple training script in <code class="language-plaintext highlighter-rouge">src/train.py</code> that adjusts to our needs. You could also use <a href="https://github.com/open-mmlab/mmdetection/blob/master/tools/train.py" target="_blank" rel="noopener noreferrer">mmdetection train tool</a> since I just applied some minor modifications to it that will allow us to use dvc params.</p>

<p>We can add a new step to our data pipeline that executes our training step. For example this would be enough for running an experiment with our actual configs:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">train</span><span class="pi">:</span>
  <span class="na">cmd</span><span class="pi">:</span> <span class="s">python src/train.py</span>
    <span class="s">--dataset configs/coco_sample.py</span>
    <span class="s">--model configs/retinanet_r50_fpn.py</span>
    <span class="s">--schedule configs/schedule_1x.py</span>
    <span class="s">--runtime configs/runtime.py</span>
    <span class="s">--work_dir training/checkpoints</span>
  <span class="na">deps</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">configs/</span>
    <span class="pi">-</span> <span class="s">processed_data</span>
    <span class="pi">-</span> <span class="s">src/train.py</span>
  <span class="na">outs</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">training/checkpoints</span>
  <span class="na">live</span><span class="pi">:</span>
    <span class="na">training/metrics</span><span class="pi">:</span>
      <span class="na">summary</span><span class="pi">:</span> <span class="kc">true</span>
      <span class="na">html</span><span class="pi">:</span> <span class="kc">true</span>
</code></pre></div></div>

<p>Note how I have added the <code class="language-plaintext highlighter-rouge">live</code> key to notify DVC that our script will be saving metrics in the <code class="language-plaintext highlighter-rouge">training/metrics</code> folder. Also, this will generate a html file that we can use to visualize in real time our train progress. So simple!</p>

<p>We can run again DVC repro as many times as we want changing our config files as needed for trying different hyperparameters or model configurations. Nevertheless, DVC guys recommend yo to use <a href="https://dvc.org/doc/start/experiments" target="_blank" rel="noopener noreferrer">DVC experiments</a> when you are tryining different configurations. So that‚Äôs what we are going to do! Note this is a recent feature and I had to open a couple issues since I found a couple ‚Äúbugs‚Äù or unexpected behavior such <a href="https://github.com/iterative/dvc/issues/6465">[1]</a>, <a href="https://github.com/iterative/dvc/issues/5477?notification_referrer_id=MDE4Ok5vdGlmaWNhdGlvblRocmVhZDE1OTA3ODE1MTM6MTU5Njk2Njc%3D#issuecomment-905234950">[2]</a>.</p>

<p>Let‚Äôs do our first training by running <code class="language-plaintext highlighter-rouge">dvc exp run</code>! You can monitor training progress by opening your <code class="language-plaintext highlighter-rouge">training/metrics.html</code> file:</p>

<table>
  <tbody>
    <tr>
      <td><img src="/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/loss.webp" alt="loss curve" target="_blank" rel="noopener noreferrer" /></td>
      <td><img src="/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/vmap.webp" alt="vmap curve" target="_blank" rel="noopener noreferrer" /></td>
    </tr>
  </tbody>
</table>

<p>Training will be done soon (depening on your GPU and machine) and we can check our results by running:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>dvc exp show
<span class="go">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚îÉ Experiment              ‚îÉ Created  ‚îÉ  step ‚îÉ learning_rate ‚îÉ momentum ‚îÉ val.bbox_mAP ‚îÉ val.bbox_mAP_50 ‚îÉ val.bbox_mAP_75 ‚îÉ val.bbox_mAP_s ‚îÉ val.bbox_mAP_m ‚îÉ val.bbox
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚îÇ workspace               ‚îÇ -        ‚îÇ 13057 ‚îÇ         1e-05 ‚îÇ      0.9 ‚îÇ        0.198 ‚îÇ           0.368 ‚îÇ           0.194 ‚îÇ          0.008 ‚îÇ          0.142 ‚îÇ         
‚îÇ main                    ‚îÇ 11:24 AM ‚îÇ     - ‚îÇ             - ‚îÇ        - ‚îÇ            - ‚îÇ               - ‚îÇ               - ‚îÇ              - ‚îÇ              - ‚îÇ         
‚îÇ ‚îî‚îÄ‚îÄ cefe59e [exp-acc34] ‚îÇ 01:20 PM ‚îÇ 13057 ‚îÇ         1e-05 ‚îÇ      0.9 ‚îÇ        0.198 ‚îÇ           0.368 ‚îÇ           0.194 ‚îÇ          0.008 ‚îÇ          0.142 ‚îÇ         
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
</span></code></pre></div></div>

<p>The possibilty of tracking your hyperparameters is what I most like about experiments. We can change one hyperparameter for a experiment, DVC will remember this for you and will help you to compare different experiments. This is nice but there is room for improvement since at this moment for running an experiment with a different parameter we need to:</p>

<ol>
  <li>Add our parameter to our <code class="language-plaintext highlighter-rouge">params.yaml</code></li>
  <li>Specify that our train step depends on this parameter</li>
  <li>Run experiment with -S flag updating the parameter value.</li>
</ol>

<p>These steps are fine when you just change the learning rate or the number of epochs. Nevertheless I consider it does not scales to complex settings where you try a few dozens of different hyperparameters‚Ä¶ There is <a href="https://github.com/iterative/dvc/issues/5477#issuecomment-905440724" target="_blank" rel="noopener noreferrer">an open issue</a> where I shared my personal opinion, you can go there and read different thinkings since there is a small discussion going on about this new feature.</p>

<p>Let‚Äôs increase our L2 regularization or weight decay to see how it affects our results:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># dvc.yaml</span>
<span class="na">train</span><span class="pi">:</span>
  <span class="na">cmd</span><span class="pi">:</span> <span class="s">python src/train.py</span>
    <span class="s">--dataset configs/coco_sample.py</span>
    <span class="s">--model configs/retinanet_r50_fpn.py</span>
    <span class="s">--schedule configs/schedule_1x.py</span>
    <span class="s">--runtime configs/runtime.py</span>
    <span class="s">--work_dir training/checkpoints</span>
  <span class="na">deps</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">configs/</span>
    <span class="pi">-</span> <span class="s">processed_data</span>
    <span class="pi">-</span> <span class="s">src/train.py</span>
  <span class="na">outs</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">training/checkpoints</span>
  <span class="na">live</span><span class="pi">:</span>
    <span class="na">training/metrics</span><span class="pi">:</span>
      <span class="na">summary</span><span class="pi">:</span> <span class="kc">true</span>
      <span class="na">html</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">params</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">optimizer.weight_decay</span>
</code></pre></div></div>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># params.yaml</span>
<span class="na">optimizer</span><span class="pi">:</span>
  <span class="na">weight_decay</span><span class="pi">:</span> <span class="m">0.001</span> <span class="c1"># this is the same value we have in configs/schedule_1x.py</span>
</code></pre></div></div>

<p>Now we run the experiment with our new value for <code class="language-plaintext highlighter-rouge">weight_decay</code></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dvc exp run <span class="nt">-S</span> optimizer.weight_decay<span class="o">=</span>0.001
</code></pre></div></div>

<p>Experiment will start running and once is finished we can compare our results by running:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>dvc exp show <span class="nt">--no-timestamp</span> <span class="nt">--include-metrics</span> val.bbox_mAP <span class="nt">--include-metrics</span> val.bbox_mAP_50
<span class="go">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ Experiment              ‚îÉ val.bbox_mAP ‚îÉ val.bbox_mAP_50 ‚îÉ optimizer.weight_decay ‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ workspace               ‚îÇ          0.2 ‚îÇ           0.371 ‚îÇ 0.001                  ‚îÇ
‚îÇ main                    ‚îÇ            - ‚îÇ               - ‚îÇ -                      ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ cefe59e [exp-acc34] ‚îÇ        0.198 ‚îÇ           0.368 ‚îÇ 0.0001                 ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ 36da522 [exp-6d4ed] ‚îÇ          0.2 ‚îÇ           0.371 ‚îÇ 0.001                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</span></code></pre></div></div>

<p>Note I have filtered some of the metrics using <code class="language-plaintext highlighter-rouge">--include-metrics</code> flag so can easily see the most important ones. It seems that the ‚Äòweight_decay` parameter has an impact on results, and we have been to increase our mAP by 0.01. For this, let‚Äôs keep this experiment and commit our changes:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dvc exp apply exp-6d4ed 
git add src/train.py dvc.lock params.yaml dvc.yaml configs processed_data training/metrics.json
git commit <span class="nt">-a</span> <span class="nt">-m</span> <span class="s2">"Save experiment with </span><span class="sb">`</span><span class="nv">weight_decay</span><span class="o">=</span>.001<span class="sb">`</span><span class="s2">"</span>
</code></pre></div></div>

<p>And finally we can send all our results to respective remotes! These are Github and our Gdrive:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git push
dvc push
</code></pre></div></div>

<p>We have covered most of the step of the official <a href="https://dvc.org/doc/start/experiments" target="_blank" rel="noopener noreferrer">DVC experiments tutorial</a>. You can go there and check more info about how cleaning up your experiments and how to pull specific ones.</p>

<h3 id="43-results-visualization">4.3 Results Visualization</h3>

<p>We have trained our model and we have an idea of how it performs thanks to the mAP metrics but we all like to the the bounding boxes over our images so we can get a fully understanding of how the model performs! I have create a simple eval script in <code class="language-plaintext highlighter-rouge">src/eval.py</code> that will latest model and paint a subset of validation images. We simply need to add a new step to our <code class="language-plaintext highlighter-rouge">dvc.yaml</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">eval</span><span class="pi">:</span> 
  <span class="na">cmd</span><span class="pi">:</span> <span class="s">python src/eval.py</span>
    <span class="s">--dataset configs/coco_sample.py</span>
    <span class="s">--model configs/retinanet_r50_fpn.py</span>
    <span class="s">--checkpoint_file training/checkpoints/latest.pth</span>
    <span class="s">--output_dir eval/</span>  
    <span class="s">--n_samples </span><span class="m">20</span>
    <span class="s">--score_threshold .5</span>
  <span class="na">deps</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">src/eval.py</span>
    <span class="pi">-</span> <span class="s">training/checkpoints/latest.pth</span>
  <span class="na">outs</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">eval/</span>
</code></pre></div></div>

<p>I am going to run <code class="language-plaintext highlighter-rouge">dvc repro</code> since I have already commit and pushed my changes from last experiment. This is going to create the <code class="language-plaintext highlighter-rouge">eval</code> folder which contains the painted images, see a few examples below:</p>

<table>
  <tbody>
    <tr>
      <td><img src="/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/doggy.webp" alt="dog in couch" target="_blank" rel="noopener noreferrer" /></td>
      <td><img src="/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/fish.webp" alt="tv fishbowl" target="_blank" rel="noopener noreferrer" /></td>
    </tr>
    <tr>
      <td><img src="/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/tv.webp" alt="abandoned tv" target="_blank" rel="noopener noreferrer" /></td>
      <td><img src="/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/cat.webp" alt="lonely cat" target="_blank" rel="noopener noreferrer" /></td>
    </tr>
  </tbody>
</table>

<p>It seems our model is not perfect‚Ä¶ it mistook a fish tank for a TV! Anyway this was expected, the mAP metric is pretty low but even though we can see how it performs pretty well in the other images. You can go and check more results yourself but keep in mind that SOTA models in COCO dataset (80 classes) achieve a mAP ~0.6 and that‚Äôs a large difference wrt to our simple model. If you want to know more about COCO ranking I recommend you to check <a href="https://paperswithcode.com/sota/object-detection-on-coco" target="_blank" rel="noopener noreferrer">paperswithcode</a> web.</p>

<p>Once we have evaluated our model we can commit and push these results!</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git add dvc.lock .gitignore 
git commit <span class="nt">-m</span> <span class="s2">"Run dvc repro eval"</span>
git push
dvc push
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>We can use <code class="language-plaintext highlighter-rouge">DVC</code> combined with <code class="language-plaintext highlighter-rouge">mmdetection</code> to easily train object detection models, compare them and save different versions and experiments. Summarizing this post we have:</p>

<ul>
  <li>
    <p>Import a dataset into our repository using <code class="language-plaintext highlighter-rouge">dvc import</code></p>
  </li>
  <li>
    <p>Setup a <code class="language-plaintext highlighter-rouge">DVC</code> data pipeline and understand how it works</p>
  </li>
  <li>Train a model using <code class="language-plaintext highlighter-rouge">mmdetection</code>:
    <ul>
      <li>Understand <code class="language-plaintext highlighter-rouge">mmdetection</code> config files</li>
      <li>Take advantage of DVC metrics to configure our trainings and compare experiments</li>
      <li>Use <code class="language-plaintext highlighter-rouge">dvclive</code> to monitor the training progress</li>
    </ul>
  </li>
  <li>Obtain bbox predictions and paint them over our validation set</li>
</ul>

<p><em>Any ideas for future posts or is there something you would like to comment? Please feel free to reach out via <a href="https://twitter.com/mmeendez8" target="_blank" rel="noopener noreferrer">Twitter</a> or <a href="https://github.com/mmeendez8" target="_blank" rel="noopener noreferrer">Github</a></em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Training a pytorch object detection model with mmdetection combined with DVC (Data Version Control) allows you to version your code, checkpoints and experiments. Learn how to do it and maximize the reproducibility of your experiments!]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mmeendez8.github.io/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/thumbnail.jpg" /><media:content medium="image" url="https://mmeendez8.github.io/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/thumbnail.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Export to Github Cache with Docker Buildx</title><link href="https://mmeendez8.github.io/2021/07/19/new-docker-cache-is-out.html" rel="alternate" type="text/html" title="Export to Github Cache with Docker Buildx" /><published>2021-07-19T00:00:00+00:00</published><updated>2021-07-19T00:00:00+00:00</updated><id>https://mmeendez8.github.io/2021/07/19/new-docker-cache-is-out</id><content type="html" xml:base="https://mmeendez8.github.io/2021/07/19/new-docker-cache-is-out.html"><![CDATA[<p>I have recently <a href="/2021/04/23/cache-docker.html" target="_blank" rel="noopener noreferrer">uploaded a post</a> with some tricks for reducing the time you spend when building Docker images on Github Actions. That did indeed work pretty well for me until now, but it was a naive solution while waiting for <a href="https://docs.docker.com/buildx/working-with-buildx/" target="_blank" rel="noopener noreferrer">Docker BuildX</a> integration with Github cache. The wait is over and we do not need to manually cache files since Docker BuildX will do everything as we expected!</p>

<h2 id="1-get-the-basics">1. Get the basics</h2>

<p>You can read <a href="/2021/04/23/cache-docker.html" target="_blank" rel="noopener noreferrer">my previous post</a> to get the whole picture but I also recommend you to visit the official pull requests that lead to this new feature:</p>

<ul>
  <li><a href="https://github.com/docker/buildx/pull/535" target="_blank" rel="noopener noreferrer">This</a> is the buildx code that has been merged for allowing the use of github internal cache</li>
  <li><a href="https://github.com/docker/build-push-action/pull/406#issuecomment-879184394" target="_blank" rel="noopener noreferrer">This</a> draft PR contains an example of how to use it.</li>
</ul>

<p>If you read through all of those you probably realize that we have been waiting for new buildx 0.6 version and buildkit 0.9 to be generally available‚Ä¶ but that happened just a few days ago!</p>

<center>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">New BuildKit v0.9.0 and Docker Buildx v0.6.0 releases are out with Github cache backend, OpenTelemetry support, Dockerfile Here-docs, better errors, variable support in mount flags etc. <a href="https://t.co/uo89yvSo5j">https://t.co/uo89yvSo5j</a> <a href="https://t.co/L0QM7stmC5">https://t.co/L0QM7stmC5</a></p>&mdash; T√µnis Tiigi (@tonistiigi) <a href="https://twitter.com/tonistiigi/status/1416161830469201920?ref_src=twsrc%5Etfw">July 16, 2021</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>

<p>At this moment we are waiting for Github virtual environments to have new buildx 0.6.0 in Ubuntu base images, but they are generated on weekends and deployed during the week so we might have to wait a week or two before that happens. Anyway we can already test the new feature and add it to our pipelines!</p>

<h3 id="edit-91121">Edit 9/11/21:</h3>

<p>Github virtual environments have been updated so we can use build-push action without extra configurations.</p>

<h2 id="2-simple-example">2. Simple example</h2>

<p>I updated my CI pipeline to support the new feature. I can now remove all conditionals that I was using before to reduce building time when Dockerfile or conda.yaml were not modified. The simplified pipeline would look like this:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">name</span><span class="pi">:</span> <span class="s">Continuous Integration new cache</span>

<span class="na">on</span><span class="pi">:</span>
  <span class="na">push</span><span class="pi">:</span>
    <span class="na">branches</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">main</span><span class="pi">]</span>
  <span class="na">pull_request</span><span class="pi">:</span>
    <span class="na">branches</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">main</span><span class="pi">]</span>

<span class="na">jobs</span><span class="pi">:</span>
  <span class="na">build_docker</span><span class="pi">:</span>
    <span class="na">runs-on</span><span class="pi">:</span> <span class="s">ubuntu-latest</span>

    <span class="na">steps</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">uses</span><span class="pi">:</span> <span class="s">actions/checkout@v2</span>

      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Set up Docker Buildx</span>
        <span class="na">uses</span><span class="pi">:</span> <span class="s">docker/setup-buildx-action@v1</span>

      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Login to GitHub Container Registry</span>
        <span class="na">uses</span><span class="pi">:</span> <span class="s">docker/login-action@v1</span>
        <span class="na">with</span><span class="pi">:</span>
          <span class="na">registry</span><span class="pi">:</span> <span class="s">ghcr.io</span>
          <span class="na">username</span><span class="pi">:</span> <span class="s">mmeendez8</span>
          <span class="na">password</span><span class="pi">:</span> <span class="s">$</span>

      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Build production image</span>
        <span class="na">uses</span><span class="pi">:</span> <span class="s">docker/build-push-action@v2</span>
        <span class="na">with</span><span class="pi">:</span>
          <span class="na">context</span><span class="pi">:</span> <span class="s">.</span>
          <span class="na">file</span><span class="pi">:</span> <span class="s">Dockerfile</span>
          <span class="na">push</span><span class="pi">:</span> <span class="kc">true</span>
          <span class="na">tags</span><span class="pi">:</span> <span class="s">ghcr.io/mmeendez8/cache_docker/ci_dlc:latest</span>
          <span class="na">cache-from</span><span class="pi">:</span> <span class="s">type=gha</span>
          <span class="na">cache-to</span><span class="pi">:</span> <span class="s">type=gha,mode=max</span>

  <span class="na">lint_and_test</span><span class="pi">:</span>
    <span class="na">needs</span><span class="pi">:</span> <span class="s">build_docker</span>
    <span class="na">runs-on</span><span class="pi">:</span> <span class="s">ubuntu-latest</span>
    <span class="na">container</span><span class="pi">:</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">ghcr.io/mmeendez8/cache_docker/ci_dlc:latest</span>
      <span class="na">credentials</span><span class="pi">:</span>
        <span class="na">username</span><span class="pi">:</span> <span class="s">mmeendez8</span>
        <span class="na">password</span><span class="pi">:</span> <span class="s">$</span>

    <span class="na">steps</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">uses</span><span class="pi">:</span> <span class="s">actions/checkout@v2</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Lint code</span>
        <span class="na">run</span><span class="pi">:</span> <span class="pi">|</span>
          <span class="s">pre-commit install</span>
          <span class="s">pre-commit run</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Run tests</span>
        <span class="na">run</span><span class="pi">:</span> <span class="s">pytest tests</span>
</code></pre></div></div>

<p>Note how <code class="language-plaintext highlighter-rouge">cache-from</code> and <code class="language-plaintext highlighter-rouge">cache-to</code> type is set now to <code class="language-plaintext highlighter-rouge">gha</code> (github action). The first time the action is triggered the cache is empty so Docker will need to build all layers from scratch:</p>

<div class="post-center-image">
<a href="/assets/images/fullsize/posts/2021-07-19-new-docker-cache-is-out/empty_cache.jpg">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2021-07-19-new-docker-cache-is-out/empty_cache-677-4bba7b45f.jpg" alt="Image showing empty cache" srcset="/generated/assets/images/fullsize/posts/2021-07-19-new-docker-cache-is-out/empty_cache-400-c943f5a58.webp 400w, /generated/assets/images/fullsize/posts/2021-07-19-new-docker-cache-is-out/empty_cache-600-c943f5a58.webp 600w, /generated/assets/images/fullsize/posts/2021-07-19-new-docker-cache-is-out/empty_cache-677-c943f5a58.webp 677w" sizes="(max-width: 767px) 100vw, 80vw" width="677" height="415" />
</a>

</div>

<p>But after this cache is full, so we can reuse all our layers in next builds, if the images was not modified, or just some of them when we apply changes to our Dockerfile. Let‚Äôs trigger a new build with an empty commit and check the time it needs now:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git commit <span class="nt">--allow-empty</span> <span class="nt">-m</span> <span class="s2">"Test build"</span>
git push
</code></pre></div></div>

<div class="post-center-image">
<a href="/assets/images/fullsize/posts/2021-07-19-new-docker-cache-is-out/full_cache.jpg">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2021-07-19-new-docker-cache-is-out/full_cache-673-3df8281a5.jpg" alt="Image showing full cache" srcset="/generated/assets/images/fullsize/posts/2021-07-19-new-docker-cache-is-out/full_cache-400-70343b0e6.webp 400w, /generated/assets/images/fullsize/posts/2021-07-19-new-docker-cache-is-out/full_cache-600-70343b0e6.webp 600w, /generated/assets/images/fullsize/posts/2021-07-19-new-docker-cache-is-out/full_cache-673-70343b0e6.webp 673w" sizes="(max-width: 767px) 100vw, 80vw" width="673" height="420" />
</a>

</div>

<p>That‚Äôs it! It only took 22 seconds to build our image.</p>

<h2 id="conclusion">Conclusion</h2>

<ul>
  <li>
    <p>Docker Buildx is a powerful enhancement and we should try to take full advantage of it</p>
  </li>
  <li>
    <p>It is very simple to use Github Cache with build-push-action now</p>
  </li>
</ul>

<p><em>Any ideas for future posts or is there something you would like to comment? Please feel free to reach out via <a href="https://twitter.com/mmeendez8" target="_blank" rel="noopener noreferrer">Twitter</a> or <a href="https://github.com/mmeendez8" target="_blank" rel="noopener noreferrer">Github</a></em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Github actions cache is integrated with Docker buildx. Learn how to create a simple pipeline using build-push action and Github Cache. Test the new buildx cache-to exporter!]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mmeendez8.github.io/assets/images/fullsize/posts/2021-07-19-new-docker-cache-is-out/thumbnail.jpg" /><media:content medium="image" url="https://mmeendez8.github.io/assets/images/fullsize/posts/2021-07-19-new-docker-cache-is-out/thumbnail.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Version control your dataset with DVC</title><link href="https://mmeendez8.github.io/2021/07/01/dvc-tutorial.html" rel="alternate" type="text/html" title="Version control your dataset with DVC" /><published>2021-07-01T00:00:00+00:00</published><updated>2021-07-01T00:00:00+00:00</updated><id>https://mmeendez8.github.io/2021/07/01/dvc-tutorial</id><content type="html" xml:base="https://mmeendez8.github.io/2021/07/01/dvc-tutorial.html"><![CDATA[<p><a href="https://dvc.org/" target="_blank" rel="noopener noreferrer">Data Version Control (DVC)</a> is one of the most amazing projects in recent years. Before using it, we used to have trouble reproducing our models and experiments. We store our images and annotations in high-volume network attached storage where multiple people work every day, so there was no proper way to modify images or annotations while maintaining a correct and reproducible change history. You can imagine how often someone accidentally deletes an image, modifies some annotations, or infinitely more random problems that ended in trouble for properly reproducing our experiments.</p>

<p>In this post I will try to show how to configure DVC and how it can help us to maintain version of our datasets that can be easily integrated with Github.</p>

<h2 id="why-dvc">Why DVC?</h2>

<p>The first thing we should do is understanding how DVC works, we can check its <a href="https://dvc.org/" target="_blank" rel="noopener noreferrer">landing page</a> which contains some intuitive explanations:</p>

<ul>
  <li>Machine Learning projects are defined by code and data</li>
  <li>We know how to track code using git</li>
  <li>DVC main target is to built something similar to git for tracking data and models. It must be:
    <ul>
      <li>Flexible</li>
      <li>Easy to learn</li>
      <li>Work with any ML framework</li>
    </ul>
  </li>
</ul>

<p>I always recommend to watch this introduction video that greatly summarizes the main idea behind this tool.</p>

<center>
    <iframe loading="lazy" width="560" height="315" src="https://www.youtube.com/embed/UbL7VUpv1Bs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">
    </iframe>
</center>

<p>If you paid attention, you will probably have a general idea of how this large files are tracked. Light-weight files (.dvc files) are used as ‚Äúpointers‚Äù to large files, so we can use git to track those pointers and then retrieve the associated large files in our local filesystem with dvc. In other words, you do not need to worry anymore about uploading large files to git (forget about LFS), DVC will handle everything for you once it is properly configured. That‚Äôs pretty cool isn‚Äôt it?</p>

<h2 id="setting-up-the-environment">Setting up the environment</h2>

<p>I have pushed all changes to a <a href="https://github.com/mmeendez8/coco_sample/" target="_blank" rel="noopener noreferrer">Github repository</a> that you can consult in case you need.</p>

<h3 id="install-dvc">Install DVC</h3>

<p>Let‚Äôs setup DVC for our experiment. If you check their <a href="https://dvc.org/doc/install/linux#installation-on-linux" target="_blank" rel="noopener noreferrer">installation guide</a>, you will realize that depending on the type of remote storage you plan to use, it might be necessary to install additional deps. For this tutorial we are going to use Google Drive because it is probably the most accesible to everyone. I always use Conda environment for Python package management, do the following for creating a new environment and installing dvc on it:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda  create <span class="nt">-n</span> coco_sample <span class="nv">python</span><span class="o">=</span>3.8 <span class="nt">-y</span>
conda activate coco_sample
pip <span class="nb">install </span>dvc[gdrive]
</code></pre></div></div>

<h3 id="get-the-data">Get the data</h3>

<p>We are going to use a <a href="https://course.fast.ai/datasets#coco" target="_blank" rel="noopener noreferrer">subset of the COCO dataset created by fast.ai</a>. The fast.ai subset contains all images that contain one of five selected categories, restricting objects to just those five categories; the categories are: chair couch tv remote book vase. You can download and extract it using the following commands:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget https://s3.amazonaws.com/fast-ai-coco/coco_sample.tgz <span class="nt">-P</span> data
<span class="nb">tar </span>zxvf data/coco_sample.tgz <span class="nt">-C</span> data
<span class="nb">rm  </span>data/coco_sample.tgz
</code></pre></div></div>

<p>You should now have all images in <code class="language-plaintext highlighter-rouge">data/coco_sample/train_sample</code> and their corresponding annotations in <code class="language-plaintext highlighter-rouge">data/coco_sample/annotations/train_sample.json</code></p>

<h3 id="visualize-our-data">Visualize our data</h3>

<p>It is always good to take a look at the data to get an idea of what kind of images we are dealing with. We are going to use our tool <a href="https://github.com/Gradiant/pyodi" target="_blank" rel="noopener noreferrer">pyodi</a>, which allows us to retrieve annotations from a COCO formatted file and paint then over the corresponding image. We can install it using pip and run the paint-annotations script pointing to our data and annotations folder:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>pyodi
pyodi paint-annotations data/coco_sample/annotations/train_sample.json data/coco_sample/train_sample output/painted_images <span class="nt">--first_n</span> 10
</code></pre></div></div>

<p>This will paint the first 10 images of the dataset and save them into <code class="language-plaintext highlighter-rouge">output/painted_images</code> folder.</p>

<div class="post-center-image">
<a href="/assets/images/fullsize/posts/2021-07-01-dvc-tutorial/painted_coco_sample.jpg">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2021-07-01-dvc-tutorial/painted_coco_sample-666-8250e9564.jpg" alt="Coco image with painted detections" srcset="/generated/assets/images/fullsize/posts/2021-07-01-dvc-tutorial/painted_coco_sample-400-6775c1de5.webp 400w, /generated/assets/images/fullsize/posts/2021-07-01-dvc-tutorial/painted_coco_sample-600-6775c1de5.webp 600w, /generated/assets/images/fullsize/posts/2021-07-01-dvc-tutorial/painted_coco_sample-666-6775c1de5.webp 666w" sizes="(max-width: 767px) 100vw, 80vw" width="666" height="500" />
</a>

</div>

<h2 id="version-the-data">Version the data</h2>

<p>If we follow <a href="https://dvc.org/doc/start" target="_blank" rel="noopener noreferrer">DVC get started page</a>, we need to initialize the project running <code class="language-plaintext highlighter-rouge">dvc init</code> so let‚Äôs run that first and commit those internal files to github.com</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git init
dvc init
git commit <span class="nt">-m</span> <span class="s2">"Initialize DVC"</span>
</code></pre></div></div>

<p>Let‚Äôs now add our data and annotations to DVC.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dvc add data
</code></pre></div></div>

<p>This may take a while since it needs to compute hashes for all files in our data directory. Two files are generated, the <code class="language-plaintext highlighter-rouge">.gitignore</code> that will inform git that it must ignore the <code class="language-plaintext highlighter-rouge">data</code>/ directory and <code class="language-plaintext highlighter-rouge">data.dvc</code>, a small text file in a human-readable format used by dvc to track changes that we can upload to git. Let‚Äôs add can add this files and commit the new changes.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git add .gitignore data.dvc 
git commit <span class="nt">-m</span> <span class="s2">"Added data to dvc"</span>
</code></pre></div></div>

<p>Last step will be to configure our Google Drive remote. For this, I created a new folder in my personal gdrive called <code class="language-plaintext highlighter-rouge">dvc_data</code> so you should do the same in your own gdrive. After that I will add that folder as a dvc remote and I will need to grant DVC the necessary permissions to access my gdrive account. This can be easily done after running <code class="language-plaintext highlighter-rouge">dvc push</code> following the link that promts when running the command. Have in mind that the push step may take a while to complete depending on your internet connection.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dvc remote add <span class="nt">-d</span> gdrive-remote gdrive://root/dvc_data
git commit .dvc/config <span class="nt">-m</span> <span class="s2">"Configure local remote"</span> <span class="c"># Commit changes to git</span>
dvc push <span class="c"># Upload data to gdrive</span>
</code></pre></div></div>

<p>We are now tracking our images and annotations with DVC and have pushed it to our google drive remote storage.</p>

<h2 id="split-the-data">Split the data</h2>

<p>We have a file <code class="language-plaintext highlighter-rouge">train_sample.json</code> that contains all our annotations. We need to split this file in training and validation subsets so we can properly train our model in a near future. We are going to use pyodi‚Äôs <a href="https://gradiant.github.io/pyodi/reference/apps/coco-split/" target="_blank" rel="noopener noreferrer">coco random-split</a> app for this task, since it is very easy to execute.
Let‚Äôs reserve a 20% of the total data for validation:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pyodi coco random-split data/coco_sample/annotations/train_sample.json data/coco_sample/annotations/split <span class="nt">--val-percentage</span> 0.2
</code></pre></div></div>

<p>This creates two new files <code class="language-plaintext highlighter-rouge">split_train.json</code>and <code class="language-plaintext highlighter-rouge">split_val.json</code>. Let‚Äôs add them to dvc, that will magically recognize that we have only added two new files, and commit changes to git:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dvc add data/
git commit data.dvc <span class="nt">-m</span> <span class="s2">"Add dataset splits"</span>
dvc push
</code></pre></div></div>

<h2 id="tag-our-data-with-versions">Tag our data with versions</h2>

<p>You must bear in mind that if we now move through our git history, we can also retrieve the state of our data in that commit or specific moment thanks to DVC. Anyway, I always like to tag the versions of my dataset so that I can easily understand what the state was at all times. Let‚Äôs do this now and tag the initial version of our dataset and push our changes to github.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git tag <span class="nt">-a</span> <span class="s2">"v1.0"</span> <span class="nt">-m</span> <span class="s2">"Initial version, fast.ai COCO subset. 0.2 validation split"</span>
git push <span class="nt">--tags</span>
</code></pre></div></div>

<p>Let‚Äôs imagine that for any reason we need to create a new version of our dataset saving only ten percent of our data as validation. We could simply run pyodi, override our actual annotations and push the new changes without fear to loose any data. Let‚Äôs try that:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pyodi coco random-split data/coco_sample/annotations/train_sample.json data/coco_sample/annotations/split <span class="nt">--val-percentage</span> 0.1
dvc add data/
git commit data.dvc <span class="nt">-m</span> <span class="s2">"New split with 0.1 validation"</span>
dvc push
git tag <span class="nt">-a</span> <span class="s2">"v2.0"</span> <span class="nt">-m</span> <span class="s2">"0.2 Validation split"</span>
git push <span class="nt">--tags</span>
</code></pre></div></div>

<p>That‚Äôs it we now have two version of our dataset. If we want to move to our previous version we can simply run:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git checkout v1.0
dvc checkout
</code></pre></div></div>

<h2 id="continuous-integration-for-data">Continuous Integration for data</h2>

<p>We have used DVC for track and save data the same way we do with code. So we can also add some test to our data to make sure that we do not commit any error that can harm our training. Since we have used Google Drive as a remote storage for our data, we can configure our CI pipeline to download our data from there and run our tests.</p>

<p>First of all we need to store our gdrive credentials as a Github secret. You can go to you repository settings in Github and create a new secret named <code class="language-plaintext highlighter-rouge">GDRIVE_CREDENTIALS_DATA</code>, and paste there the contents of your <code class="language-plaintext highlighter-rouge">.dvc/tmp/gdrive-user-credentials.json</code>. This file should have been automatically created after you give DVC permissions to your Google Drive account. You can read more about this in <a href="https://dvc.org/doc/user-guide/setup-google-drive-remote#authorization" target="_blank" rel="noopener noreferrer">DVC documentation</a>.</p>

<p>Let‚Äôs create an example test in charge of checking that our annotations follow COCO format guidelines. We can use pydantic for data validation defining how annotation should be using python type annotations. We now for example that categories or our dataset can only take six different values and bounding boxes must be a list of four integers. Pydantic allows us to define this rules in a very efficient an flexible manner.</p>

<p>Create a new file <code class="language-plaintext highlighter-rouge">tests/test_annotations.py</code> and paste the following content:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">json</span>
<span class="kn">from</span> <span class="n">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="n">pytest</span>

<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">get_args</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Literal</span>
<span class="kn">from</span> <span class="n">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">conint</span><span class="p">,</span> <span class="n">confloat</span><span class="p">,</span> <span class="n">conlist</span>

<span class="n">CATEGORY_NAMES</span> <span class="o">=</span> <span class="n">Literal</span><span class="p">[</span><span class="sh">"</span><span class="s">chair</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">couch</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">tv</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">remote</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">book</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">vase</span><span class="sh">"</span><span class="p">]</span> 

<span class="k">class</span> <span class="nc">COCOCategory</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="nb">id</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">name</span><span class="p">:</span> <span class="n">CATEGORY_NAMES</span>


<span class="k">class</span> <span class="nc">COCOImage</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="nb">id</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">file_name</span><span class="p">:</span> <span class="nb">str</span>


<span class="k">class</span> <span class="nc">COCOAnnotation</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">image_id</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">bbox</span><span class="p">:</span> <span class="nf">conlist</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">min_items</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">max_items</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">category_id</span><span class="p">:</span> <span class="nb">int</span>


<span class="k">class</span> <span class="nc">COCODetectionDataset</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">images</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">COCOImage</span><span class="p">]</span>
    <span class="n">annotations</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">COCOAnnotation</span><span class="p">]</span>
    <span class="n">categories</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">COCOCategory</span><span class="p">]</span>


<span class="nd">@pytest.mark.parametrize</span><span class="p">(</span><span class="sh">"</span><span class="s">split</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">val</span><span class="sh">"</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">test_coco_format</span><span class="p">(</span><span class="n">split</span><span class="p">):</span>

    <span class="n">annotations_file</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">data/coco_sample/annotations/split_</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s">.json</span><span class="sh">"</span>
    
    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">annotations_file</span><span class="p">,</span> <span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="nc">COCODetectionDataset</span><span class="p">(</span><span class="o">**</span><span class="n">json</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">f</span><span class="p">))</span>

    <span class="c1"># Check image ids are unique
</span>    <span class="n">image_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">img</span><span class="p">.</span><span class="nb">id</span> <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">.</span><span class="n">images</span><span class="p">]</span>
    <span class="n">image_ids_set</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">image_ids</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nf">len</span><span class="p">(</span><span class="n">image_ids</span><span class="p">)</span> <span class="o">==</span> <span class="nf">len</span><span class="p">(</span><span class="n">image_ids_set</span><span class="p">)</span>

    <span class="c1"># Check annotation ids are unique
</span>    <span class="n">categories</span> <span class="o">=</span> <span class="p">[</span><span class="n">cat</span><span class="p">.</span><span class="nb">id</span> <span class="k">for</span> <span class="n">cat</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">.</span><span class="n">categories</span><span class="p">]</span>
    <span class="n">categories_set</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">categories</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nf">len</span><span class="p">(</span><span class="n">categories</span><span class="p">)</span> <span class="o">==</span> <span class="nf">len</span><span class="p">(</span><span class="n">categories_set</span><span class="p">)</span>

    <span class="c1"># Check each annotation corresponds with existent image
</span>    <span class="k">for</span> <span class="n">annotation</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">.</span><span class="n">annotations</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">annotation</span><span class="p">.</span><span class="n">image_id</span> <span class="ow">in</span> <span class="n">image_ids_set</span>
</code></pre></div></div>

<p>Note we added a test that will ensure that:</p>

<ul>
  <li>Loaded data is validated with pydantic</li>
  <li>Image ids are unique</li>
  <li>Category ids are unique</li>
  <li>All annotations are associated with an existent image id</li>
</ul>

<p>We have the test so we need to define a workflow that automatically runs this when we push some changes to our main branch. Create a new file <code class="language-plaintext highlighter-rouge">.github/workflows/ci.yaml</code> and paste the following code:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">name</span><span class="pi">:</span> <span class="s">Continuous Integration</span>

<span class="na">on</span><span class="pi">:</span>
  <span class="na">push</span><span class="pi">:</span>
    <span class="na">branches</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">main</span><span class="pi">]</span>

<span class="na">jobs</span><span class="pi">:</span>
  <span class="na">test</span><span class="pi">:</span>
    <span class="na">runs-on</span><span class="pi">:</span> <span class="s">ubuntu-latest</span>
    <span class="na">steps</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">uses</span><span class="pi">:</span> <span class="s">actions/checkout@v2</span>
      <span class="pi">-</span> <span class="na">uses</span><span class="pi">:</span> <span class="s">iterative/setup-dvc@v1</span> 

      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Get annotations from dvc remote</span>
        <span class="na">run</span><span class="pi">:</span> <span class="s">dvc pull data/coco_sample/annotations</span> 
        <span class="na">env</span><span class="pi">:</span>
          <span class="na">GDRIVE_CREDENTIALS_DATA</span><span class="pi">:</span> <span class="s">$</span>

      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Run tests</span>
        <span class="na">run</span><span class="pi">:</span> <span class="pi">|</span>
          <span class="s">pip install pytest pydantic</span>
          <span class="s">pytest tests</span>
</code></pre></div></div>

<p>This script is very intuitive. First we checkout our repo and setup DVC. After that we pull just the annotation files using using the command <code class="language-plaintext highlighter-rouge">dvc pull data/coco_sample/annotations</code>. Think that if we run <code class="language-plaintext highlighter-rouge">dvc pull</code> without extra argument we would have to wait before all images are downloaded and we do not need them for this type of test.</p>

<p>If we add, commit and push all this changes to Github we can see how our workflow triggers and runs the associated job:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git add .github tests
git commit <span class="nt">-m</span> <span class="s2">"Add tests"</span>
git push
</code></pre></div></div>

<div class="post-center-image">
<a href="/assets/images/fullsize/posts/2021-07-01-dvc-tutorial/ci.jpg">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2021-07-01-dvc-tutorial/ci-800-4fd35c01c.jpg" alt="Github CI results caption" srcset="/generated/assets/images/fullsize/posts/2021-07-01-dvc-tutorial/ci-400-58e8b520e.webp 400w, /generated/assets/images/fullsize/posts/2021-07-01-dvc-tutorial/ci-600-58e8b520e.webp 600w, /generated/assets/images/fullsize/posts/2021-07-01-dvc-tutorial/ci-800-58e8b520e.webp 800w, /generated/assets/images/fullsize/posts/2021-07-01-dvc-tutorial/ci-917-58e8b520e.webp 917w" sizes="(max-width: 767px) 100vw, 80vw" width="917" height="472" />
</a>

</div>

<p>That‚Äôs it! Our test completed successfully!</p>

<h2 id="conclusion">Conclusion</h2>

<p>DVC help us to keep version of our data and models. In this short post we have learnt a few things:</p>

<ul>
  <li>
    <p>We have learned how to setup DVC and add a Google Drive remote</p>
  </li>
  <li>
    <p>We have pushed our data to the DVC remote and tagged different versions of it</p>
  </li>
  <li>
    <p>We have added some simple tests for our data and how to set up a CI worflow that runs on Github servers.</p>
  </li>
</ul>

<p><em>Any ideas for future posts or is there something you would like to comment? Please feel free to reach out via <a href="https://twitter.com/mmeendez8" target="_blank" rel="noopener noreferrer">Twitter</a> or <a href="https://github.com/mmeendez8" target="_blank" rel="noopener noreferrer">Github</a></em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Learn how to use DVC with a Google Drive remote for tracking changes on your dataset and create Continuous Integration pipelines at Github to test your data as you do with code.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mmeendez8.github.io/assets/images/fullsize/posts/2021-07-01-dvc-tutorial/thumbnail.jpg" /><media:content medium="image" url="https://mmeendez8.github.io/assets/images/fullsize/posts/2021-07-01-dvc-tutorial/thumbnail.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>