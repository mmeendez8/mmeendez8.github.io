<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://miguel-mendez-ai.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://miguel-mendez-ai.com/" rel="alternate" type="text/html" /><updated>2024-09-01T10:23:31+00:00</updated><id>https://miguel-mendez-ai.com/feed.xml</id><entry><title type="html">Understanding MOT Metrics</title><link href="https://miguel-mendez-ai.com/2024/08/25/mot-tracking-metrics.html" rel="alternate" type="text/html" title="Understanding MOT Metrics" /><published>2024-08-25T00:00:00+00:00</published><updated>2024-08-25T00:00:00+00:00</updated><id>https://miguel-mendez-ai.com/2024/08/25/mot-tracking-metrics</id><content type="html" xml:base="https://miguel-mendez-ai.com/2024/08/25/mot-tracking-metrics.html"><![CDATA[<p>After a long time, I have finally sat down to write this blog post on tracking metrics. It builds on my last post about <a href="/2023/11/08/tracking-by-detection-overview.html">tracking by detection</a> and explores how we measure tracking performance.</p>

<p>In this article, I’ll provide an introduction to tracking metrics, starting from the basic principles and breaking down the key differences between various metrics. I’ll focus on three popular metrics: MOTA, IDF1, and HOTA, which are widely used in the <a href="https://paperswithcode.com/sota/multi-object-tracking-on-mot20-1" target="_blank" rel="noopener noreferrer">Multi-Object Tracking (MOT) community</a>. Understanding these is crucial because the choice of metric can significantly impact how we interpret a tracker’s performance.</p>

<p>Let’s get started!</p>

<h2 id="the-basics">The basics</h2>

<h3 id="hungarian-algorithm">Hungarian algorithm</h3>

<p>The Hungarian algorithm plays a crucial role for tracking metrics, primarily used to:</p>

<ol>
  <li>Optimize bipartite matching between detections and ground truth objects per frame</li>
  <li>Assign tracks to ground truth trajectories across the entire sequence</li>
</ol>

<p>Let’s for now focus on the first point. The algorithm matches predicted tracks to ground truth objects in each frame, maximizing overall IoU scores. This results in:</p>

<ul>
  <li><strong>TP</strong>: True Positives (matches with IoU above threshold)</li>
  <li><strong>FP</strong>: False Positives (unmatched predictions)</li>
  <li><strong>FN</strong>: False Negatives (unmatched ground truth objects)</li>
</ul>

<p>While a detailed explanation of the algorithm is beyond the scope of this post, understanding its basic function helps in grasping how these metrics work.
For a more in-depth explanation of the Hungarian algorithm, check out <a href="https://www.thinkautonomous.ai/blog/hungarian-algorithm/" target="_blank" rel="noopener noreferrer">this excellent tutorial</a>.</p>

<h3 id="deta">DetA</h3>

<p>The Detection Accuracy (DetA) measures how well a tracker localizes objects in each frame, typically using Intersection over Union (IoU) thresholds. It essentially quantifies the spatial accuracy of detections.</p>

<div class="post-center-image">
    <a href="/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/iou.png">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/iou-720-056a141c9.png" alt="IoU diagram" srcset="/generated/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/iou-400-cf6ad2ee9.webp 400w, /generated/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/iou-720-cf6ad2ee9.webp 720w" sizes="(max-width: 767px) 100vw, 80vw" width="720" height="342" />
</a>

</div>

<p class="image-caption"><em>Figure 1. IoU diagram <a href="https://jonathonluiten.medium.com/how-to-evaluate-tracking-with-the-hota-metrics-754036d183e1" target="_blank" rel="noopener noreferrer">from jonathanluiten</a></em></p>

<p>So once we have the TP, FP, and FN, we can compute the DetA as:</p>

\[\text{DetA} = \frac{\text{TP}}{\text{TP} + \text{FP} + \text{FN}}\]

<h3 id="assa">AssA</h3>

<p>The Association Accuracy (AssA), on the other hand, evaluates how accurately a tracker maintains object identities across frames. It focuses on the temporal consistency of ID assignments, measuring how well the tracker links detections of the same object over time. See, for example, the image below, extracted from HOTA <a href="#references">[1]</a>:</p>

<div class="post-center-image" style="max-width: 300px; margin: 0 auto;">
    <a href="/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/hota_assa_example.png">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/hota_assa_example-800-de2d31800.png" alt="AssA example from HOTA paper" srcset="/generated/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/hota_assa_example-400-805303a6a.webp 400w, /generated/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/hota_assa_example-800-805303a6a.webp 800w, /generated/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/hota_assa_example-953-805303a6a.webp 953w" sizes="(max-width: 767px) 100vw, 80vw" width="953" height="870" />
</a>

</div>

<p class="image-caption"><a id="figure-2"></a>
<em>Figure 2. Different association results example (from HOTA <a href="#references">[1]</a>)</em></p>

<p>We can observe different tracking results (A, B, C) for a single ground truth object (GT):</p>

<ul>
  <li><strong>A</strong>: Detects the object 50% of the time with consistent identity</li>
  <li><strong>B</strong>: Detects the object 70% of the time, but assigns two different identities</li>
  <li><strong>C</strong>: Detects the object 100% of the time, but assigns up to four different identities</li>
</ul>

<p>Which result is best? This is what the Association Accuracy (AssA) metric aims to determine. Different tracking metrics like MOTA, IDF1, and HOTA approach this question in various ways, each with its own methodology and emphasis on detection accuracy versus identity consistency.</p>

<h2 id="mota-multiple-object-tracking-accuracy">MOTA (Multiple Object Tracking Accuracy)</h2>

<p>MOTA introduces the concept of identity tracking to object detection metrics. It incorporates identity switches (IDSW), which occur when a single ground truth (GT) object is assigned to different track predictions over time.</p>

<p>The computation of MOTA involves temporal dependency, penalizing track assignment changes between consecutive frames. An IDSW is counted when a GT target $i$ matches track $j$ in the current frame but was matched to a different track $k$ ($k ≠ j$) in the previous frame.</p>

<p>In practice, the Hungarian matching algorithm is modified to minimize identity switches from the previous frame. In <a href="https://github.com/JonathonLuiten/TrackEval/blob/master/trackeval/metrics/clear.py#L81" target="_blank" rel="noopener noreferrer">TrackEval code</a> this is done using a simple gating trick:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">score</span> <span class="o">=</span> <span class="nc">IoU</span><span class="p">(</span><span class="n">GT</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
<span class="k">if</span> <span class="n">pred</span> <span class="o">==</span> <span class="nf">previous_assigned_id</span><span class="p">(</span><span class="n">GT</span><span class="p">):</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">score</span> <span class="o">*</span> <span class="mi">1000</span>
</code></pre></div></div>

<p>The MOTA metric is computed across all frames as:</p>

\[\text{MOTA} = 1 - \frac{\sum_t (FN_t + FP_t + IDSW_t)}{\sum_t GT_t}\]

<p>Where $t$ is the frame index, FN are False Negatives, FP are False Positives, IDSW are Identity Switches, and GT is the number of ground truth objects.</p>

<p>While MOTA’s simplicity is appealing, it has some limitations:</p>

<ol>
  <li>It only considers the <strong>previous frame</strong> for IDSW, so each switch is penalized <strong>only once</strong>, regardless of how long the incorrect assignment persists.</li>
  <li>It can be <strong>dominated by FP and FN</strong> in crowded scenes, making IDSW less impactful.</li>
  <li><strong>IoU threshold is fixed</strong> so more or less detection accuracy is not reflected on the metric</li>
</ol>

<h2 id="idf1">IDF1</h2>

<p>IDF1 addresses some of MOTA’s limitations by focusing on how long the tracker correctly identifies an object, rather than just counting errors. It’s based on the concept of Identification Precision (IDP) and Identification Recall (IDR).</p>

<p>It computes the assignment between prediction and ground truth objects across the entire video, rather than frame by frame.</p>

<p>The metric is simple:</p>

\[\text{IDF1} = \frac{2 * \text{IDTP}}{2 * \text{IDTP} + \text{IDFP} + \text{IDFN}}\]

<p>Where:</p>

<ul>
  <li><strong>IDTP</strong> (ID True Positive): The number of correctly identified detections</li>
  <li><strong>IDFP</strong> (ID False Positive): Tracker predictions that don’t match any ground truth</li>
  <li><strong>IDFN</strong> (ID False Negative): Ground truth trajectories that aren’t tracked</li>
</ul>

<p>The global assignment is computed using the Hungarian algorithm. It picks the best combination between prediction and ground truth that maximizes IDF1 for the whole video. It is easier to understand this by observing the image introduced in HOTA paper:</p>

<div class="post-center-image" style="max-width: 800px; margin: 0 auto;">
    <a href="/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/idf1.png">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/idf1-800-cacd2088c.png" alt="IDF1 metric diagram" srcset="/generated/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/idf1-400-e8f603c7d.webp 400w, /generated/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/idf1-800-e8f603c7d.webp 800w, /generated/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/idf1-1109-e8f603c7d.webp 1109w" sizes="(max-width: 767px) 100vw, 80vw" width="1109" height="664" />
</a>

</div>

<p class="image-caption"><em>Figure 3. IDF1 metric diagram</em></p>

<p>The main problem I see with IDF1 is finding the best one-to-one matching between predicted and ground truth trajectories for the entire sequence since it can oversimplify complex tracking scenarios:</p>

<p>Imagine a corner kick in football. A tracker might correctly follow Player A running into the box, lose them in a cluster, and then mistakenly pick up Player B after the ball is cleared. IDF1 might treat this as one partially correct track for either Player A or B, ignoring that it’s correct for different players at different times.</p>

<p>This simplification can misrepresent a tracker’s performance in complex situations like crowded football plays, where player interactions and occlusions are frequent.</p>

<p>Key advantages of IDF1:</p>

<ol>
  <li>It’s more sensitive to <strong>long-term tracking consistency</strong>.</li>
  <li>It balances <strong>precision and recall</strong> of identity predictions.</li>
  <li>It’s less affected by the number of objects in the scene than MOTA.</li>
</ol>

<p>However, IDF1 also has limitations:</p>

<ol>
  <li><strong>IDF1 can decrease when improving detection</strong>. Just avoiding FP can result in a better metric (A vs C in <a href="#figure-2">Figure 2</a>)</li>
  <li><strong>IoU threshold is fixed</strong> so more or less detection accuracy is not reflected on the metric</li>
</ol>

<p>More limitations are presented in the HOTA paper. I recommend you to have a read because it is very well explained and intuitive.</p>

<h2 id="hota-higher-order-tracking-accuracy">HOTA (Higher Order Tracking Accuracy)</h2>

<p>HOTA is a more recent metric designed to address the limitations of both MOTA and IDF1. It aims to provide a balanced assessment of detection and association performance. HOTA can be broken down into DetA (Detection Accuracy) and AssA (Association Accuracy), allowing separate analyses of these aspects.</p>

<p>The core HOTA formula is:</p>

\[\text{HOTA}_{\alpha} = \sqrt{\text{DetA}_{\alpha} \cdot \text{AssA}_{\alpha}}\]

<p>In this formula, the $\alpha$ term represents the different Intersection over Union (IoU) thresholds used to compute the metric. A True Positive (TP) is only considered when the match IoU score is above the given $\alpha$ threshold. The metric uses 19 different $\alpha$ values, ranging from 0.05 to 0.95 in increments of 0.05.</p>

<p>HOTA uses global alignment (high-order association) between predicted and ground truth detections, similar to IDF1, but also incorporates localization accuracy. This means that HOTA evaluates both the ability to detect objects accurately and to maintain correct associations over time.</p>

<p>The HOTA algorithm can be summarized in the following steps:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">each</span> <span class="n">frame</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">each</span> <span class="n">α</span><span class="p">:</span>
        <span class="n">matching</span> <span class="n">between</span> <span class="n">gt</span> <span class="ow">and</span> <span class="nf">preds </span><span class="p">(</span><span class="n">Hungarian</span> <span class="n">algorithm</span><span class="p">)</span>
        <span class="n">obtain</span> <span class="n">TP</span><span class="p">,</span> <span class="n">FP</span> <span class="ow">and</span> <span class="n">FN</span> <span class="k">from</span> <span class="n">previous</span> <span class="n">matching</span>
        <span class="n">compute</span> <span class="n">AssA</span> <span class="n">across</span> <span class="n">the</span> <span class="n">entire</span> <span class="n">video</span> <span class="k">for</span> <span class="n">each</span> <span class="n">TP</span><span class="p">.</span>
</code></pre></div></div>

<div class="post-center-image">
    <a href="/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/hota.jpg">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/hota-800-4a2b59f98.jpg" alt="HOTA metric diagram" srcset="/generated/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/hota-400-86dc5e9da.webp 400w, /generated/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/hota-800-86dc5e9da.webp 800w, /generated/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/hota-1384-86dc5e9da.webp 1384w" sizes="(max-width: 767px) 100vw, 80vw" width="1384" height="722" />
</a>

</div>

<p class="image-caption"><em>Figure 4. HOTA metric diagram</em></p>

<p>In the original paper, Ass-IoU is referred to as the metric obtained by computing DetA across the entire sequence for a single true positive (TP) match in the current frame. The AssA metric can then be defined as follows:</p>

\[\text{AssA} = \frac{1}{|\text{TP}|} \sum_{c \in \text{TP}} \text{Ass-IoU}(c)\]

<p>HOTA drawbacks:</p>

<ul>
  <li><strong>Not Ideal for Online Tracking</strong>: HOTA’s association score depends on future associations across the entire video, making it less suitable for evaluating online tracking where future data isn’t available.</li>
  <li><strong>Doesn’t Account for Fragmentation</strong>: HOTA does not penalize fragmented tracking results, as it is designed to focus on long-term global tracking, which may not align with all application needs.</li>
</ul>

<p>If you want to learn more about HOTA, I recommend reading the blog post by <a href="https://jonathonluiten.medium.com/how-to-evaluate-tracking-with-the-hota-metrics-754036d183e1" target="_blank" rel="noopener noreferrer">Jonathon Luiten</a>. He is one of the authors of the HOTA paper, and his post is an excellent resource for learning how to use the metric to compare different trackers.</p>

<h2 id="how-do-these-metrics-compare-to-each-other">How do these metrics compare to each other?</h2>

<p>We have examined how MOTA, IDF1, and HOTA function. Each metric has its own strengths and limitations. While HOTA is generally recommended for most applications, the choice of metric ultimately depends on your specific tracking scenario. The HOTA paper provides an excellent comparison that effectively captures the differences between these metrics:</p>

<div class="post-center-image">
    <a href="/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/metrics.png">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/metrics-800-b92eaeabf.png" alt="Metric comparison" srcset="/generated/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/metrics-400-66584ffac.webp 400w, /generated/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/metrics-800-66584ffac.webp 800w, /generated/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/metrics-1172-66584ffac.webp 1172w" sizes="(max-width: 767px) 100vw, 80vw" width="1172" height="556" />
</a>

</div>

<p class="image-caption"><em>Figure 5. Metric comparison</em></p>

<p>Having already introduced the left side of the image, let’s now focus on the right side, which displays metrics for each tracker output. The leftmost metric, DetA, exclusively evaluates detection quality. It yields the best results when the tracker accurately detects objects, regardless of their track ID.
On the opposite end, we have AssA (derived from the HOTA definition). This metric prioritizes track ID consistency, which is why output A performs best in this category.
The authors demonstrate how HOTA positions itself in the middle, striking a balance between detection quality and association accuracy.</p>

<p>The most suitable metric depends on your specific application. For instance:</p>
<ol>
  <li>If you’re developing a simple camera system to count people in a room, you might prioritize detection quality (DetA).</li>
  <li>In a criminal tracking system where maintaining consistent track IDs is crucial, you should focus on AssA.</li>
  <li>For most applications, such as sports tracking systems, you’ll need to balance both aspects. In these scenarios, HOTA emerges as the optimal choice, providing a comprehensive evaluation of tracker performance.</li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>In this post, we’ve explored three key metrics used in Multi-Object Tracking: MOTA, IDF1, and HOTA. Each metric offers unique insights into tracking performance, with its own strengths and limitations. MOTA provides a straightforward measure but may be oversimplistic in complex scenarios. IDF1 focuses on long-term consistency but may not fully capture detection improvements. HOTA, which attempts to balance detection and association accuracy, has emerged as the standard metric used today for benchmarking tracking algorithms.</p>

<h2 id="references">References</h2>

<ul>
  <li>[<a href="https://arxiv.org/pdf/1603.00831" target="_blank" rel="noopener noreferrer">1</a>] Milan, A., Leal-Taixé, L., Reid, I., Roth, S., &amp; Schindler, K. (2016). MOT16: A benchmark for multi-object tracking. arXiv preprint arXiv:1603.00831.</li>
  <li>[<a href="https://arxiv.org/pdf/1609.01775" target="_blank" rel="noopener noreferrer">2</a>] Ristani, E., Solera, F., Zou, R., Cucchiara, R., &amp; Tomasi, C. (2016, October). Performance measures and a data set for multi-target, multi-camera tracking. In European conference on computer vision (pp. 17-35). Cham: Springer International Publishing.</li>
  <li>[<a href="https://arxiv.org/pdf/2009.07736" target="_blank" rel="noopener noreferrer">3</a>] Luiten, J., Osep, A., Dendorfer, P., Torr, P., Geiger, A., Leal-Taixé, L., &amp; Leibe, B. (2021). Hota: A higher order metric for evaluating multi-object tracking. International journal of computer vision, 129, 548-578.</li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Explore the essential metrics in Multi-Object Tracking (MOT) with this comprehensive guide. We break down and compare key metrics like MOTA, IDF1, and HOTA, while also covering DetA and AssA. Understand how these metrics assess spatial accuracy and temporal consistency in object tracking.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://miguel-mendez-ai.com/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/thumbnail.jpg" /><media:content medium="image" url="https://miguel-mendez-ai.com/assets/images/fullsize/posts/2024-08-25-mot-tracking-metrics/thumbnail.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">A Guide to Horizontal Pod Autoscaler</title><link href="https://miguel-mendez-ai.com/2024/04/28/kubernetes-horizontal-pod-autoscaler-guide.html" rel="alternate" type="text/html" title="A Guide to Horizontal Pod Autoscaler" /><published>2024-04-28T00:00:00+00:00</published><updated>2024-04-28T00:00:00+00:00</updated><id>https://miguel-mendez-ai.com/2024/04/28/kubernetes-horizontal-pod-autoscaler-guide</id><content type="html" xml:base="https://miguel-mendez-ai.com/2024/04/28/kubernetes-horizontal-pod-autoscaler-guide.html"><![CDATA[<p>A few weeks ago, while reviewing service metrics in Grafana, I noticed some unexpected behaviour in one of our services—there were more pods than necessary given the current traffic load. This led me to uncover that the extra pods were spawned by the Horizontal Pod Autoscaler (HPA) based on the metrics we had configured (a while ago). Understanding HPA took me a few hours. This is a task typically handled by specialized teams in larger companies, but working at a startup forces you to wear many hats and I often find myself analyzing how models perform in production. In this post, I’ll discuss the issues I encountered with HPA and demonstrate how a simple <a href="#visualization-tool">visualization tool</a> can help anticipate the number of replicas needed.</p>

<h2 id="what-is-hpa">What is HPA?</h2>

<p>The <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/" target="_blank" rel="noopener noreferrer">Horizontal Pod Autoscaler (HPA)</a> in Kubernetes automatically adjusts the number of pod replicas in a deployment, replicaset, or statefulset based on observed CPU utilization or other select metrics. This feature is very useful for managing application scalability and resource efficiency, particularly in environments with variable workloads.</p>

<p>For example Statsbomb can use HPA to handle increased traffic during a weekend when there are more games being played. The HPA can automatically scale up the number of web server pods to maintain performance, and scale down during off-peak hours to reduce costs. This dynamic adjustment helps ensure that the application consistently meets performance targets without manual intervention.</p>

<p>In next sections I will briefly explain how HPA works and how to use <a href="#visualization-tool">this simple tool</a> to ease your scaling decisions.</p>

<h2 id="how-does-hpa-work">How does HPA work?</h2>

<p>First of all we need to make sure we understand the concepts of <code class="language-plaintext highlighter-rouge">requests</code> and <code class="language-plaintext highlighter-rouge">limits</code> since they are fundamental to how resources are allocated and managed across the pods in a cluster.</p>

<ul>
  <li><strong>Requests</strong>: This value specifies the amount of CPU or memory that Kubernetes guarantees to a pod. When a pod is scheduled, the Kubernetes scheduler uses this request value to decide on which node the pod can fit. So this number ensures the pod has the resources it needs to run.</li>
  <li><strong>Limits</strong>: This value specifies the maximum amount of CPU or memory that a pod can use. If a pod exceeds this limit, Kubernetes will throttle the pod or kill it. This is how k8s ensures that a single pod does not consume all the resources in a node.</li>
</ul>

<p>Imagine our deployment has the following setup:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># deployment.yaml</span>
<span class="na">resources</span><span class="pi">:</span>
  <span class="na">limits</span><span class="pi">:</span>
    <span class="na">memory</span><span class="pi">:</span> <span class="s">2000Mi</span>
    <span class="na">cpu</span><span class="pi">:</span> <span class="s">1500m</span>
  <span class="na">requests</span><span class="pi">:</span>
    <span class="na">memory</span><span class="pi">:</span> <span class="s">1350Mi</span>
    <span class="na">cpu</span><span class="pi">:</span> <span class="s">500m</span>
    
</code></pre></div></div>

<p>And our HPA is configured in the following manner:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">autoscaling/v2</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">HorizontalPodAutoscaler</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">autoscaler-name</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">scaleTargetRef</span><span class="pi">:</span>
    <span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
    <span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">autoscaler-name</span>
  <span class="na">minReplicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">maxReplicas</span><span class="pi">:</span> <span class="m">3</span>
  <span class="na">metrics</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">type</span><span class="pi">:</span> <span class="s">Resource</span>
    <span class="na">resource</span><span class="pi">:</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">cpu</span>
      <span class="na">target</span><span class="pi">:</span>
        <span class="na">type</span><span class="pi">:</span> <span class="s">Utilization</span>
        <span class="na">averageUtilization</span><span class="pi">:</span> <span class="m">90</span>
  <span class="pi">-</span> <span class="na">type</span><span class="pi">:</span> <span class="s">Resource</span>
    <span class="na">resource</span><span class="pi">:</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">memory</span>
      <span class="na">target</span><span class="pi">:</span>
        <span class="na">type</span><span class="pi">:</span> <span class="s">Utilization</span>
        <span class="na">averageUtilization</span><span class="pi">:</span> <span class="m">90</span>
</code></pre></div></div>

<p>So what does this mean? Well if you are a proper engineer what you would do is check the <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details" target="_blank" rel="noopener noreferrer">official docs</a> and try to carefully understand this. But if you are like me… you probably would make some assumptions and hope for the best (only to end up having to read the docs 😅).</p>

<p>My first guess was that this would configure HPA to scale up the number of pods when memory or CPU usage exceeded 90%. However, I overlooked a crucial detail: the calculation also needs to include the current number of replicas. Here’s how HPA actually works:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )]
</code></pre></div></div>

<p>Where:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">currentReplicas</code> is the number of replicas the deployment is currently running.</li>
  <li><code class="language-plaintext highlighter-rouge">currentMetricValue</code> is the current value of the metric we are monitoring (e.g. CPU usage).</li>
  <li><code class="language-plaintext highlighter-rouge">desiredMetricValue</code> is the target value for the metric we are monitoring (e.g. 90% requested CPU usage).</li>
</ul>

<p>So let’s see what I observed in Grafana that day.</p>

<div class="post-center-image">
    <a href="/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/memory_cpu.png">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/memory_cpu-800-e6106cc03.png" alt="Grafana plot showing memory and CPU usages" srcset="/generated/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/memory_cpu-400-6e4143c17.webp 400w, /generated/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/memory_cpu-800-6e4143c17.webp 800w, /generated/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/memory_cpu-1248-6e4143c17.webp 1248w" sizes="(max-width: 767px) 100vw, 80vw" width="1248" height="742" />
</a>

</div>

<p class="image-caption"><em>Figure 1. Grafana plot showing memory and CPU usages. Note pod colors differ between CPU and Memory usage</em></p>

<p>What I observed was that at 12:20, we had three pods running with memory usage around <code class="language-plaintext highlighter-rouge">1100 MB</code> and CPU usage less than <code class="language-plaintext highlighter-rouge">100m</code>. Both metrics appeared to be below the target values: <code class="language-plaintext highlighter-rouge">1215 MB (=0.9*1350 MB)</code> for memory and <code class="language-plaintext highlighter-rouge">450m (=0.9*500m)</code> for CPU. So, why were there three pods running?</p>

<h2 id="visualization-tool">Visualization tool</h2>

<p>Before moving forward with the debbuging I would like to introduce the visualization tool I have built. It allows you to enter the specific details of your current/target metrics, as well as the current number of replicas. Based on those inputs, it computes and displays the desired number replicas using the scaling formula above.</p>

<div class="app-container" id="vistool">

    <style>
        .form-select {
            font-family: Graphik, Arial, sans-serif;
        }

        .form-input {
            width: 15%;
        }

        .form-container {
            display: flex;
            flex-wrap: wrap;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 20px;
        }

        .form-item {
            flex: 1 1 auto;
            display: flex;
            align-items: center;
            margin: 5px;
        }

        @media (min-width: 768px) {
            .form-container {
                flex-direction: row;
            }

            .form-item {
                max-width: 30%;
            }
        }

        .form-item > label {
            flex: 0;
            white-space: nowrap;
            margin-right: 10px;
        }

        .form-item > .form-input,
        .form-item > .form-select {
            flex: 2;
            max-width: 70%;
        }

        .axis-form {
            margin-top: 20px;
            min-width: fit-content;
            max-width: 30%;
            width: 100%;
            margin: auto;
            text-align: center;
            margin-bottom: 20px;
        }

        .axis-form .form-select {
            width: 100%;
        }
    </style>

    <div class="form-container">

        <div class="form-item">
            <label for="current_replicas">Current Replicas:</label>
            <input type="number" id="current_replicas" class="form-input" value="1" min="1" />
        </div>
        <div class="form-item">
            <label for="current_metric_value">Current Metric:</label>
            <input type="number" id="current_metric_value" class="form-input" value="1100" step="1" />
        </div>
        <div class="form-item">
            <label for="target_metric_value">Desired Metric:</label>
            <input type="number" id="target_metric_value" class="form-input" value="1200" step="1" />
        </div>
    </div>
    <div>
        <canvas id="replicasChart"></canvas>
    </div>
    <form class="axis-form">
        <label for="xAxisSelect"></label>
        <select id="xAxisSelect" class="form-select">
            <option value="current_metric">Current Metric</option>
            <option value="replicas">Current Replicas</option>
            <option value="target_metric">Desired Metric</option>
        </select>
    </form>
    <script type="text/javascript" src="/libs/custom/hpa.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/hammerjs@2.0.8"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-zoom@2.0.1/dist/chartjs-plugin-zoom.min.js"></script>

</div>

<h2 id="debugging-hpa">Debugging HPA</h2>

<p>Let’s start by inspecting what has happened to our application step by step using our visualization tool using previous metrics. First, let’s check the memory usage at 12:18:</p>

<div class="post-center-image">
    <a href="/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/memory_init.png">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/memory_init-800-b2ed377f4.png" alt="HPA Memory expected replicas" srcset="/generated/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/memory_init-400-10c77e3a0.webp 400w, /generated/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/memory_init-800-10c77e3a0.webp 800w, /generated/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/memory_init-973-10c77e3a0.webp 973w" sizes="(max-width: 767px) 100vw, 80vw" width="973" height="562" />
</a>

</div>

<p class="image-caption"><em>Figure 2. HPA memory expected replica</em></p>

<p>The memory usage seems to be below the 90% so the number of replicas would be set to 1. Let’s do the same for the CPU usage assuming a value of <code class="language-plaintext highlighter-rouge">600m</code> at that time:</p>

<div class="post-center-image">
    <a href="/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/cpu_init.png">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/cpu_init-800-54edac61a.png" alt="HPA CPU expected replicas" srcset="/generated/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/cpu_init-400-640475e30.webp 400w, /generated/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/cpu_init-800-640475e30.webp 800w, /generated/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/cpu_init-972-640475e30.webp 972w" sizes="(max-width: 767px) 100vw, 80vw" width="972" height="562" />
</a>

</div>

<p class="image-caption"><em>Figure 3. HPA CPU expected replica</em></p>

<p>At startup, the CPU usage for the pod exceeded the target value of <code class="language-plaintext highlighter-rouge">450m</code>. This means that the <code class="language-plaintext highlighter-rouge">currentMetricValue / desiredMetricValue</code> ratio was greater than one, indicating that the autoscaler needed to scale up the replicas. But by how much? Let’s adjust the x-axis of the plot to display the number of replicas:</p>

<div class="post-center-image">
    <a href="/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/cpu_stairs.png">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/cpu_stairs-800-919de227e.png" alt="HPA CPU expected replicas showing stairs pattern" srcset="/generated/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/cpu_stairs-400-b615656dc.webp 400w, /generated/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/cpu_stairs-800-b615656dc.webp 800w, /generated/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/cpu_stairs-998-b615656dc.webp 998w" sizes="(max-width: 767px) 100vw, 80vw" width="998" height="572" />
</a>

</div>

<p class="image-caption"><em>Figure 4. HPA CPU current replicas vs expected replicas</em></p>

<p>There you go! We can clearly see the staircase pattern, similar to $f(x) = x + 1$. This occurs because the <code class="language-plaintext highlighter-rouge">currentMetricValue / desiredMetricValue</code> ratio is greater than 1, prompting the autoscaler to continuously increase the number of replicas until it reaches the maximum allowed. In this instance, <code class="language-plaintext highlighter-rouge">maxReplicas</code> was set to 3. Thus, we have identified the root of the problem!</p>

<h2 id="why-is-hpa-not-scaling-down">Why is HPA not scaling down?</h2>

<p>Although CPU usage spiked at startup, it quickly returned to low levels. So why isn’t the HPA scaling down the number of replicas? It appears that the CPU requirement is well below the target value of <code class="language-plaintext highlighter-rouge">450m</code>, as illustrated in <em>Figure 1</em>. According to the official HPA documentation:</p>

<blockquote>
  <p>“If multiple metrics are specified in a HorizontalPodAutoscaler, this calculation is done for each metric, and then the largest of the desired replica counts is chosen.”</p>
</blockquote>

<p>This indicates that the issue now lies with memory usage. Grafana shows us that memory usage has remained constant after the scaling. According to <em>Figure 2</em>, the expected number of replicas should be just 1. However, since the HPA previously increased our replicas to 3, when we view the same plot with the number of replicas on the x-axis, it reveals the following:</p>

<div class="post-center-image">
    <a href="/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/memory_stairs.png">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/memory_stairs-800-78d16daf5.png" alt="HPA memory current replicas vs expected replicas" srcset="/generated/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/memory_stairs-400-d4bea4075.webp 400w, /generated/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/memory_stairs-800-d4bea4075.webp 800w, /generated/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/memory_stairs-973-d4bea4075.webp 973w" sizes="(max-width: 767px) 100vw, 80vw" width="973" height="567" />
</a>

</div>

<p class="image-caption"><em>Figure 5. HPA memory current replicas vs expected replicas</em></p>

<p>With the current memory usage, the HPA behaves like the function $f(x) = x$, preventing the number of replicas from scaling down. This is why we continuously see three pods running, even though the pods are not receiving much traffic.</p>

<h2 id="what-can-we-do">What can we do?</h2>

<p>We have a couple of options to address this problem. For instance, we could change the memory and CPU targets in the HPA settings. Increasing the CPU requirements could help us avoid the initial spike in replicas and prevent the staircase pattern. On the other hand, if we increase the memory requirements, we could avoid the bottleneck and scale down the number of replicas. However, this isn’t a permanent solution because if our application’s memory use fluctuates, we could encounter the same issue again. Instead, we should tackle the root cause and for this we need to differentiate between the fixed and variable usage of our metrics.</p>

<p>The initial spike in CPU usage was due to the startup process, which is a fixed usage. As we could see before, adding more replicas doesn’t alleviate the problem because the usage is related with the startup process and not the traffic. Conversely, when requests arrive at our service, the CPU usage can change, allowing the HPA to scale up or scale down the number of replicas. This represents variable usage.</p>

<p>The memory usage always stays the same, no matter how many pods are running. It is another example of fixed usage. Because of this, the HPA behaves like a function where $f(x) = x$. Therefore, adjusting the number of pods based on memory usage doesn’t work well because the memory doesn’t vary with the traffic. The best strategy is to <strong>remove the memory metric from the HPA</strong> settings and scale only based on the CPU metric.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this post, we explored how the Horizontal Pod Autoscaler (HPA) in Kubernetes manages the number of pods based on CPU and memory usage. This is very important for keeping applications running smoothly as demands fluctuate. However, we learned the importance of monitoring HPA settings and adjusting them to fit real-world scenarios.</p>

<p>Through an example, we’ve discovered how various metric patterns can influence the HPA’s behavior and the importance of selecting appropriate metrics. Metrics that do not adjust with traffic levels or pod count may not be appropriate for making scaling decisions.</p>

<p>For anyone using Kubernetes, whether in a small startup or a large company, understanding how to properly manage these settings is essential. It’s all about deeply knowing your systems and making the right adjustments to maintain efficiency and responsiveness.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Discover how Kubernetes' Horizontal Pod Autoscaler (HPA) functions using a real use case. Learn to manage and observe HPA in action with practical examples. This post will show you how to optimize resource usage, streamline pod scaling, and enhance application performance using a simple visualization tool.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://miguel-mendez-ai.com/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/test.jpg" /><media:content medium="image" url="https://miguel-mendez-ai.com/assets/images/fullsize/posts/2024-04-28-kubernetes-horizontal-pod-autoscaler-guide/test.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Exploring Transformer-Based Methods in Video Instance Segmentation</title><link href="https://miguel-mendez-ai.com/2024/04/15/video-segmentation.html" rel="alternate" type="text/html" title="Exploring Transformer-Based Methods in Video Instance Segmentation" /><published>2024-04-15T00:00:00+00:00</published><updated>2024-04-15T00:00:00+00:00</updated><id>https://miguel-mendez-ai.com/2024/04/15/video-segmentation</id><content type="html" xml:base="https://miguel-mendez-ai.com/2024/04/15/video-segmentation.html"><![CDATA[<p>I had recently visited the most recent literature on Video Segmentation and I was startled by how lost I found myself. If you are versed in this field, you are probably familiar with terms such as SVOS, UVOS, VIS, Zero-Shot, One-Shot, etc. If you are not, then you will probably find yourself as lost as I was a few weeks ago.</p>

<p>My main intention was to focus solely on the specific topic I was interested in. This is Video instance Segmentation (VIS), which extends the image instance segmentation task from the image domain to the video domain. The goal is to segment object instances from a predefined category set in videos, then associate the instance identities across frames. It can also be seen as a combination of instance segmentation and object tracking in videos.</p>

<p>As usually I will be trying to update this post with the most recent papers and code implementations I find interesting.</p>

<h2 id="introduction-to-video-segmentation">Introduction to Video Segmentation</h2>

<p>Video segmentation condenses a lot of different tasks which can have multiple names. I personaly like the taxonomy used in the <a href="https://youtube-vos.org/dataset/" target="_blank" rel="noopener noreferrer">Youtube-VOS Dataset</a>, which is one of the main benchmarks in this field, so I will stick with it through this post. The different tasks are:</p>

<ul>
  <li><strong>Video Object Segmentation (VOS)</strong>: targets at segmenting a particular object instance throughout the entire video sequence given only the object mask of the first frame.</li>
  <li><strong>Video Instance Segmentation (VIS)</strong>: extends image instance segmentation to videos, aiming to segment and track object instances across frames.</li>
  <li><strong>Referring Video Object Segmentation (RVOS)</strong>: is a task that requires to segment a particular object instance in a video given a natural language expression that refers to the object instance.</li>
</ul>

<div class="post-center-image">
<a href="/assets/svg/posts/2024-04-15-video-segmentation/video-segmentation-types-diagram.svg" class="lightbox-image" title="Diagram with the different video segmentation methods">
 <img src="/assets/svg/posts/2024-04-15-video-segmentation/video-segmentation-types-diagram.svg" loading="lazy" alt="Diagram with the different video segmentation methods" />
</a>
</div>

<p class="image-caption"><em>Diagram with the different video segmentation methods</em></p>

<h2 id="key-concepts">Key concepts</h2>

<h3 id="input-sequence-length">Input sequence length</h3>

<p>The input sequence length is crucial in video segmentation. Longer sequences provide more context for accurately segmenting objects, even through occlusions or appearance changes. However, they require more computational power and they cause an increase in training and inference times. Bear in mind that transformers have a quadratic complexity with respect to the sequence length. That’s the reason why most of the models we are going to discuss here are trained with very short clips (mostly between 2 and 8 frames).</p>

<h3 id="stride">Stride</h3>

<p>The input sequence length defines the number of frames processed in parallel. The stride is the one that determines the temporal distance between adjacent frames in the input sequence. A stride of 1 means that the input sequence is a continuous sequence of frames, while a stride of 2 means that every other frame is skipped. By increasing the stride, the system can work faster because even though it will be looking at the same amount of frames at once, it will need to process fewer frames in total.</p>

<h3 id="offline-vs-online">Offline vs Online</h3>

<p>Many video segmentation approaches are categorized as offline. They process the entire video in one go, which is ideal for short videos and limited by the maximum length the model can process. On the other hand online methods divide videos into overlapping chunks, and results from these segments are merged using a rule-based post-tracking method. This approach ensures continuous tracking across the video by processing and linking instances from overlapping segments.</p>

<h2 id="video-instance-segmentation">Video Instance Segmentation</h2>

<p>From now on I will exclusively focus on Video Instance Segmentation (VIS). Most papers before 2020 were based on either:</p>

<ul>
  <li><strong>Top-down approach</strong>: following tracking-by-detection methods (you can check <a href="/2023/11/08/tracking-by-detection-overview.html">this other post</a> for more information on this topic)</li>
  <li><strong>Bottom-up approach</strong>: clustering pixel embeddings into objects.</li>
</ul>

<p>These method suffered from different issues, and around 2020, transformer-based approaches began to appear. Most of the research focused on how to throw a transformer into this problem that could hold up to the state-of-the-art.</p>

<h3 id="datasets">Datasets</h3>

<p>The most common dataset used for VIS is called <a href="https://youtube-vos.org/dataset/vis/" target="_blank" rel="noopener noreferrer">YouTube-VIS</a>. It comprises three different versions:</p>

<ul>
  <li><strong>YouTube-VIS-2019</strong>: 2,883 high-resolution YouTube videos with 40 object categories. Longest video is 1,000 frames. Longest video only contains 36 frames so it is easy to execute on offline mode.</li>
  <li><strong>YouTube-VIS-2021</strong>: 3,859 high-resolution YouTube video with an improved 40-category label set by merging some and adding new ones. Longer video lengths force to use a near-online approach.</li>
  <li><strong>YouTube-VIS-2022</strong>: not considered in this post since it is more recent than the papers that are covered.</li>
</ul>

<p>The following table summarizes the papers I will be discussing in this post and its performance on the YouTube-VIS-2019 dataset.</p>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th>Method</th>
        <th>Backbone</th>
        <th>MST</th>
        <th>FPS</th>
        <th>AP</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>VisTR</td>
        <td>ResNet-50</td>
        <td>❌</td>
        <td>51.1</td>
        <td>36.2</td>
      </tr>
      <tr>
        <td>VisTR</td>
        <td>ResNet-101</td>
        <td>❌</td>
        <td>43.5</td>
        <td>40.1</td>
      </tr>
      <tr>
        <td>IFC</td>
        <td>ResNet-50</td>
        <td>✅</td>
        <td><strong>107.1</strong></td>
        <td>41.2</td>
      </tr>
      <tr>
        <td>IFC</td>
        <td>ResNet-101</td>
        <td>✅</td>
        <td>89.4</td>
        <td>42.6</td>
      </tr>
      <tr>
        <td>TeViT</td>
        <td>MsgShifT</td>
        <td>❌</td>
        <td>68.9</td>
        <td>45.9</td>
      </tr>
      <tr>
        <td>TeViT</td>
        <td>MsgShifT</td>
        <td>✅</td>
        <td>68.9</td>
        <td><strong>46.6</strong></td>
      </tr>
    </tbody>
  </table>

</div>

<p class="image-caption"><em>Table 1. Comparisons on YouTube-VIS-2019 dataset from TeViT paper [5]. MST indicates multi-scale training strategy. FPS measured with a single TESLA V100. Note all methods used offline evaluation for reporting metrics.</em></p>

<p>Note the differences in precision when comparing with the reported results in Youtube-VIS-2021 dataset. This is due to the increase in video sizes, which forces the model to work in online mode, processing chunks of the video that then need to be merged.</p>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th>Method</th>
        <th>Backbone</th>
        <th>AP</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>IFC</td>
        <td>ResNet-101</td>
        <td>35.2  (36.6 reported in [4])</td>
      </tr>
      <tr>
        <td>TeViT</td>
        <td>MsgShifT</td>
        <td><strong>37.9</strong></td>
      </tr>
    </tbody>
  </table>

</div>

<p class="image-caption"><em>Table 2. Comparisons on YouTube-VIS-2021 dataset from TeViT paper [5].</em></p>

<h3 id="vistr-2021">VisTR (2021)</h3>

<p>VisTR, short for VIS Transformer, emerged as one of the initial transformer-based VIS methods to achieve notable accuracy on the YouTube-VIS dataset, thanks to an effective adaptation of DETR [2] for segmentation. This framework processes a fixed sequence of video frames using a ResNet backbone to independently extract features from each image. These extracted features are then concatenated, enriched with a 3D positional encoding, and injected into an encoder-decoder transformer architecture, which outputs a sequence of object predictions in order.</p>

<div class="post-center-image">
    <a href="/assets/images/fullsize/posts/2024-04-15-video-segmentation/vistr.png">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2024-04-15-video-segmentation/vistr-800-32d23dc52.png" alt="VisTR architecture diagram" srcset="/generated/assets/images/fullsize/posts/2024-04-15-video-segmentation/vistr-400-9bdfeb185.webp 400w, /generated/assets/images/fullsize/posts/2024-04-15-video-segmentation/vistr-800-9bdfeb185.webp 800w, /generated/assets/images/fullsize/posts/2024-04-15-video-segmentation/vistr-1366-9bdfeb185.webp 1366w" sizes="(max-width: 767px) 100vw, 80vw" width="1366" height="470" />
</a>

</div>

<p class="image-caption"><em>VisTR architecture diagram</em></p>

<p>Key ideas we need to highlight about this method:</p>

<ol>
  <li>Instance queries are fixed, learnable parameters that determine the number of instances that can be predicted (input of decoder in the diagram).</li>
  <li>Training involves instance sequence prediction over $N$ frames that requires ground truth matching to compute the loss.</li>
  <li>The Hungarian algorithm is used to find a bipartite matching between ground-truth and prediction (as in DETR [2]).</li>
  <li>Solving 3 using masks is very expensive. A module is added to obtain bounding boxes from instance predictions and solve the matching efficiently.</li>
  <li>The loss consists on a combination of the bounding box, the mask and the class prediction errors.</li>
  <li>A 3D conv module fuses temporal information before masks are obtained.</li>
</ol>

<p><strong>Drawbacks</strong>: The models are trained on 8 V100 GPUs of 32G RAM, with 1 video clip per GPU. Either low resolution or short clips are used to fit in memory. VisTR remains as a complete offline strategy because it takes the entire video as an input (from IFC paper).</p>

<h3 id="ifc">IFC</h3>

<p>Inter-frame Communication Transformers (IFC) leverages the idea that, since humans can summarize scenes briefly and consecutive frames often share similarities, it’s feasible to communicate frame differences with minimal data. To reduce computational load, IFC utilizes a number of ‘memory tokens’ to exchange information between frames, thus lowering the complexity of space-time attention.</p>

<div class="post-center-image">
    <a href="/assets/images/fullsize/posts/2024-04-15-video-segmentation/ifc.png">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2024-04-15-video-segmentation/ifc-800-f24fde520.png" alt="IFC architecture diagram" srcset="/generated/assets/images/fullsize/posts/2024-04-15-video-segmentation/ifc-400-691dbc6bb.webp 400w, /generated/assets/images/fullsize/posts/2024-04-15-video-segmentation/ifc-800-691dbc6bb.webp 800w, /generated/assets/images/fullsize/posts/2024-04-15-video-segmentation/ifc-1065-691dbc6bb.webp 1065w" sizes="(max-width: 767px) 100vw, 80vw" width="1065" height="477" />
</a>

</div>

<p class="image-caption"><em>IFC architecture diagram</em></p>

<p>The architecture integrates Transformer encoders with ResNet feature maps and learnable memory tokens. Encoder blocks are composed of:</p>
<ul>
  <li><strong>Encode-Receive ($\xi$)</strong>: fuses frame features and memory tokens, blending frame and temporal data.</li>
  <li><strong>Gather-Communicate ($\zeta$)</strong>: processes memory tokens across frames for inter-frame communication.</li>
</ul>

<p>The decoder used a fixed number of object queries ($N_q$) that is indepentent on the number of input frames. It features two heads:</p>

<ul>
  <li>A class head for class probability distribution of instances $p(c) \in \mathbb{R}^{N_q \times |\mathbb{C}|}$.</li>
  <li>A segmentation head producing $N_q$ conditional convolutional weights $w \in \mathbb{R}^{N_q \times C}$, convolved with the output of the spatial decoder (reshaped encoder output and upscaled à la FPN).</li>
</ul>

<p>Loss calculation also follows DETR incorporating the Hungarian algorithm, applying it directly to the masks.</p>

<h3 id="tevit">TeViT</h3>

<p>The Temporally Efficient Vision Transformer (TeViT) advances the ideas from IFC by using fewer parameters to fuse information across video frames and including a fully transformer backbone. It also introduces minor improvements in its head stage for more efficient processing.</p>

<div class="post-center-image">
    <a href="/assets/images/fullsize/posts/2024-04-15-video-segmentation/tevit.png">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2024-04-15-video-segmentation/tevit-800-21694f269.png" alt="TeViT architecture diagram" srcset="/generated/assets/images/fullsize/posts/2024-04-15-video-segmentation/tevit-400-fd261f96c.webp 400w, /generated/assets/images/fullsize/posts/2024-04-15-video-segmentation/tevit-800-fd261f96c.webp 800w, /generated/assets/images/fullsize/posts/2024-04-15-video-segmentation/tevit-1500-fd261f96c.webp 1500w, /generated/assets/images/fullsize/posts/2024-04-15-video-segmentation/tevit-1546-fd261f96c.webp 1546w" sizes="(max-width: 767px) 100vw, 80vw" width="1546" height="736" />
</a>

</div>

<p class="image-caption"><em>TeViT architecture diagram</em></p>

<p>At its core, TeViT employs a pyramid vision transformer [6] structure and innovates by replacing IFC’s memory tokens with temporal messenger tokens. These tokens are periodically shifted along the temporal axis within each block to merge information from distinct frames. This shift operation is straightforward, yet remarkably effective, adding no extra parameters to the system.</p>

<p>The head implementation emphasizes modeling temporal relations at the instance level, drawing on the principles of QueryInst [7]. As illustrated in the diagram, the same instance queries are initially applied across every frame. These queries are processed through a parameter-shared multi-head self-attention (MHSA) mechanism and a dynamic convolutional layer [8], which integrates the data with instance region features from the backbone. Finally, task-specific heads (such as classification, box, and mask heads) generate predictions for a sequence of video instances.</p>

<p>The loss computation incorporates the Hungarian algorithm alongside a combination of box, mask, and class prediction errors (details provided in the paper).</p>

<h2 id="conclusion">Conclusion</h2>

<p>Exploring the literature around video instance segmentation (VIS) has been a fun experience. Transformers are now showing up in most of the research in this area. It is quite fascinating to observe ongoing efforts aimed at reducing the complexities associated with video processing, such as minimizing the number of parameters needed to merge time-related features effectively. The influence of the DETR paper on all the methods discussed is also noteworthy.</p>

<p>I will keep updating this post with new and relevant research findings. Feel free to share your thoughts in the comments below or suggest any papers you would like me to explore next.</p>

<h2 id="references">References</h2>

<ul>
  <li>[<a href="https://arxiv.org/pdf/1905.04804.pdf" target="_blank" rel="noopener noreferrer">1</a>] Yang, L., Fan, Y., &amp; Xu, N. (2019). Video instance segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 5188-5197).</li>
  <li>[<a href="https://arxiv.org/pdf/2011.14503.pdf" target="_blank" rel="noopener noreferrer">2</a>] Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., &amp; Xia, H. (2021). End-to-end video instance segmentation with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 8741-8750).</li>
  <li>[<a href="https://arxiv.org/pdf/2005.12872.pdf" target="_blank" rel="noopener noreferrer">3</a>] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., &amp; Zagoruyko, S. (2020, August). End-to-end object detection with transformers. In European conference on computer vision (pp. 213-229). Cham: Springer International Publishing.</li>
  <li>[<a href="https://arxiv.org/pdf/2106.03299.pdf" target="_blank" rel="noopener noreferrer">4</a>] Hwang, S., Heo, M., Oh, S. W., &amp; Kim, S. J. (2021). Video instance segmentation using inter-frame communication transformers. Advances in Neural Information Processing Systems, 34, 13352-13363.</li>
  <li>[<a href="https://arxiv.org/abs/2204.08412" target="_blank" rel="noopener noreferrer">5</a>] Yang, S., Wang, X., Li, Y., Fang, Y., Fang, J., Liu, W., … &amp; Shan, Y. (2022). Temporally efficient vision transformer for video instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 2885-2895).</li>
  <li>[<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Pyramid_Vision_Transformer_A_Versatile_Backbone_for_Dense_Prediction_Without_ICCV_2021_paper.pdf" target="_blank" rel="noopener noreferrer">6</a>] Wang, W., Xie, E., Li, X., Fan, D. P., Song, K., Liang, D., … &amp; Shao, L. (2021). Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 568-578).</li>
  <li>[<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Fang_Instances_As_Queries_ICCV_2021_paper.pdf" target="_blank" rel="noopener noreferrer">7</a>] Fang, Y., Yang, S., Wang, X., Li, Y., Fang, C., Shan, Y., … &amp; Liu, W. (2021). Instances as queries. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 6910-6919).</li>
  <li>[<a href="https://arxiv.org/pdf/1912.03458.pdf" target="_blank" rel="noopener noreferrer">8</a>] Chen, Y., Dai, X., Liu, M., Chen, D., Yuan, L., &amp; Liu, Z. (2020). Dynamic convolution: Attention over convolution kernels. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 11030-11039).</li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Get a detailed overview of the transformative impact of transformer models on video instance segmentation. Learn about the evolution of VIS methods, the role of datasets like YouTube-VIS, and the latest research trends that are defining the future of video processing technologies. This includes detailed comparisons among leading models like VisTR, IFC, and TeViT.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://miguel-mendez-ai.com/assets/images/fullsize/posts/2024-04-15-video-segmentation/thumbnail.jpg" /><media:content medium="image" url="https://miguel-mendez-ai.com/assets/images/fullsize/posts/2024-04-15-video-segmentation/thumbnail.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Building an Efficient Docker Image with Poetry and PyTorch</title><link href="https://miguel-mendez-ai.com/2024/03/12/poetry-torch-docker.html" rel="alternate" type="text/html" title="Building an Efficient Docker Image with Poetry and PyTorch" /><published>2024-03-12T00:00:00+00:00</published><updated>2024-03-12T00:00:00+00:00</updated><id>https://miguel-mendez-ai.com/2024/03/12/poetry-torch-docker</id><content type="html" xml:base="https://miguel-mendez-ai.com/2024/03/12/poetry-torch-docker.html"><![CDATA[<p>The goal of this post is straightforward: to guide you through the creation of a Docker image equipped with Poetry for dependency management and Torch for running deep learning models, specifically utilizing Python 3.11. While the task may seem simple at first glance, it involves several tricks that I believe can be very useful to share. This guide will utilize Docker Buildx, a powerful feature that might be unfamiliar to some, yet it is enabled by default in newer Docker releases.</p>

<h2 id="docker-ubuntu-and-python-311">Docker, Ubuntu, and Python 3.11</h2>

<p>I was surprised by the scarcity of information available on creating a Docker image with Python 3.11 as the system’s default Python version. While one could use the official Python images, I personally prefer to have complete control over and understanding of my Docker images. This practice helps avoid unnecessary dependencies and can be particularly beneficial when addressing issues as unknown environment variables or extra packages that may be set in the base images.</p>

<p>Note that for building our images we will use Docker Buildx, which is a Docker CLI plugin that extends the capabilities of the Docker CLI. There are two different ways of building images with Docker Buildx: using the <code class="language-plaintext highlighter-rouge">docker buildx build</code> command or setting up the <code class="language-plaintext highlighter-rouge">DOCKER_BUILDKIT</code> environment variable.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Using the docker buildx build command</span>
docker buildx build <span class="nt">-t</span> my-image:latest <span class="nt">-f</span> my-file <span class="nb">.</span>

<span class="c"># Setting up the DOCKER_BUILDKIT environment variable</span>
<span class="nb">export </span><span class="nv">DOCKER_BUILDKIT</span><span class="o">=</span>1
docker build <span class="nt">-t</span> my-image:latest <span class="nt">-f</span> my-file <span class="nb">.</span>
</code></pre></div></div>

<p>Let’s begin by configuring our Dockerfile:</p>

<div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:22.04</span>

<span class="c"># Set non-interactive mode to avoid prompts during build</span>
<span class="k">ARG</span><span class="s"> DEBIAN_FRONTEND=noninteractive</span>

<span class="c"># Install system tools and libraries.</span>
<span class="c"># Utilize --mount flag of Docker Buildx to cache downloaded packages, avoiding repeated downloads</span>
<span class="k">RUN </span><span class="nt">--mount</span><span class="o">=</span><span class="nb">type</span><span class="o">=</span>cache,id<span class="o">=</span>apt-build,target<span class="o">=</span>/var/cache/apt <span class="se">\
</span>    apt-get update <span class="o">&amp;&amp;</span> <span class="se">\ </span>
    apt-get install -y software-properties-common &amp;&amp; \
    # Add the Deadsnakes PPA for Python 3.11
    add-apt-repository ppa:deadsnakes/ppa &amp;&amp; \
    apt-get update &amp;&amp; apt-get install -y --no-install-recommends \
        curl \
        build-essential \
        python3.11 \
        python3-pip \
        python3.11-venv \
        python3.11-dev &amp;&amp; \
    # Clean up to keep the image size small
    rm -rf /var/lib/apt/lists/*  &amp;&amp; \
    # Set Python 3.11 as the default Python version
    update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1 &amp;&amp; \
    update-alternatives --set python3 /usr/bin/python3.11 &amp;&amp; \
    update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1 &amp;&amp; \
    update-alternatives --set python /usr/bin/python3.11

<span class="k">ENTRYPOINT</span><span class="s"> [ "/bin/bash" ]</span>
</code></pre></div></div>

<p>This Dockerfile is relatively standard. We are utilizing Ubuntu 22.04 as our base image and installing Python 3.11 from the Deadsnakes PPA. Moreover, we’re setting Python 3.11 as our default Python version using update-alternatives.</p>

<p>For those curious about the <code class="language-plaintext highlighter-rouge">--mount</code> flag, it’s a Docker Buildx feature that caches downloaded packages, preventing them from being downloaded again when adding new packages. This feature can significantly reduce the time required to build your images.</p>

<h2 id="poetry">Poetry</h2>

<p>Next, let’s install Poetry using the official installer, which I like because it’s simple and straightforward. What I usually do is prevent Poetry from creating a new virtual environment. Instead, I manage the environment manually, giving me greater control over my project’s configuration.</p>

<p>If you’re curious about the need for a virtual environment in our Docker image, there are a couple of reasons. Firstly, isolating your project’s dependencies from the system Python ensures a clean, conflict-free environment. Secondly, you might consider using a multi-stage build. This means first installing everything needed in an initial stage and then copying the final virtual environment to the second clean stage. This would make our image smaller and faster to build. However, this is not the focus of this post and I will not cover it here.</p>

<div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:22.04</span>

<span class="k">ARG</span><span class="s"> DEBIAN_FRONTEND=noninteractive</span>

<span class="c"># Set environment variables</span>
<span class="k">ENV</span><span class="s"> PYTHONUNBUFFERED=1 \</span>
    PIP_DISABLE_PIP_VERSION_CHECK=on \
    PIP_DEFAULT_TIMEOUT=100 \
    POETRY_VERSION=1.7.1 \
    POETRY_HOME="/opt/poetry" \
    POETRY_NO_INTERACTION=1 \
    POETRY_VIRTUALENVS_CREATE=false \
    VENV_PATH="/app/.venv" # Use custom venv, avoid auto-creation by Poetry

<span class="c"># Install system tools and libraries.</span>
<span class="c"># Utilize --mount flag of Docker Buildx to cache downloaded packages, avoiding repeated downloads</span>
<span class="k">RUN </span><span class="nt">--mount</span><span class="o">=</span><span class="nb">type</span><span class="o">=</span>cache,id<span class="o">=</span>apt-build,target<span class="o">=</span>/var/cache/apt <span class="se">\
</span>    apt-get update <span class="o">&amp;&amp;</span> <span class="se">\ </span>
    apt-get install -y software-properties-common &amp;&amp; \
    # Add the Deadsnakes PPA for Python 3.11
    add-apt-repository ppa:deadsnakes/ppa &amp;&amp; \
    apt-get update &amp;&amp; apt-get install -y --no-install-recommends \
        curl \
        build-essential \
        python3.11 \
        python3-pip \
        python3.11-venv \
        python3.11-dev &amp;&amp; \
    # Clean up to keep the image size small
    rm -rf /var/lib/apt/lists/*  &amp;&amp; \
    # Set Python 3.11 as the default Python version
    update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1 &amp;&amp; \
    update-alternatives --set python3 /usr/bin/python3.11 &amp;&amp; \
    update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1 &amp;&amp; \
    update-alternatives --set python /usr/bin/python3.11

<span class="c"># Set PATH to include Poetry and custom venv</span>
<span class="k">ENV</span><span class="s"> PATH="$POETRY_HOME/bin:$VENV_PATH/bin:$PATH"</span>

<span class="c"># Install poetry</span>
<span class="k">RUN </span>curl <span class="nt">-sSL</span> https://install.python-poetry.org | python - <span class="nt">--version</span> <span class="nv">$POETRY_VERSION</span>

<span class="c"># Create and prepare the virtual environment</span>
<span class="k">RUN </span>python <span class="nt">-m</span> venv <span class="nv">$VENV_PATH</span> <span class="o">&amp;&amp;</span> <span class="se">\
</span>    python <span class="nt">-m</span> pip <span class="nb">install</span> <span class="nt">--upgrade</span> pip <span class="o">&amp;&amp;</span> <span class="se">\
</span>    pip cache purge <span class="o">&amp;&amp;</span> <span class="nb">rm</span> <span class="nt">-Rf</span> /root/.cache/pip/http
    
<span class="k">WORKDIR</span><span class="s"> /app</span>

<span class="k">ENTRYPOINT</span><span class="s"> [ "/bin/bash" ]</span>
</code></pre></div></div>

<p>I’ve set <code class="language-plaintext highlighter-rouge">VENV_PATH</code> to <code class="language-plaintext highlighter-rouge">{project-dir}/.venv</code>. This is because Poetry might not follow the environment variable we set earlier, as mentioned in the <a href="https://python-poetry.org/docs/configuration/#virtualenvscreate" target="_blank" rel="noopener noreferrer">official docs</a>, and instead creates its own environment. Everything else in the setup is quite standard. If you have any questions, please feel free to ask.</p>

<h2 id="pytorch">Pytorch</h2>

<p>Installing Torch with Poetry can be tricky because Torch can be installed with or without GPU support, making it challenging to support both CPU and GPU versions in your pyproject.toml file. For example, you might use the CPU version for Continuous Integration (CI) and the GPU version for running your models in production. Many issues related to this are discussed on the <a href="https://github.com/python-poetry/poetry/issues/6409" target="_blank" rel="noopener noreferrer">Poetry GitHub</a>. After trying different methods, my preferred solution is to install dependencies using Poetry and then install Torch using pip. We need to ensure Torch is installed inside our virtual environment (venv), which requires setting the correct paths in advance (apologies if you were expecting a more complex solution 😑). Here’s what you need to add:</p>

<div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Copy dependency files to the app directory</span>
<span class="k">COPY</span><span class="s"> poetry.lock pyproject.toml /app</span>

<span class="c"># Install dependencies with Poetry and Torch with pip, caching downloaded packages</span>
<span class="k">RUN </span><span class="nt">--mount</span><span class="o">=</span><span class="nb">type</span><span class="o">=</span>cache,target<span class="o">=</span>/root/.cache <span class="se">\
</span>    poetry <span class="nb">install</span> <span class="nt">--without</span> dev <span class="o">&amp;&amp;</span> <span class="se">\
</span>    <span class="c"># Install torch GPU</span>
    pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 -f https://download.pytorch.org/whl/torch_stable.html 

<span class="c"># Copy the entire project code to the container</span>
<span class="k">COPY</span><span class="s"> ./ /source/</span>
</code></pre></div></div>

<p>Note that we first copy the <code class="language-plaintext highlighter-rouge">poetry.lock</code> and <code class="language-plaintext highlighter-rouge">pyproject.toml</code> to the image and then we run <code class="language-plaintext highlighter-rouge">poetry install --without dev</code>. Only after deps have been installed we copy our code to the image. This is a good practice to avoid installing the dependencies every time we change our code. Once again we are using the <code class="language-plaintext highlighter-rouge">--mount</code> flag to cache the downloaded packages.</p>

<p>Although we install Torch with GPU support, we do not install the CUDA toolkit separately. This is because all necessary CUDA binaries are included in the Torch wheel; hence, we specify <code class="language-plaintext highlighter-rouge">cu118</code> in the installation command to ensure compatibility. This is what makes the torch wheel huge, because it includes code for multiple CUDA architectures so the same binary can be used on different GPUs. If you want to obtain a smaller image, you can build torch from source and only specify the architecture you need for your GPU. <a href="https://github.com/pytorch/pytorch/issues/17621" target="_blank" rel="noopener noreferrer">This</a> is a good thread about this topic.</p>

<h2 id="final-solution">Final solution</h2>

<p>Here’s the final Dockerfile that puts everything we’ve talked about into action, hope this can help you to build your own image.</p>

<div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:22.04</span>

<span class="k">ARG</span><span class="s"> DEBIAN_FRONTEND=noninteractive</span>

<span class="c"># Set environment variables</span>
<span class="k">ENV</span><span class="s"> PYTHONUNBUFFERED=1 \</span>
    PIP_DISABLE_PIP_VERSION_CHECK=on \
    PIP_DEFAULT_TIMEOUT=100 \
    POETRY_VERSION=1.7.1 \
    POETRY_HOME="/opt/poetry" \
    POETRY_NO_INTERACTION=1 \
    POETRY_VIRTUALENVS_CREATE=false \
    VENV_PATH="/app/.venv" # Use custom venv, avoid auto-creation by Poetry

<span class="c"># Install system tools and libraries.</span>
<span class="c"># Utilize --mount flag of Docker Buildx to cache downloaded packages, avoiding repeated downloads</span>
<span class="k">RUN </span><span class="nt">--mount</span><span class="o">=</span><span class="nb">type</span><span class="o">=</span>cache,id<span class="o">=</span>apt-build,target<span class="o">=</span>/var/cache/apt <span class="se">\
</span>    apt-get update <span class="o">&amp;&amp;</span> <span class="se">\ </span>
    apt-get install -y software-properties-common &amp;&amp; \
    # Add the Deadsnakes PPA for Python 3.11
    add-apt-repository ppa:deadsnakes/ppa &amp;&amp; \
    apt-get update &amp;&amp; apt-get install -y --no-install-recommends \
        curl \
        build-essential \
        python3.11 \
        python3-pip \
        python3.11-venv \
        python3.11-dev &amp;&amp; \
    # Clean up to keep the image size small
    rm -rf /var/lib/apt/lists/*  &amp;&amp; \
    # Set Python 3.11 as the default Python version
    update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1 &amp;&amp; \
    update-alternatives --set python3 /usr/bin/python3.11 &amp;&amp; \
    update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1 &amp;&amp; \
    update-alternatives --set python /usr/bin/python3.11

<span class="c"># Set PATH to include Poetry and custom venv</span>
<span class="k">ENV</span><span class="s"> PATH="$POETRY_HOME/bin:$VENV_PATH/bin:$PATH"</span>

<span class="c"># Install poetry</span>
<span class="k">RUN </span>curl <span class="nt">-sSL</span> https://install.python-poetry.org | python - <span class="nt">--version</span> <span class="nv">$POETRY_VERSION</span>

<span class="c"># Create and prepare the virtual environment</span>
<span class="k">RUN </span>python <span class="nt">-m</span> venv <span class="nv">$VENV_PATH</span> <span class="o">&amp;&amp;</span> <span class="se">\
</span>    python <span class="nt">-m</span> pip <span class="nb">install</span> <span class="nt">--upgrade</span> pip <span class="o">&amp;&amp;</span> <span class="se">\
</span>    pip cache purge <span class="o">&amp;&amp;</span> <span class="nb">rm</span> <span class="nt">-Rf</span> /root/.cache/pip/http
    
<span class="k">WORKDIR</span><span class="s"> /app</span>

<span class="c"># Copy dependency files to the app directory</span>
<span class="k">COPY</span><span class="s"> poetry.lock pyproject.toml /app</span>

<span class="c"># Install dependencies with Poetry and Torch with pip, caching downloaded packages</span>
<span class="k">RUN </span><span class="nt">--mount</span><span class="o">=</span><span class="nb">type</span><span class="o">=</span>cache,target<span class="o">=</span>/root/.cache <span class="se">\
</span>    poetry <span class="nb">install</span> <span class="nt">--without</span> dev <span class="o">&amp;&amp;</span> <span class="se">\
</span>    pip <span class="nb">install </span><span class="nv">torch</span><span class="o">==</span>2.0.1+cu118 <span class="nv">torchvision</span><span class="o">==</span>0.15.2+cu118 <span class="nt">-f</span> https://download.pytorch.org/whl/torch_stable.html 

<span class="c"># Copy the entire project code to the container</span>
<span class="k">COPY</span><span class="s"> ./ /source/</span>

<span class="k">ENTRYPOINT</span><span class="s"> [ "/bin/bash" ]</span>
</code></pre></div></div>

<p><em>Any ideas for future posts or is there something you would like to comment? Please feel free to reach out via <a href="https://twitter.com/mmeendez8" target="_blank" rel="noopener noreferrer">Twitter</a> or <a href="https://github.com/mmeendez8" target="_blank" rel="noopener noreferrer">Github</a></em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Learn to create a Docker image for your Pytorch projects. Discover how to manage dependencies with Poetry and Python 3.11. We'll walk you through using Docker Buildx, handling Torch versions, and optimizing your build. Ideal for developers ready to quickly start their deep learning projects.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://miguel-mendez-ai.com/assets/images/fullsize/posts/2024-03-12-poetry-torch-docker/thumbnail.jpg" /><media:content medium="image" url="https://miguel-mendez-ai.com/assets/images/fullsize/posts/2024-03-12-poetry-torch-docker/thumbnail.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">NFL Field Mapping: A Journey Through Sports Analytics and Homography</title><link href="https://miguel-mendez-ai.com/2024/02/07/nfl-field-mapping.html" rel="alternate" type="text/html" title="NFL Field Mapping: A Journey Through Sports Analytics and Homography" /><published>2024-02-07T00:00:00+00:00</published><updated>2024-02-07T00:00:00+00:00</updated><id>https://miguel-mendez-ai.com/2024/02/07/nfl-field-mapping</id><content type="html" xml:base="https://miguel-mendez-ai.com/2024/02/07/nfl-field-mapping.html"><![CDATA[<p>I have been working in sports analytics for 2 years now. I am mainly focusing on the computer vision side of things but saying “sports analytics” is a good way to make it sound more interesting. My goal is simple: extract as much information as possible from sports event video feeds and ensure the data is high-quality. In order to achieve this, I must be able to pinpoint the real world location of the objects observed in the video feed. In other words, map pixels in the video feed to real world coordinates. That mapping is what I refer to as homography. I have already written a post in the company blog about this. If you are interested in why it matters and what it can be used for, you can check it out <a href="https://statsbomb.com/articles/football/creating-better-data-ai-homography-estimation/" target="_blank" rel="noopener noreferrer">here</a>.</p>

<p>Lately, I’ve been inspired by the incredible projects people have been creating with the help of Copilot. This sparked my curiosity to explore firsthand the experience of coding with a heavy reliance on this tool. I’ve decided to challenge myself by attempting to write some JavaScript code. So, be kind and withhold judgement on any of the code you see. After all, I’m no JavaScript developer; I’m essentially in GPT-4’s hands here 😎.</p>

<h2 id="the-goal">The goal</h2>

<p>The goal is straightforward: for any given American football NFL event video feed, I want to map the pixels in the video to their corresponding real-world coordinates. Essentially, I want to pinpoint the location of the ball, the players, the goal, etc. The simplest approach to tackle this problem consists of working at the frame level and figuring out how to match each image to a predefined pitch template.</p>

<p>When I joined <a href="https://statsbomb.com/" target="_blank" rel="noopener noreferrer">StatsBomb</a>, there was a similar and more advanced tool already developed. My goal here was just to be sure I could replicate the entire process from scratch and have a complete understanding of it. So I wanted to develop a simple web app that allows users to upload an image and then find the correspondence between that image and the pitch template. The pitch template is a basic image of the pitch, including lines and goalposts.</p>

<h2 id="the-result">The result</h2>

<p>Here’s the result for you to explore directly. There are just a few steps to follow:</p>

<ol>
  <li>Upload image of a NFL game</li>
  <li>Select at least four points in the image and the pitch template. Be sure to select the same points in both images.</li>
  <li>Click on the “Compute homography” button to see the warped pitch template overlaid on the uploaded image.</li>
</ol>

<p>Make sure to read the following sections for a deeper understanding of how it all works.</p>

<link rel="stylesheet" type="text/css" href="/libs/custom/homography.css" />

<div class="app-container">
    <div>
        <div id="button-container" class="button-container">
            <div>
                <input type="file" id="imageLoader" name="imageLoader" />
            </div>
            <div>
                <button id="removeLastPoint">Remove Last Point</button>
                <button id="computeHomography">Compute Homography</button>
            </div>
        </div>
        <div id="template-container" class="container">
            <div id="templateContainer" class="templateContainer">
                <canvas id="templateCanvas" class="templateCanvas"></canvas>
                <canvas id="pointsTemplateCanvas" class="pointsTemplateCanvas"></canvas>
            </div>
            <div id="canvasContainer" class="canvasContainer">
                <canvas id="imageCanvas" class="imageCanvas"></canvas>
                <canvas id="pointsImageCanvas" class="pointsImageCanvas"></canvas>
            </div>
        </div>
    </div>
    <script src="https://docs.opencv.org/master/opencv.js" type="text/javascript"></script>
    <script type="text/javascript" src="/libs/custom/homography.js"></script>
</div>

<p>You can find all javascript code in <a href="https://github.com/mmeendez8/mmeendez8.github.io/blob/main/libs/custom/homography.js" target="_blank" rel="noopener noreferrer">this file</a>.</p>

<h2 id="some-extra-details">Some extra details</h2>

<h3 id="pitch-template">Pitch template</h3>

<p>The pitch template is a controlled image I use to model the real-world pitch. By mapping image objects to it, I can directly measure their distances and angles. This is crucial for extracting meaningful information later on, such as ball position, player location, and player speed.</p>

<p>First, I must understand how the dimensions of an American football pitch are defined. <a href="https://turftank.com/us/academy/how-big-is-a-football-field/" target="_blank" rel="noopener noreferrer">This</a> page is an excellent resource. It’s worth noting that I solely focused on NFL dimensions, as NCAA fields differ slightly.</p>

<div class="post-center-image">
    <a href="/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/pitchdims.png">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/pitchdims-800-d46bc69a9.png" alt="Diagram showing NFL pitch dimensions including length and width in yards." srcset="/generated/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/pitchdims-400-fe42e0bc7.webp 400w, /generated/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/pitchdims-800-fe42e0bc7.webp 800w, /generated/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/pitchdims-1024-fe42e0bc7.webp 1024w" sizes="(max-width: 767px) 100vw, 80vw" width="1024" height="527" />
</a>

</div>

<p><em>NFL Pitch dimensions obtained from <a href="https://turftank.com/us/academy/how-big-is-a-football-field/" target="_blank" rel="noopener noreferrer">https://turftank.com/us/academy/how-big-is-a-football-field/</a></em></p>

<p>I created a simple image with the same resolution as the NFL pitch, <em>120 x 53.3 px</em>. This means one pixel in the image equals one yard in the real world. Next, I added end zones, hash marks, yard numbers, and all the necessary elements, each positioned accurately. I have to admit that although this task should be relatively simple and mechanical, it took me a while to achieve a decent result. Be sure to check the real code, but see below a small example that can help you realize the amount of handcrafting needed to create a good template:</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1">// Define the pitch dimensions and other constants</span>
    <span class="kd">const</span> <span class="nx">pitchHeight</span> <span class="o">=</span> <span class="mi">53</span> <span class="o">+</span> <span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">;</span>
    <span class="kd">const</span> <span class="nx">pitchWidth</span> <span class="o">=</span> <span class="mi">120</span><span class="p">;</span>
    <span class="kd">const</span> <span class="nx">lineWidth</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span> 
    <span class="kd">const</span> <span class="nx">ctxTemplate</span> <span class="o">=</span> <span class="nx">templateCanvas</span><span class="p">.</span><span class="nf">getContext</span><span class="p">(</span><span class="dl">'</span><span class="s1">2d</span><span class="dl">'</span><span class="p">);</span>

    <span class="c1">// Set size and color of template canvas</span>
    <span class="nx">ctxTemplate</span><span class="p">.</span><span class="nx">strokeStyle</span> <span class="o">=</span> <span class="dl">'</span><span class="s1">red</span><span class="dl">'</span><span class="p">;</span>
    <span class="nx">ctxTemplate</span><span class="p">.</span><span class="nx">fillStyle</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">black</span><span class="dl">"</span><span class="p">;</span>
    <span class="nx">ctxTemplate</span><span class="p">.</span><span class="nf">fillRect</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nx">templateCanvas</span><span class="p">.</span><span class="nx">width</span><span class="p">,</span> <span class="nx">templateCanvas</span><span class="p">.</span><span class="nx">height</span><span class="p">);</span>
    
    <span class="c1">// Translate to avoid cropping the sidelines</span>
    <span class="nx">ctxTemplate</span><span class="p">.</span><span class="nx">lineWidth</span> <span class="o">=</span> <span class="nx">lineWidth</span><span class="p">;</span>
    <span class="nx">ctxTemplate</span><span class="p">.</span><span class="nf">translate</span><span class="p">(</span><span class="nx">lineWidth</span><span class="p">,</span> <span class="nx">lineWidth</span><span class="p">);</span>

    <span class="c1">// Side lines</span>
    <span class="nf">addLine</span><span class="p">({</span> <span class="na">x</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="na">y</span><span class="p">:</span> <span class="mi">0</span> <span class="p">},</span> <span class="p">{</span> <span class="na">x</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="na">y</span><span class="p">:</span> <span class="nx">pitchHeight</span> <span class="p">});</span>
    <span class="nf">addLine</span><span class="p">({</span> <span class="na">x</span><span class="p">:</span> <span class="nx">pitchWidth</span><span class="p">,</span> <span class="na">y</span><span class="p">:</span> <span class="mi">0</span> <span class="p">},</span> <span class="p">{</span> <span class="na">x</span><span class="p">:</span> <span class="nx">pitchWidth</span><span class="p">,</span> <span class="na">y</span><span class="p">:</span> <span class="nx">pitchHeight</span> <span class="p">});</span>

    <span class="c1">// End lines</span>
    <span class="nf">addLine</span><span class="p">({</span> <span class="na">x</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="na">y</span><span class="p">:</span> <span class="mi">0</span> <span class="p">},</span> <span class="p">{</span> <span class="na">x</span><span class="p">:</span> <span class="nx">pitchWidth</span><span class="p">,</span> <span class="na">y</span><span class="p">:</span> <span class="mi">0</span> <span class="p">});</span>
    <span class="nf">addLine</span><span class="p">({</span> <span class="na">x</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="na">y</span><span class="p">:</span> <span class="nx">pitchHeight</span> <span class="p">},</span> <span class="p">{</span> <span class="na">x</span><span class="p">:</span> <span class="nx">pitchWidth</span><span class="p">,</span> <span class="na">y</span><span class="p">:</span> <span class="nx">pitchHeight</span> <span class="p">});</span>
</code></pre></div></div>

<p>I repeated this process for all pitch elements, resulting in the image below:</p>

<div class="post-center-image">
    <a href="/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/pitchtemplate.png">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/pitchtemplate-800-cf747f767.png" alt="NFL resulting pitch template image" srcset="/generated/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/pitchtemplate-400-fb2e1d535.webp 400w, /generated/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/pitchtemplate-800-fb2e1d535.webp 800w, /generated/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/pitchtemplate-1500-fb2e1d535.webp 1500w, /generated/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/pitchtemplate-1645-fb2e1d535.webp 1645w" sizes="(max-width: 767px) 100vw, 80vw" width="1645" height="824" />
</a>

</div>

<p class="image-caption"><em>NFL resulting pitch template image</em></p>

<p>Bear in mind though that you are observing a scaled-up version of a <em>120 x 53.3 px</em> image in the web app, adjusted to match your screen size.</p>

<h3 id="recovering-the-homography">Recovering the homography</h3>

<p>Homography maps the pitch template to the uploaded image, allowing for corresponding points between the two. The theory behind this is extensive and beyond this post’s scope. To be fair, I can’t do better than what my colleague Iñaki Rabanillo has done in his own blog. So, I will just refer you to his <a href="https://inakiraba91.github.io/projective-geometry-estimating-the-homography-matrix.html" target="_blank" rel="noopener noreferrer">post</a>, be sure to check it out since it is a brilliant piece of work.</p>

<p>To sum it up, the problem we need to solve consists mostly of finding a homography transformation that is represented by a 3x3 matrix. This will allow us to go from pixel coordinates $p_i$ in the image to real world coordinates $p_t$ in the template image. To do so, we just need to carry out a matrix multiplication:</p>

\[p_t = H \cdot p_i\]

<p>If you are now wondering how can you multiply a 3x3 matrix by a 2x1 vector, you are right. We need to add a 1 to the vector to make it a 3x1 vector. This is a common trick in computer vision and it is called homogeneous coordinates. It is a way to represent points in a way that makes it easier to perform transformations on them. Be sure to check <a href="https://inakiraba91.github.io/projective-geometry-building-the-homography-matrix-from-scratch.html" target="_blank" rel="noopener noreferrer">Iñaki’s post</a> for a deeper understanding of this.</p>

<p>The homography matrix $H$ is a 3x3 matrix that has 8 degrees of freedom. This means that we need at least 4 points in the real image and their corresponding pairs in the template image to solve for the homography. This is a simple problem to solve and there are a lot of libraries that can do it for you. I used OpenCV library although you could just create the system of equations and solve it using any linear algebra library (specially if you are used to work with Javascript more than me).</p>

<p>This is the code that retrieves the homography matrix from the list of points and applies it to the template image:</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="kd">function</span> <span class="nf">computeHomography</span><span class="p">()</span> <span class="p">{</span>
        <span class="c1">// Check that both lists of points have the same size and that their size is at least 4</span>
        <span class="k">if </span><span class="p">(</span><span class="nx">pointsImage</span><span class="p">.</span><span class="nx">length</span> <span class="o">!==</span> <span class="nx">pointsTemplate</span><span class="p">.</span><span class="nx">length</span> <span class="o">||</span> <span class="nx">pointsImage</span><span class="p">.</span><span class="nx">length</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">)</span> <span class="p">{</span>
            <span class="nf">alert</span><span class="p">(</span><span class="dl">'</span><span class="s1">Both lists of points must have the same size and contain at least 4 points</span><span class="dl">'</span><span class="p">);</span>
            <span class="k">return</span><span class="p">;</span>
        <span class="p">}</span>

        <span class="c1">// Convert points to cv.Mat format</span>
        <span class="kd">let</span> <span class="nx">imagePoints</span> <span class="o">=</span> <span class="nx">cv</span><span class="p">.</span><span class="nf">matFromArray</span><span class="p">(</span><span class="nx">pointsImage</span><span class="p">.</span><span class="nx">length</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nx">cv</span><span class="p">.</span><span class="nx">CV_32FC2</span><span class="p">,</span> <span class="nx">pointsImage</span><span class="p">.</span><span class="nf">flatMap</span><span class="p">(</span><span class="nx">point</span> <span class="o">=&gt;</span> <span class="p">[</span><span class="nb">Math</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="nx">point</span><span class="p">.</span><span class="nx">x</span><span class="p">),</span> <span class="nb">Math</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="nx">point</span><span class="p">.</span><span class="nx">y</span><span class="p">)]));</span>
        <span class="kd">let</span> <span class="nx">templatePoints</span> <span class="o">=</span> <span class="nx">cv</span><span class="p">.</span><span class="nf">matFromArray</span><span class="p">(</span><span class="nx">pointsTemplate</span><span class="p">.</span><span class="nx">length</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nx">cv</span><span class="p">.</span><span class="nx">CV_32FC2</span><span class="p">,</span> <span class="nx">pointsTemplate</span><span class="p">.</span><span class="nf">flatMap</span><span class="p">(</span><span class="nx">point</span> <span class="o">=&gt;</span> <span class="p">[</span><span class="nb">Math</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="nx">point</span><span class="p">.</span><span class="nx">x</span><span class="p">),</span> <span class="nb">Math</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="nx">point</span><span class="p">.</span><span class="nx">y</span><span class="p">)]));</span>

        <span class="c1">// Compute homography</span>
        <span class="kd">let</span> <span class="nx">homography</span> <span class="o">=</span> <span class="nx">cv</span><span class="p">.</span><span class="nf">findHomography</span><span class="p">(</span><span class="nx">templatePoints</span><span class="p">,</span> <span class="nx">imagePoints</span><span class="p">);</span>

        <span class="c1">// Check if homography is none because of colinear points</span>
        <span class="k">if </span><span class="p">(</span><span class="nx">homography</span><span class="p">.</span><span class="nf">empty</span><span class="p">())</span>
        <span class="p">{</span>
            <span class="nf">alert</span><span class="p">(</span><span class="dl">"</span><span class="s2">Could not found any homography for these sets of points. Be sure they are not colinear.</span><span class="dl">"</span><span class="p">);</span>
            <span class="k">return</span><span class="p">;</span>
        <span class="p">}</span>

        <span class="c1">// Warp the template image using the homography</span>
        <span class="kd">let</span> <span class="nx">warpedTemplate</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">cv</span><span class="p">.</span><span class="nc">Mat</span><span class="p">();</span>
        <span class="nx">cv</span><span class="p">.</span><span class="nf">warpPerspective</span><span class="p">(</span><span class="nx">templateImage</span><span class="p">,</span> <span class="nx">warpedTemplate</span><span class="p">,</span> <span class="nx">homography</span><span class="p">,</span> <span class="nx">sourceImage</span><span class="p">.</span><span class="nf">size</span><span class="p">());</span>

        <span class="c1">// Add the warped template to the source image</span>
        <span class="kd">let</span> <span class="nx">resultWeighted</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">cv</span><span class="p">.</span><span class="nc">Mat</span><span class="p">();</span>
        <span class="nx">cv</span><span class="p">.</span><span class="nf">addWeighted</span><span class="p">(</span><span class="nx">sourceImage</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nx">warpedTemplate</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nx">resultWeighted</span><span class="p">);</span>
        <span class="nx">cv</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="dl">'</span><span class="s1">imageCanvas</span><span class="dl">'</span><span class="p">,</span> <span class="nx">resultWeighted</span><span class="p">);</span>

        <span class="c1">// Clean up memory</span>
        <span class="nx">imagePoints</span><span class="p">.</span><span class="k">delete</span><span class="p">();</span>
        <span class="nx">templatePoints</span><span class="p">.</span><span class="k">delete</span><span class="p">();</span>
        <span class="nx">warpedTemplate</span><span class="p">.</span><span class="k">delete</span><span class="p">();</span>

        <span class="k">return</span> <span class="nx">homography</span><span class="p">;</span>
    <span class="p">}</span>
</code></pre></div></div>

<h3 id="projecting-between-image-and-template">Projecting between image and template</h3>

<p>Once we have the homography matrix, we can project any point in the image to the template (and viceversa). This just requires us to do a simple matrix multiplication. As it was mentioned before:</p>

\[p_t = H \cdot p_i\]

<p>It is also possible to do the opposite, projecting a point in the template to the image:</p>

\[p_i = H^{-1} \cdot p_t\]

<p>Quite simple, right?</p>

<h2 id="conclusion">Conclusion</h2>

<p>I hope you enjoyed this post and the web app. I have been using python for a long time and seeing things coming to life in the browser feels like something new now. I also had time to revisit some of the theory behind the homography and I am always happy to do so. I hope you found it interesting and that you can use it as a starting point for your own projects. I am sure there are a lot of improvements that can be done to this code and I would love to hear your thoughts on it.</p>

<p><em>Any ideas for future posts or is there something you would like to comment? Please feel free to reach out via <a href="https://twitter.com/mmeendez8" target="_blank" rel="noopener noreferrer">Twitter</a> or <a href="https://github.com/mmeendez8" target="_blank" rel="noopener noreferrer">Github</a></em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Dive into the world of sports analytics with a hands-on guide to mapping NFL fields to real-world coordinates using homography. Discover how computer vision techniques can transform video feed data into actionable insights, and explore the development of a web app for image calibration with the help of Copilot. Whether you're a sports enthusiast or a developer curious about the intersection of technology and sports, join to this journey through the fascinating process of sports field registration.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://miguel-mendez-ai.com/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/thumbnail.jpg" /><media:content medium="image" url="https://miguel-mendez-ai.com/assets/images/fullsize/posts/2024-02-07-nfl-field-mapping/thumbnail.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Deploying MLFlow in AWS with Terraform</title><link href="https://miguel-mendez-ai.com/2023/11/22/deploy-mlflow-terraform.html" rel="alternate" type="text/html" title="Deploying MLFlow in AWS with Terraform" /><published>2023-11-22T00:00:00+00:00</published><updated>2023-11-22T00:00:00+00:00</updated><id>https://miguel-mendez-ai.com/2023/11/22/deploy-mlflow-terraform</id><content type="html" xml:base="https://miguel-mendez-ai.com/2023/11/22/deploy-mlflow-terraform.html"><![CDATA[<p>Training and deploying machine learning models is a complex process. There are lots of steps involved – think data prep, model training, evaluating how good your model is, and then deploying it. Especially when it comes to training and evaluation, it’s super important to have a tool that makes life easier, something that lets us compare different experiments and track their performance (like losses, hyperparameters, metrics, etc.).</p>

<p>There are a bunch of tools out there for this, but we’re going to focus on one: <a href="https://mlflow.org/" target="_blank" rel="noopener noreferrer">MLFlow</a>. The goal of this post is to learn how to set up MLFlow in AWS, and we’re going to use Terraform for that. <a href="https://www.terraform.io/" target="_blank" rel="noopener noreferrer">Terraform</a> is awesome because (disclaimer: I hate its syntax) it lets you define all your infrastructure as code so you will never ever forget what button did you click to set up that AWS instance.</p>

<h2 id="why-mlflow">Why MLFlow?</h2>

<p>MLFlow is an open-source platform designed for managing the entire machine learning lifecycle. It’s great for tracking experiments, managing models, and even deploying them into production. Personally, I have primarily used MLFlow for tracking experiments. I became quite familiar with it in my previous job and really appreciated its features, especially the ability to create custom visualizations and share experiment links with colleagues. These links can also be embedded in reports, which is super handy.</p>

<p>I really missed using it and was eager to set it up in my current job. But, as often happens, more pressing tasks always seemed to take priority. To be fair, for basic tracking needs, you can get by with TensorBoard. It’s a straightforward tool that provides all the essentials. However, it starts to feel a bit overwhelming when you’re juggling a large number of experiments. Also, I did not have much experience with Terraform, just added a few lines here and there to existing configurations to set up some permissions and such. So, I decided to kill two birds with one stone and get into this project.</p>

<h2 id="what-do-we-need">What do we need?</h2>

<p>Before we dive in, let’s make sure we have got everything we need. We are going to use Terraform for setting up our infrastructure, so you’ll need that installed. This post assumes you’ve got Terraform ready and configured for your AWS account, so we’ll skip that part and focus on what we need to add to our Terraform configuration. If you haven’t set up Terraform yet, no problem – just follow <a href="https://learn.hashicorp.com/tutorials/terraform/aws-build?in=terraform/aws-get-started" target="_blank" rel="noopener noreferrer">this guide</a> for the setup. Also, I have deployed the MLFlow server in AWS EKS using <a href="https://fluxcd.io/" target="_blank" rel="noopener noreferrer">Flux</a>, a great tool for deploying services in Kubernetes. It’s not essential for this tutorial, though. Feel free to deploy the MLFlow server manually or with any other tool that you’re comfortable with.</p>

<p>Now, the MLFlow tracking server needs a few key components:</p>

<ul>
  <li>A <strong>database</strong> to store all your data</li>
  <li>A <strong>storage bucket</strong> for artifacts, like model checkpoints.</li>
  <li>A <strong>server</strong> to run the tracking server, which is essentially a Docker image with MLFlow installed.</li>
</ul>

<p>Let’s take it step by step and see how we can set up each of these components.</p>

<h2 id="database">Database</h2>

<p>MLFlow supports a bunch of different databases, but we are going to use Postgres – it’s popular, and I’m quite familiar with it.We will use AWS RDS to set up our database and its security group. Here’s what to add to your Terraform configuration:</p>

<p>First, we create a random password without special characters:</p>

<div class="language-hcl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Creates a random password for the database</span>
<span class="nx">resource</span> <span class="s2">"random_password"</span> <span class="s2">"mlflow_db"</span> <span class="p">{</span>
  <span class="nx">length</span>  <span class="o">=</span> <span class="mi">16</span>
  <span class="nx">special</span> <span class="o">=</span> <span class="kc">false</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Next, we set up a security group for the database that allows TCP traffic on port 5432 (typical for PostgreSQL databases). We are limiting incoming traffic to only our VPN. If you are not looking to restrict traffic, you can skip this step.</p>

<div class="language-hcl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This specifies the security group for the database</span>
<span class="nx">module</span> <span class="s2">"mlflow_db_security_group"</span> <span class="p">{</span>
  <span class="nx">source</span>  <span class="o">=</span> <span class="s2">"terraform-aws-modules/security-group/aws"</span>
  <span class="nx">version</span> <span class="o">=</span> <span class="s2">"~&gt; 4"</span>

  <span class="nx">name</span>        <span class="o">=</span> <span class="s2">"mlflow-db-sg"</span>
  <span class="nx">description</span> <span class="o">=</span> <span class="s2">"RDS Aurora ingress security group"</span>
  <span class="nx">vpc_id</span>      <span class="o">=</span> <span class="nx">aws_vpc</span><span class="err">.</span><span class="nx">main</span><span class="p">.</span><span class="nx">id</span> <span class="c1"># You need to use you own vpc id here</span>

  <span class="nx">ingress_with_cidr_blocks</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
      <span class="nx">from_port</span>   <span class="o">=</span> <span class="mi">5432</span>
      <span class="nx">to_port</span>     <span class="o">=</span> <span class="mi">5432</span>
      <span class="nx">protocol</span>    <span class="o">=</span> <span class="s2">"tcp"</span>
      <span class="nx">description</span> <span class="o">=</span> <span class="s2">"RDS Aurora access from within VPC"</span>
      <span class="nx">cidr_blocks</span> <span class="o">=</span> <span class="p">[</span><span class="nx">var</span><span class="p">.</span><span class="nx">your_vpn</span><span class="p">]</span>
    <span class="p">},</span>
  <span class="p">]</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Finally, we create a RDS Aurora cluster for the MLFlow database. The setup is straightforward, but you can always refer to the <a href="https://registry.terraform.io/modules/terraform-aws-modules/rds-aurora/aws/latest" target="_blank" rel="noopener noreferrer">official documentation</a> for more details. Notice how we use the earlier created password and security group to restrict access. We are creating a single writer instance, assuming limited traffic. If you expect more, consider adding a read replica in the instances map.</p>

<div class="language-hcl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Database configuration</span>
<span class="nx">module</span> <span class="s2">"mlflow_cluster_db"</span> <span class="p">{</span>
  <span class="nx">source</span>         <span class="o">=</span> <span class="s2">"terraform-aws-modules/rds-aurora/aws"</span>
  <span class="nx">version</span>        <span class="o">=</span> <span class="s2">"6.2.0"</span>

  <span class="nx">name</span>           <span class="o">=</span> <span class="s2">"mlflow-db"</span>
  <span class="nx">engine</span>         <span class="o">=</span> <span class="s2">"aurora-postgresql"</span>
  <span class="nx">engine_version</span> <span class="o">=</span> <span class="s2">"14.5"</span>
  <span class="nx">instance_class</span> <span class="o">=</span> <span class="s2">"db.r5.large"</span>
  <span class="nx">instances</span> <span class="o">=</span> <span class="p">{</span>
    <span class="nx">one</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="p">}</span>

  <span class="nx">database_name</span>          <span class="o">=</span> <span class="s2">"mlflow"</span>
  <span class="nx">master_username</span>        <span class="o">=</span> <span class="s2">"mlflow"</span>
  <span class="nx">create_random_password</span> <span class="o">=</span> <span class="kc">false</span>
  <span class="nx">master_password</span>        <span class="o">=</span> <span class="nx">random_password</span><span class="err">.</span><span class="nx">mlflow_db</span><span class="err">.</span><span class="nx">result</span>

  <span class="nx">create_security_group</span>  <span class="o">=</span> <span class="kc">false</span>
  <span class="nx">subnets</span>                <span class="o">=</span> <span class="nx">local</span><span class="p">.</span><span class="nx">subnets_ids_database</span>
  <span class="nx">vpc_security_group_ids</span> <span class="o">=</span> <span class="p">[</span><span class="nx">module</span><span class="p">.</span><span class="nx">mlflow_db_security_group</span><span class="p">.</span><span class="nx">security_group_id</span><span class="p">]</span>

  <span class="nx">storage_encrypted</span>   <span class="o">=</span> <span class="kc">true</span>
  <span class="nx">apply_immediately</span>   <span class="o">=</span> <span class="kc">true</span>
  <span class="nx">monitoring_interval</span> <span class="o">=</span> <span class="mi">10</span>

  <span class="nx">enabled_cloudwatch_logs_exports</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"postgresql"</span><span class="p">]</span>

  <span class="nx">tags</span> <span class="o">=</span> <span class="p">{</span>
    <span class="nx">Environment</span> <span class="o">=</span> <span class="s2">"dev"</span>
    <span class="nx">Terraform</span>   <span class="o">=</span> <span class="s2">"true"</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>That is about it for the database setup. Lastly, we need to store the DB user, password, and endpoint for later connection. I will use AWS Secrets Manager for this. How you manage secrets may vary; the only thing you really want to avoid is to store them in source code. You could also use AWS Parameter Store or Hashicorp Vault, for example. Here’s how to store the secrets in AWS Secrets Manager:</p>

<div class="language-hcl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">resource</span> <span class="s2">"aws_secretsmanager_secret"</span> <span class="s2">"mlflow_db_master_username_id"</span> <span class="p">{</span>
  <span class="nx">name</span>  <span class="o">=</span> <span class="s2">"mlflow-username"</span>
<span class="p">}</span>

<span class="nx">resource</span> <span class="s2">"aws_secretsmanager_secret_version"</span> <span class="s2">"mlflow_db_master_username"</span> <span class="p">{</span>
  <span class="nx">secret_id</span>     <span class="o">=</span> <span class="nx">aws_secretsmanager_secret</span><span class="p">.</span><span class="nx">mlflow_db_master_username_id</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nx">id</span>
  <span class="nx">secret_string</span> <span class="o">=</span> <span class="nx">module</span><span class="p">.</span><span class="nx">mlflow_cluster_db</span><span class="p">.</span><span class="nx">cluster_master_username</span>
<span class="p">}</span>

<span class="nx">resource</span> <span class="s2">"aws_secretsmanager_secret"</span> <span class="s2">"mlflow_db_master_password_id"</span> <span class="p">{</span>
  <span class="nx">name</span>  <span class="o">=</span> <span class="s2">"mlflow-password"</span>
<span class="p">}</span>

<span class="nx">resource</span> <span class="s2">"aws_secretsmanager_secret_version"</span> <span class="s2">"mlflow_db_master_password"</span> <span class="p">{</span>
  <span class="nx">secret_id</span>     <span class="o">=</span> <span class="nx">aws_secretsmanager_secret</span><span class="p">.</span><span class="nx">mlflow_db_master_password_id</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nx">id</span>
  <span class="nx">secret_string</span> <span class="o">=</span> <span class="nx">module</span><span class="p">.</span><span class="nx">mlflow_cluster_db</span><span class="p">.</span><span class="nx">cluster_master_password</span>
<span class="p">}</span>

<span class="nx">resource</span> <span class="s2">"aws_secretsmanager_secret"</span> <span class="s2">"mlflow_db_endpoint_id"</span> <span class="p">{</span>
  <span class="nx">name</span>  <span class="o">=</span> <span class="s2">"mlflow-db-writer-endpoint"</span>
<span class="p">}</span>

<span class="nx">resource</span> <span class="s2">"aws_secretsmanager_secret_version"</span> <span class="s2">"mlflow_db_endpoint"</span> <span class="p">{</span>
  <span class="nx">secret_id</span>     <span class="o">=</span> <span class="nx">aws_secretsmanager_secret</span><span class="p">.</span><span class="nx">mlflow_db_endpoint_id</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nx">id</span>
  <span class="nx">secret_string</span> <span class="o">=</span> <span class="nx">module</span><span class="p">.</span><span class="nx">mlflow_cluster_db</span><span class="p">.</span><span class="nx">cluster_endpoint</span>
<span class="p">}</span>
</code></pre></div></div>

<h2 id="storage-bucket-and-iam-role">Storage bucket and IAM Role</h2>

<p>Next up is setting up a storage bucket for all the artifacts, and we’re going to use AWS S3 for this. Here’s the Terraform configuration needed:</p>

<div class="language-hcl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">resource</span> <span class="s2">"aws_s3_bucket"</span> <span class="s2">"mlflow_artifacts_bucket"</span> <span class="p">{</span>
  <span class="nx">bucket</span> <span class="o">=</span> <span class="s2">"mlflow-artifacts-bucket"</span>
<span class="p">}</span>

<span class="nx">resource</span> <span class="s2">"aws_s3_bucket_ownership_controls"</span> <span class="s2">"mlflow_bucket_ownership"</span> <span class="p">{</span>
  <span class="nx">bucket</span> <span class="o">=</span> <span class="nx">aws_s3_bucket</span><span class="p">.</span><span class="nx">mlflow_artifacts_bucket</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nx">id</span>
  <span class="nx">rule</span> <span class="p">{</span>
    <span class="nx">object_ownership</span> <span class="o">=</span> <span class="s2">"BucketOwnerPreferred"</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="nx">resource</span> <span class="s2">"aws_s3_bucket_acl"</span> <span class="s2">"mlflow_bucket_acl"</span> <span class="p">{</span>
  <span class="nx">depends_on</span> <span class="o">=</span> <span class="p">[</span><span class="nx">aws_s3_bucket_ownership_controls</span><span class="p">.</span><span class="nx">mlflow_bucket_ownership</span><span class="p">]</span>

  <span class="nx">bucket</span> <span class="o">=</span> <span class="nx">aws_s3_bucket</span><span class="p">.</span><span class="nx">mlflow_artifacts_bucket</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nx">id</span>
  <span class="nx">acl</span>    <span class="o">=</span> <span class="s2">"private"</span>
<span class="p">}</span>
</code></pre></div></div>

<p>This configuration creates an S3 bucket named ‘mlflow-artifacts-bucket’. It also sets up ownership controls, ensuring new objects uploaded without an ACL are owned by the bucket owner. Additionally, it enforces a private ACL for the bucket, securing the stored data.</p>

<p>Now, MLFlow needs to access this bucket from EKS, so we’ll create an IAM Role for Service Accounts. Here’s how to do it:</p>

<div class="language-hcl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">resource</span> <span class="s2">"aws_iam_role"</span> <span class="s2">"mlflow-role"</span> <span class="p">{</span>
  <span class="nx">name</span> <span class="o">=</span> <span class="s2">"mlflow-role"</span>

  <span class="nx">assume_role_policy</span> <span class="o">=</span> <span class="nx">jsonencode</span><span class="p">({</span>
    <span class="nx">Version</span> <span class="o">=</span> <span class="s2">"2012-10-17"</span><span class="p">,</span>
    <span class="nx">Statement</span> <span class="o">=</span> <span class="p">[</span>
      <span class="p">{</span>
        <span class="nx">Action</span> <span class="o">=</span> <span class="s2">"sts:AssumeRole"</span><span class="p">,</span>
        <span class="nx">Effect</span> <span class="o">=</span> <span class="s2">"Allow"</span><span class="p">,</span>
        <span class="nx">Principal</span> <span class="o">=</span> <span class="p">{</span>
          <span class="nx">Service</span> <span class="o">=</span> <span class="s2">"ec2.amazonaws.com"</span> 
        <span class="p">},</span>
      <span class="p">},</span>
    <span class="p">],</span>
  <span class="p">})</span>
<span class="p">}</span>

<span class="c1"># Custom Policy for Specific S3 Bucket Access</span>
<span class="nx">resource</span> <span class="s2">"aws_iam_policy"</span> <span class="s2">"mlflow_s3_policy"</span> <span class="p">{</span>
  <span class="nx">name</span>   <span class="o">=</span> <span class="s2">"mlflow_s3_policy"</span>
  <span class="nx">policy</span> <span class="o">=</span> <span class="nx">jsonencode</span><span class="p">({</span>
    <span class="nx">Version</span> <span class="o">=</span> <span class="s2">"2012-10-17"</span><span class="p">,</span>
    <span class="nx">Statement</span> <span class="o">=</span> <span class="p">[</span>
      <span class="p">{</span>
        <span class="nx">Action</span> <span class="o">=</span> <span class="p">[</span>
          <span class="s2">"s3:GetObject"</span><span class="p">,</span>
          <span class="s2">"s3:GetObject*"</span><span class="p">,</span>
          <span class="s2">"s3:ListBucket"</span><span class="p">,</span>
          <span class="s2">"s3:PutObject"</span><span class="p">,</span>
          <span class="s2">"s3:DeleteObject"</span>
        <span class="p">],</span>
        <span class="nx">Effect</span>   <span class="o">=</span> <span class="s2">"Allow"</span><span class="p">,</span>
        <span class="nx">Resource</span> <span class="o">=</span> <span class="p">[</span>
          <span class="s2">"arn:aws:s3:::mlflow-artifacts-bucket"</span><span class="p">,</span>
          <span class="s2">"arn:aws:s3:::mlflow-artifacts-bucket/*"</span>
        <span class="p">]</span>
      <span class="p">}</span>
    <span class="p">]</span>
  <span class="p">})</span>
<span class="p">}</span>

<span class="c1"># Attach Custom Policy to the Role</span>
<span class="nx">resource</span> <span class="s2">"aws_iam_role_policy_attachment"</span> <span class="s2">"s3_policy_attachment"</span> <span class="p">{</span>
  <span class="nx">role</span>       <span class="o">=</span> <span class="nx">aws_iam_role</span><span class="p">.</span><span class="nx">mlflow-role</span><span class="p">.</span><span class="nx">name</span>
  <span class="nx">policy_arn</span> <span class="o">=</span> <span class="nx">aws_iam_policy</span><span class="p">.</span><span class="nx">mlflow_s3_policy</span><span class="p">.</span><span class="nx">arn</span>
  <span class="nx">depends_on</span> <span class="o">=</span> <span class="p">[</span><span class="nx">aws_iam_policy</span><span class="p">.</span><span class="nx">mlflow_s3_policy</span><span class="p">]</span>
<span class="p">}</span>
</code></pre></div></div>

<p>This code sets up a new role, <code class="language-plaintext highlighter-rouge">mlflow-role</code>, and attaches a custom policy, <code class="language-plaintext highlighter-rouge">mlflow_s3_policy</code>, to it. This policy grants specific permissions to access the S3 bucket we created earlier.</p>

<h2 id="docker">Docker</h2>

<p>Before we proceed, it’s important to note that the official MLFlow Docker image doesn’t include the necessary libraries for connecting with AWS S3 and Postgres. To address this, we’ll need to create a custom image that includes these libraries. This is done by crafting a Dockerfile like the one below:</p>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ghcr.io/mlflow/mlflow:v2.7.1</span>

<span class="k">RUN </span>apt-get <span class="nt">-y</span> update <span class="o">&amp;&amp;</span> <span class="se">\
</span>    apt-get <span class="nt">-y</span> <span class="nb">install </span>python3-dev build-essential pkg-config <span class="o">&amp;&amp;</span> <span class="se">\
</span>    pip <span class="nb">install</span> <span class="nt">--upgrade</span> pip <span class="o">&amp;&amp;</span> <span class="se">\
</span>    pip <span class="nb">install </span>psycopg2-binary boto3

<span class="k">CMD</span><span class="s"> ["bash"]</span>
</code></pre></div></div>

<p>This Dockerfile starts with the official MLFlow image and adds the required libraries. After crafting this file, you can build the image and push it to your ECR repository, or any other repository you prefer to use.</p>

<h2 id="deployment">Deployment</h2>

<p>Finally, we’ll set up the server to run the MLflow tracking server on AWS EKS. We typically use Flux for deploying services in Kubernetes, so we’ll create a <code class="language-plaintext highlighter-rouge">kustomization.yaml</code> file containing all the necessary resources. Alternatively, you could also deploy using Terraform, although I am less familiar with this method as we primarily utilize Flux for our deployments.</p>

<p>We will begin with defining secrets. Our secrets are stored in AWS Secrets Manager and accessed from Kubernetes. We use the <a href="https://github.com/external-secrets/kubernetes-external-secrets" target="_blank" rel="noopener noreferrer">External Secrets</a> package for simplicity. Here is what our <code class="language-plaintext highlighter-rouge">secrets.yaml</code> file looks like:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s2">"</span><span class="s">kubernetes-client.io/v1"</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ExternalSecret</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mlflow-secrets</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">monitoring</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">backendType</span><span class="pi">:</span> <span class="s">secretsManager</span>
  <span class="na">data</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span>  <span class="s">mlflow-username</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">AWS_SECRET_MLFLOW_USERNAME</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">mlflow-password</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">AWS_SECRET_MLFLOW_PASSWORD</span>
    <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s">mlflow-db-writer-endpoint</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">AWS_SECRET_MLFLOW_HOST</span>
</code></pre></div></div>

<p>Next, we need a service account for our deployment, defined in <code class="language-plaintext highlighter-rouge">service-account.yaml</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ServiceAccount</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mlflow-sa</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">eks.amazonaws.com/role-arn</span><span class="pi">:</span> <span class="s">arn:aws:iam::941819254007:role/mlflow-role</span>
</code></pre></div></div>

<p>This configuration uses the earlier created IAM role for accessing the S3 bucket.</p>

<p>We also need a service for our deployment, specified in <code class="language-plaintext highlighter-rouge">service.yaml</code> file:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mlflow</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">8080</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">http</span>
      <span class="na">targetPort</span><span class="pi">:</span> <span class="s">http</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app.kubernetes.io/name</span><span class="pi">:</span> <span class="s">mlflow</span>
</code></pre></div></div>

<p>This is a very simple service definition that just exposes port 8080 and targets all pods with the label <code class="language-plaintext highlighter-rouge">app.kubernetes.io/name: mlflow</code>.</p>

<p>Next, we need an ingress to expose our service to the outside world. We use AWS ALB Ingress Controller for this. Here’s what our <code class="language-plaintext highlighter-rouge">ingress.yaml</code> file looks like:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mlflow</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">kubernetes.io/ingress.class</span><span class="pi">:</span> <span class="s">alb</span>
    <span class="na">alb.ingress.kubernetes.io/scheme</span><span class="pi">:</span> <span class="s">internal</span>
    <span class="na">alb.ingress.kubernetes.io/target-type</span><span class="pi">:</span> <span class="s">ip</span>
    <span class="na">alb.ingress.kubernetes.io/ssl-redirect</span><span class="pi">:</span> <span class="s2">"</span><span class="s">443"</span>
    <span class="na">alb.ingress.kubernetes.io/listen-ports</span><span class="pi">:</span> <span class="s1">'</span><span class="s">[{"HTTPS":</span><span class="nv"> </span><span class="s">443}]'</span>
    <span class="na">alb.ingress.kubernetes.io/healthcheck-path</span><span class="pi">:</span> <span class="s">/</span>

<span class="na">spec</span><span class="pi">:</span>
  <span class="na">rules</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s">host-url</span>
    <span class="na">http</span><span class="pi">:</span>
      <span class="na">paths</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/</span>
          <span class="na">pathType</span><span class="pi">:</span> <span class="s">Prefix</span>
          <span class="na">backend</span><span class="pi">:</span>
            <span class="na">service</span><span class="pi">:</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">mlflow</span>
              <span class="na">port</span><span class="pi">:</span>
                <span class="na">name</span><span class="pi">:</span> <span class="s">http</span>
</code></pre></div></div>

<p>This Ingress configuration sets up access for our MLFlow tracking server. It is very simple, the configuration ensures SSL redirection to HTTPS on port 443 and that is is only accessible from within the VPC (internal). The rule specified routes traffic for the <code class="language-plaintext highlighter-rouge">host-url</code> to the MLflow service on the HTTP port.</p>

<p>Lastly, the deployment itself, outlined in <code class="language-plaintext highlighter-rouge">deployment.yaml</code> file:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">mlflow</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">monitoring</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">mlflow</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">mlflow</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">serviceAccount</span><span class="pi">:</span> <span class="s">mlflow-sa</span>
      <span class="na">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">app</span>
        <span class="na">image</span><span class="pi">:</span> <span class="s">add-your-image-here</span>
        <span class="na">command</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s2">"</span><span class="s">mlflow"</span>
        <span class="pi">-</span> <span class="s2">"</span><span class="s">server"</span>
        <span class="pi">-</span> <span class="s2">"</span><span class="s">--host=0.0.0.0"</span>
        <span class="pi">-</span> <span class="s2">"</span><span class="s">--backend-store-uri=postgresql://$(AWS_SECRET_MLFLOW_USERNAME):$(AWS_SECRET_MLFLOW_PASSWORD)@$(AWS_SECRET_MLFLOW_HOST):5432/mlflow"</span>
        <span class="pi">-</span> <span class="s2">"</span><span class="s">--default-artifact-root=s3://mlflow-artifacts-bucket"</span>
        <span class="pi">-</span> <span class="s2">"</span><span class="s">--port=8080"</span>
        <span class="na">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">http</span>
          <span class="na">containerPort</span><span class="pi">:</span> <span class="m">8080</span>
        <span class="na">envFrom</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">secretRef</span><span class="pi">:</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">mlflow-secrets</span>
        <span class="na">readinessProbe</span><span class="pi">:</span>
          <span class="na">httpGet</span><span class="pi">:</span>
            <span class="na">path</span><span class="pi">:</span> <span class="s">/</span>
            <span class="na">port</span><span class="pi">:</span> <span class="m">8080</span>
        <span class="na">startupProbe</span><span class="pi">:</span>
          <span class="na">initialDelaySeconds</span><span class="pi">:</span> <span class="m">10</span>
          <span class="na">httpGet</span><span class="pi">:</span>
            <span class="na">path</span><span class="pi">:</span> <span class="s">/</span>
            <span class="na">port</span><span class="pi">:</span> <span class="m">8080</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1G</span>
            <span class="na">cpu</span><span class="pi">:</span> <span class="s">500m</span>
          <span class="na">limits</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1G</span>
            <span class="na">cpu</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1"</span>
</code></pre></div></div>

<p>Key takeaways from this setup:</p>

<ol>
  <li>We deploy a single replica of the MLFlow pod.</li>
  <li>Replace <code class="language-plaintext highlighter-rouge">add-your-image-here</code> with the image we created earlier.</li>
  <li>Secrets created earlier are passed as environment variables in the <code class="language-plaintext highlighter-rouge">envFrom</code> section.</li>
  <li>Entrypoint is set to <code class="language-plaintext highlighter-rouge">mlflow server</code> and we pass the required arguments to connect to the database and the S3 bucket.</li>
  <li>Readiness and startup probes ensure the pod is fully operational before receiving requests.</li>
</ol>

<p>And that’s it! With these steps, our MLFlow server is ready to go and can start tracking experiments.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this post we have seen how to set up MLFlow in AWS using Terraform. We have seen how to set up a Postgres database, an S3 bucket, and a Kubernetes deployment for the MLFlow server. We have also seen how to create a custom Docker image that includes the required libraries to connect to the database and the S3 bucket.</p>

<p><em>Any ideas for future posts or is there something you would like to comment? Please feel free to reach out via <a href="https://twitter.com/mmeendez8" target="_blank" rel="noopener noreferrer">Twitter</a> or <a href="https://github.com/mmeendez8" target="_blank" rel="noopener noreferrer">Github</a></em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Explore this step-by-step guide on deploying MLFlow in AWS using Terraform. Learn how to effectively manage your machine learning lifecycle, set up a Postgres database, create a secure S3 bucket, and customize a MLFlow Docker image. Improve you Machine Learning experiment tracking and model management in the cloud.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://miguel-mendez-ai.com/assets/images/fullsize/posts/2023-11-22-deploy-mlflow-terraform/thumbnail.jpg" /><media:content medium="image" url="https://miguel-mendez-ai.com/assets/images/fullsize/posts/2023-11-22-deploy-mlflow-terraform/thumbnail.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Multi-Object Tracking by Detection: A Comprehensive Guide</title><link href="https://miguel-mendez-ai.com/2023/11/08/tracking-by-detection-overview.html" rel="alternate" type="text/html" title="Multi-Object Tracking by Detection: A Comprehensive Guide" /><published>2023-11-08T00:00:00+00:00</published><updated>2023-11-08T00:00:00+00:00</updated><id>https://miguel-mendez-ai.com/2023/11/08/tracking-by-detection-overview</id><content type="html" xml:base="https://miguel-mendez-ai.com/2023/11/08/tracking-by-detection-overview.html"><![CDATA[<h2 id="table-of-contents">Table of Contents</h2>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th> </th>
        <th>Year</th>
        <th>Appearence Features</th>
        <th>Camera Compensation</th>
        <th>HOTA MOT20</th>
        <th>Extra data</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><a href="#sort">SORT</a></td>
        <td>2016</td>
        <td>❌</td>
        <td>❌</td>
        <td> </td>
        <td>❌</td>
      </tr>
      <tr>
        <td><a href="#deepsort">DeepSORT</a></td>
        <td>2017</td>
        <td>✅</td>
        <td>❌</td>
        <td> </td>
        <td>✅</td>
      </tr>
      <tr>
        <td><a href="#bytetrack">ByteTrack</a></td>
        <td>2021</td>
        <td>✅</td>
        <td>❌</td>
        <td>61.3</td>
        <td>✅</td>
      </tr>
      <tr>
        <td><a href="#bot-sort">BoT-SORT</a></td>
        <td>2022</td>
        <td>✅</td>
        <td>✅</td>
        <td>63.3</td>
        <td>✅</td>
      </tr>
      <tr>
        <td><a href="#smiletrack">SMILEtrack</a></td>
        <td>2022</td>
        <td>✅</td>
        <td>✅  (?)</td>
        <td>63.4</td>
        <td>✅</td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="introduction">Introduction</h2>

<p>Tracking by detection is an object tracking approach that first detects objects in each frame of a video and then associates the detections across frames. This process involves matching detections by analyzing their location, appearance, or motion characteristics. Tracking by detection has become the most popular method for addressing object tracking due to the rapid development of reliable object detectors.</p>

<p>The intention of this blog is to keep myself updated with the bibliography of tracking by detection methods. My intention is to regularly update this blog with new information and resources I find interesting.
I have included the SORT and DeepSORT papers in the list, despite being older methods, as they laid the groundwork for many of the techniques covered here.</p>

<p>If you want to understand HOTA metric, I recommend you to read my other post about <a href="/2024/08/25/mot-tracking-metrics.html">tracking metrics</a></p>

<h2 id="sort">SORT</h2>

<p>It is a very good and simple work from 2016 that quickly became a standard in the field. The author’s main goal was to create the fastest possible tracker relying on the quality of the object detector predictions. Appearance features of the objects are not used; the system relies solely on bounding box position and size.</p>

<p>They employ two classical methods:</p>

<ul>
  <li>
    <p><strong>Kalman Filter:</strong> is in charge of handling motion prediction, this is, figuring out where a track is going to move in the next frame given previous states. Track states are modeled with six different variables:</p>

\[\mathbf{x} = [u,v,s,r,\dot{u},\dot{v},\dot{s}]^T,\]

    <p>These are the center of the target bounding box ($u, v$), the scales and aspect ratio of it ($s, r$) and their velocity components ($\dot{u},\dot{v},\dot{s}$).</p>
  </li>
  <li>
    <p><strong>Hungarian method:</strong> used in the data association step to match new predictions with tracks based on IoU metric.</p>
  </li>
</ul>

<div class="post-center-image">
<a href="/assets/svg/posts/2023-11-08-tracking-by-detection-overview/sort.svg" class="lightbox-image">
 <img src="/assets/svg/posts/2023-11-08-tracking-by-detection-overview/sort.svg" width="850" loading="lazy" alt="SORT architecture diagram" />
</a>
</div>

<p class="image-caption"><em>SORT architecture diagram</em></p>

<ol>
  <li>An object detector returns bounding boxes for frame 0.</li>
  <li>In T=0, a new track is created for each of the predicted bounding boxes</li>
  <li>KF will predict a new position for each of the tracks</li>
  <li>Object detector returns bounding boxes for frame 1</li>
  <li>These bounding boxes are associated with tracks positions predicted by KF</li>
  <li>New tracks are created for unmatched bounding boxes</li>
  <li>Unmatched tracks can be terminated if they are not matched to any detection for $T_{Lost}$ frames.</li>
  <li>Matched tracks and new tracks are passed to the next time step</li>
  <li>Back to 3</li>
</ol>

<h2 id="deepsort">DeepSORT</h2>

<p>DeepSORT is an extension of SORT that uses appearance features. It enhances SORT by adding a simple CNN extension that extracts appearance features from bounding boxes, improving object tracking, especially during occlusions. An object can be re-identified using appearance similarity after being occluded for a long period of time</p>

<p>Each track maintains a gallery of the last \(n\) appearance descriptors, enabling cosine distance calculations between new detections and descriptors. Track age, determined by frames since the last association, plays a crucial role in the association process. DeepSORT adopts a cascade approach, prioritizing tracks with lower ages over a single-step association between predicted Kalman states and new measurements.</p>

<div class="post-center-image">
<a href="/assets/svg/posts/2023-11-08-tracking-by-detection-overview/deepsort.svg" class="lightbox-image">
 <img src="/assets/svg/posts/2023-11-08-tracking-by-detection-overview/deepsort.svg" width="850" loading="lazy" alt="DeepSORT architecture diagram" />
</a>
</div>

<p class="image-caption"><em>DeepSORT architecture diagram</em></p>

<p>There is a small modification on the Kalman Filter prediction step that is included in the <a href="https://github.com/nwojke/deep_sort/blob/master/deep_sort/kalman_filter.py#L108" target="_blank" rel="noopener noreferrer">code</a> but not mentioned in the original paper. The matrices \(Q\), \(R\) of the Kalman Filter were chosen in SORT to be time indepent, however in DeepSORT it was suggested to choose \(Q%\), \(R\) as functions of the scale of the bounding box. This can be due to the scale is less likely to change over time than other features and it can be also be used to compensate for changes in camera’s viewpoint.</p>

<p>The cascade association step would look like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">track_age</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">maximum_age</span><span class="p">):</span>
    <span class="n">tracks_to_associate</span> <span class="o">=</span> <span class="nf">get_tracks_with_age</span><span class="p">(</span><span class="n">tracks</span><span class="p">,</span> <span class="n">track_age</span><span class="p">)</span>
    <span class="nf">associate</span><span class="p">(</span><span class="n">tracks_to_associate</span><span class="p">,</span> <span class="n">detections</span><span class="p">)</span>
    <span class="nf">remove_associated_detections</span><span class="p">(</span><span class="n">detections</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="bytetrack">ByteTrack</h2>

<p>ByteTrack is a recent object tracking algorithm that proposes a simple but effective optimization for the data association step. Most methods filter out detections with low confidence scores. This is because low-confidence detections are more likely to be false positives, or to correspond to objects that are not present in the scene. However, this can lead to problems when tracking objects that are partially occluded or that undergo significant appearance changes.</p>

<p>ByteTrack addresses this problem by using all detections, regardless of their confidence score. The algorithm works in two steps:</p>

<ol>
  <li><strong>High-confidence detections</strong>: High-confidence detections are associated with tracks using intersection-over-union (IoU) or appearance features. Both approaches are evaluated in the results section of the paper.</li>
  <li><strong>Low-confidence detections</strong>: Low-confidence detections are associated with tracks using only IoU. This is because low-confidence detections are more likely to be spurious or inaccurate, so it is important to be more conservative when associating them with tracks.</li>
</ol>

<div class="post-center-image">
<a href="/assets/svg/posts/2023-11-08-tracking-by-detection-overview/bytetrack.svg" class="lightbox-image">
 <img src="/assets/svg/posts/2023-11-08-tracking-by-detection-overview/bytetrack.svg" width="850" loading="lazy" alt="ByteTrack architecture diagram" />
</a>
</div>

<p class="image-caption"><em>ByteTrack architecture diagram</em></p>

<p>The ByteTrack algorithm has been shown to be very effective and it is currently among the top-performing methods on the <a href="https://paperswithcode.com/sota/multi-object-tracking-on-mot20-1" target="_blank" rel="noopener noreferrer">MOT Challenge leaderboard</a>.</p>

<h2 id="bot-sort">BoT-SORT</h2>

<p>I personally love the BoT-SORT paper. It is build upon ByteTrack and it combines three different ideas that work very well together. These are:</p>

<ol>
  <li>
    <p><strong>Kalman Filter update</strong>: SORT introduced a way of modelling the track state vector using a seven-tuple \(\mathbf{x} = [x_c,y_c,a,h,\dot{x_c},\dot{y_c},\dot{s}]^T\). BoT-SORT proposes to replace the scale and aspect ratio of the bounding box  (\(s\), \(a\)) with the widht and height (\(w\), \(h\)) to create an eight-tuple:</p>

\[\mathbf{x} = [x_c,y_c,w,h,\dot{x_c},\dot{y_c},\dot{w}, \dot{h}]^T\]

    <p>They also choose Q, R matrices from the Kalman Filter as functions of the bounding box width and height. Recall that in DeepSORT, only the scale of the bounding box influenced on the Q, R matrices (see section 3.1 of <a href="https://arxiv.org/pdf/2206.14651v2.pdf" target="_blank" rel="noopener noreferrer">BoT-SORT paper</a> for more details).</p>
  </li>
  <li>
    <p><strong>Camera Motion Compensation</strong>: In dynamic camera situations, objects that are static can appear to move, and objects that are moving can appear to be static. The Kalman Filter does not take camera motion into account for its predictions, so BoT-SORT proposes to incorporate this knowledge. To do this, they use the global motion compensation technique (GMC) from the OpenCV Video Stabilization module. This technique extracts keypoints from consecutive frames and computes the homography matrix between the matching pairs. This matrix can then be used to transform the prediction bounding box from the coordinate system of frame \(k − 1\) to the coordinates of the next frame \(k\) (see section 3.2 of <a href="https://arxiv.org/pdf/2206.14651v2.pdf" target="_blank" rel="noopener noreferrer">BoT-SORT paper</a> to a full formulation on how incorporate the homography matrix in the prediction step).</p>

    <div class="post-center-image">
     <a href="/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/cmc.png">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/cmc-800-3538f10c9.png" alt="Camera movement example" srcset="/generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/cmc-400-ed5e82503.webp 400w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/cmc-800-ed5e82503.webp 800w, /generated/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/cmc-1152-ed5e82503.webp 1152w" sizes="(max-width: 767px) 100vw, 80vw" width="1152" height="1296" />
</a>

 </div>

    <p class="image-caption"><em>Player is static on the pitch while throwing the ball but location on the image changes due to camera movement.</em></p>
  </li>
  <li>
    <ol>
      <li><strong>IoU - ReID Fusion</strong>:  BoT-SORT proposes a new way of solving the association step by combining motion and appearance information. The cost matrix elements are computed as follows:</li>
    </ol>

\[\hat{d}^{cos}_{i,j} = 
 \begin{equation}
 \begin{cases}
 0.5 \cdot {d}^{cos}_{i,j}, ({d}^{cos}_{i,j} &lt; \theta_{emb}) \hat{} ({d}^{iou}_{i,j} &lt; \theta_{iou})\\
 1, \text{otherwise}
 \end{cases}
 \end{equation}\]

\[C_{i,j} = min(d^{iou}_{i,j}, \hat{d}^{cos}_{i,j})\]

    <p>The appearence distance is recomputed as shown in the first equation. The idea is to filter out pairs with large iou or large appearance distance (two different thresholds are used here).  Then, the cost matrix element is updated as the minimum between the IoU and the new appearance distance. This method seems to be handcrafted, and the authors likely spent a significant amount of time evaluating different thresholds on the MOT17 dataset to arrive at this formulation. Note thresholds are callibrated using MOT17 validation set.</p>
  </li>
</ol>

<div class="post-center-image">
<a href="/assets/svg/posts/2023-11-08-tracking-by-detection-overview/botsort.svg" class="lightbox-image">
 <img src="/assets/svg/posts/2023-11-08-tracking-by-detection-overview/botsort.svg" width="850" loading="lazy" alt="BoT-SORT architecture diagram" />
</a>
</div>

<p class="image-caption"><em>BoT-SORT architecture diagram</em></p>

<h2 id="smiletrack">SMILEtrack</h2>

<p>This method currently holds the title of being the State-of-the-Art (SOTA) in the MOT17 and MOT20 datasets. It builds upon ByteTrack but throws in a handful of fresh ideas designed to give appearance features more importance.</p>

<p>I spent a couple hours trying to understand the paper but I have to admit it felt very confusing to me, so I went straight to the <a href="https://github.com/pingyang1117/SMILEtrack_Official" target="_blank" rel="noopener noreferrer">code</a>. Things got even trickier there; I noticed quite a few things that didn’t align with what was mentioned in the paper. As a results, so I opened an <a href="https://github.com/pingyang1117/SMILEtrack_Official/issues/3" target="_blank" rel="noopener noreferrer">issue</a> on the project’s GitHub repository. I’ll update this section once I hear back from the authors.</p>

<!-- Let's see what they are:

1. **Similarity Learning Module (SLM)**: It is a Siamese network that computes appearence similarity between two objects using a Patch Self-Attention (PSA) block. Think about it as a boosted feature descriptor that incorporates attention mechanism following ViT style.

2. **Similarity Matching Cascade (SMC)**: Very similar to ByteTrack, it splits the data association step in two parts depending on detection scores. First, high confidence detections are tried to be matched with the tracks, for then proceeding with the low confidence ones. In both cases, IoU and appearence features are used to compute the cost matrix. The key addition is a new **GATE function** that is used right after the high confidence association. Unmatched objects with high scores might find matches in subsequent frames due to occlusions or lighting changes. When an object passes this GATE function, a new track is created for it.

Idea is, if IoU is high but they don't look alike, probably occlusion. If they don't match but they have some past track that looks alike, create a new track for it (this i don't understand very well)
There is camera motion correction in the code but not in the paper!!!!
 -->

<h2 id="references">References</h2>

<ul>
  <li>[<a href="https://arxiv.org/pdf/1602.00763.pdf" target="_blank" rel="noopener noreferrer">1</a>] Bewley, A., Ge, Z., Ott, L., Ramos, F., &amp; Upcroft, B. (2016, September). Simple online and realtime tracking. In 2016 IEEE international conference on image processing (ICIP) (pp. 3464-3468). IEEE.</li>
  <li>[<a href="https://arxiv.org/pdf/1703.07402.pdf" target="_blank" rel="noopener noreferrer">2</a>] Wojke, N., Bewley, A., &amp; Paulus, D. (2017, September). Simple online and realtime tracking with a deep association metric. In 2017 IEEE international conference on image processing (ICIP) (pp. 3645-3649). IEEE.</li>
  <li>[<a href="https://arxiv.org/pdf/2110.06864.pdf" target="_blank" rel="noopener noreferrer">3</a>] Zhang, Y., Sun, P., Jiang, Y., Yu, D., Weng, F., Yuan, Z., … &amp; Wang, X. (2022, October). Bytetrack: Multi-object tracking by associating every detection box. In European Conference on Computer Vision (pp. 1-21). Cham: Springer Nature Switzerland.</li>
  <li>[<a href="https://arxiv.org/pdf/2206.14651.pdf" target="_blank" rel="noopener noreferrer">4</a>] Aharon, N., Orfaig, R., &amp; Bobrovsky, B. Z. (2022). BoT-SORT: Robust associations multi-pedestrian tracking. arXiv preprint arXiv:2206.14651.</li>
  <li>[<a href="https://arxiv.org/pdf/2211.08824.pdf" target="_blank" rel="noopener noreferrer">5</a>] Wang, Y. H. (2022). SMILEtrack: SiMIlarity LEarning for Multiple Object Tracking. arXiv preprint arXiv:2211.08824.</li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Dive into the complexities of object tracking in computer vision with this detailed overview of tracking by detection. Discover the intricacies and advancements of key algorithms like SORT, DeepSORT, and ByteTrack, and learn to select the right one for your project. From foundational concepts to cutting-edge techniques, the performance and applications of multi-object tracking for real-world scenarios is dissected. SORT vs DeepSORT vs ByteTrack vs BoT-SORT vs SMILEtrack.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://miguel-mendez-ai.com/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/thumbnail.png" /><media:content medium="image" url="https://miguel-mendez-ai.com/assets/images/fullsize/posts/2023-11-08-tracking-by-detection-overview/thumbnail.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Creating Better Data: AI &amp;amp; Homography Estimation</title><link href="https://miguel-mendez-ai.com/2023/09/22/homography-estimation.html" rel="alternate" type="text/html" title="Creating Better Data: AI &amp;amp; Homography Estimation" /><published>2023-09-22T00:00:00+00:00</published><updated>2023-09-22T00:00:00+00:00</updated><id>https://miguel-mendez-ai.com/2023/09/22/homography-estimation</id><content type="html" xml:base="https://miguel-mendez-ai.com/2023/09/22/homography-estimation.html"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Revolutionizing Sports Analytics with Artificial Intelligence: Discover how StatsBomb leverages cutting-edge AI and deep learning to redefine sports analytics using homography estimation, providing unparalleled insights into player performance and game strategies. Dive into the innovative world of data-driven sports analysis with StatsBomb's latest breakthroughs.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://miguel-mendez-ai.com/assets/images/fullsize/posts/2023-09-22-homography-estimation/thumbnail.jpg" /><media:content medium="image" url="https://miguel-mendez-ai.com/assets/images/fullsize/posts/2023-09-22-homography-estimation/thumbnail.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Image Transmission for Computer Vision: A Comparison of Torchserve’s REST and gRPC</title><link href="https://miguel-mendez-ai.com/2023/02/24/torchserve-grpc.html" rel="alternate" type="text/html" title="Image Transmission for Computer Vision: A Comparison of Torchserve’s REST and gRPC" /><published>2023-02-24T00:00:00+00:00</published><updated>2023-02-24T00:00:00+00:00</updated><id>https://miguel-mendez-ai.com/2023/02/24/torchserve-grpc</id><content type="html" xml:base="https://miguel-mendez-ai.com/2023/02/24/torchserve-grpc.html"><![CDATA[<p>Special thanks to <a href="https://www.linkedin.com/in/jguzmanfd/" target="_blank" rel="noopener noreferrer">Javier Guzman</a> for working with me in completing the benchmarking discussed in this post.</p>

<p>In the past few weeks, we have been exploring the potential advantages of adopting gRPC to enhance the performance of our services. Although I have conducted extensive research on this topic, I have not been able to find relevant information that specifically addresses our use case, which involves transmitting images to a model server and receiving a response in the most efficient manner. While there are numerous benchmarks that demonstrate significant performance improvements when migrating from REST to gRPC using structured data, it has been challenging to locate a similar benchmark for image transmission… And that is the main reason behind this post!</p>

<p>All the code for the different benchmarks can be found in <a href="https://github.com/mmeendez8/grpc_vs_rest" target="_blank" rel="noopener noreferrer">this Github repository</a>. You can find instructions in the README file. It’s important to note that our primary objective was to conduct this testing on our cloud infrastructure, where both the servers and clients were deployed on the same Kubernetes cluster. This allowed us to replicate a real-world scenario as closely as possible.</p>

<h2 id="some-thoughts-on-grpc">Some thoughts on gRPC</h2>

<p>When you start reading about gRPC, you soon realize that it involves two main features that can really help you to speed up your system communications.</p>

<h3 id="http2">HTTP2</h3>

<p>gRPC is built on the HTTP/2 protocol, which was specifically designed to address the latency issues of its predecessor, HTTP/1.1. There are two key features of HTTP/2 that are particularly relevant to our benchmarking efforts:</p>

<ul>
  <li>
    <p><strong>Multiplexed streams</strong>: With HTTP/2, multiple requests and responses can be transmitted over a single connection. While HTTP/1.1 can also reuse connections through pooling, the ability to multiplex streams becomes more important as the number of servers increases. When multiple HTTP requests are performed in a very short span of time, HTTP/1.1 has no way to share those connections. Therefore, it will create new connections to the content server for each HTTP request (see <a href="https://blog.codavel.com/http2-multiplexing" target="_blank" rel="noopener noreferrer">here</a> for a extended explanation)</p>
  </li>
  <li>
    <p><strong>Binary protocol</strong>: Unlike HTTP/1.1, which is text-based, HTTP/2 uses a binary protocol which facilitates more efficient parsing. This can have a significant impact on performance, particularly when dealing with large datasets such as images.</p>
  </li>
</ul>

<h3 id="protobuf">Protobuf</h3>

<p>Protocol Buffers, also known as Protobuf, is a language-agnostic binary serialization format developed by Google. It is used for efficient data <strong>serialization of structured data</strong> and communication between applications. It is faster than JSON for two reasons:</p>

<ul>
  <li><strong>Messages are shorter</strong>. In Protobuf messages do not contain any metadata or extra information such as field names and data types. This is not needed since the data schema has been strictly predefined in the <code class="language-plaintext highlighter-rouge">.proto</code> file. It also uses a compact binary representation, variable-length encoding, which means that the number of bytes required to represent a value depends on its size.</li>
  <li><strong>Serialization is faster</strong>. Converting messages to and from bytes is faster than in JSON because of its binary format and predefined schema. Decoding can be optimized and parallelized.</li>
</ul>

<p>In <a href="https://nilsmagnus.github.io/post/proto-json-sizes/" target="_blank" rel="noopener noreferrer">this post</a> you can see a good comparison of Protobuf vs JSON sizes for structured data. TLDR: Protobuf is always smaller than gzipped json but seems to lose its clear advantage when mesage sizes are large.</p>

<h3 id="how-does-this-apply-to-images">How does this apply to images?</h3>

<p>Structured data is text that has been predefined and formatted to a set structure. Protobuf can take advantage of the schema definitions of the data to speed up serialization and compression size. However, images do not fall under the category of structured text. Basically if you want to convert an image to bytes in an efficient manner and without losing information you have to use specific handcrafted methods that have been carefully designed for this, such as JPEG, PNG… In other words, Protobuf is not going to help you here since compression and serialization will depend on your image library. See this example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># create random 100x100 rgb image
</span><span class="n">image</span> <span class="o">=</span> <span class="n">numpy</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="mi">255</span>
<span class="c1"># serialize image to jpg using opencv
</span><span class="n">encoded_image</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">imencode</span><span class="p">(</span><span class="sh">"</span><span class="s">.jpg</span><span class="sh">"</span><span class="p">,</span> <span class="n">image</span><span class="p">)[</span><span class="mi">1</span><span class="p">].</span><span class="nf">tobytes</span><span class="p">()</span>
<span class="c1"># fake send with grpc
</span><span class="n">grpc</span><span class="p">.</span><span class="nf">send</span><span class="p">(</span><span class="n">encoded_image</span><span class="p">)</span>
</code></pre></div></div>

<p>The key feature here is that Protobuf is not really helping. Given that it is one of key points of gRPC, differences between REST and gRPC cannot be that high here… Let’s check this with real numbers:</p>

<h2 id="1-base-benchmark">1. Base benchmark</h2>

<p>First thing we wanted to do is check if we were able to reproduce those benchmarks we found on the web. The idea is simple, create two equivalent REST and gRPC servers and measure the time they take to process and respond to different requests.
The gRPC server has been implemented using <a href="https://grpc.io/docs/languages/python/basics/" target="_blank" rel="noopener noreferrer">python grpc library</a> and we have used <a href="https://fastapi.tiangolo.com/" target="_blank" rel="noopener noreferrer">FastAPI</a> for the REST one.</p>

<div class="post-center-image">
<a href="/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/cat_bytes.png">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/cat_bytes-768-253323f5f.png" alt="Cat being compressed to bytes" srcset="/generated/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/cat_bytes-400-b0be83dae.webp 400w, /generated/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/cat_bytes-768-b0be83dae.webp 768w" sizes="(max-width: 767px) 100vw, 80vw" width="768" height="768" />
</a>

</div>

<p class="image-caption"><em>This is what Stable Diffusion creates with the prompt “an image of a cat is being encoded into a chunk of bytes”</em></p>

<p>We decided to measure three different requests and using a single response for all of them. The gRPC <code class="language-plaintext highlighter-rouge">.proto</code> file for those requests looks like the following:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BasicRequest</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Structured data request, we expect to match online benchmarks with this
    </span><span class="sh">"""</span>
    <span class="n">field1</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">field2</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">field3</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">field4</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>

<span class="k">class</span> <span class="nc">ImageBase64Request</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Encode image as a string using Base64 encoding. 
    This is a very bad solution (but simple to do) that should always be avoided
    </span><span class="sh">"""</span>
    <span class="n">image</span><span class="p">:</span> <span class="nb">str</span>

<span class="k">class</span> <span class="nc">ImageBinaryRequest</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Contains an image encoded as bytes.
    </span><span class="sh">"""</span>
    <span class="n">image</span><span class="p">:</span> <span class="nb">bytes</span>

<span class="k">class</span> <span class="nc">BasicResponse</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">prediction1</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span>
    <span class="n">prediction2</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>
</code></pre></div></div>

<p>Note REST’s requests and responses are identical to these so we can make a fair comparison.</p>

<p>Our client does a very simple thing, sends twenty concurrent requests to each server and waits for a response. It repeats this ten times for then computing the average time it took. Pseudocode for the client it is shown below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">start</span><span class="o">=</span> <span class="nf">time</span><span class="p">()</span>
    <span class="nf">send_concurrent_request_to_specific_server</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">times</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="nf">start</span><span class="p">())</span>

<span class="n">average_time</span> <span class="o">=</span> <span class="nf">mean</span><span class="p">(</span><span class="n">times</span><span class="p">)</span>
</code></pre></div></div>

<p>We tested this for three different image sizes. Results are collected below:</p>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th> </th>
        <th>Basic (0.001 MB)</th>
        <th>B64 (0.306 MB)</th>
        <th>Binary (0.229 MB)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>REST</td>
        <td>0.0723</td>
        <td>0.0943</td>
        <td>0.0572</td>
      </tr>
      <tr>
        <td>gRPC</td>
        <td>0.0093 (x7.7)</td>
        <td>0.0179 (x5.2)</td>
        <td>0.0120 (x4.7)</td>
      </tr>
    </tbody>
  </table>

  <p>Table 1. Results for small images: 360x640</p>

</div>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th> </th>
        <th>Basic (0.001 MB)</th>
        <th>B64 (1.160 MB)</th>
        <th>Binary (0.870 MB)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>REST</td>
        <td>0.0611</td>
        <td>0.2350</td>
        <td>0.0872</td>
      </tr>
      <tr>
        <td>gRPC</td>
        <td>0.0090 (x6.7)</td>
        <td>0.0926 (x2.5)</td>
        <td>0.0570 (x1.5)</td>
      </tr>
    </tbody>
  </table>

  <p>Table 2. Results for medium images: 720x1280</p>

</div>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th> </th>
        <th>Basic (0.001 MB)</th>
        <th>B64 (3.094 MB)</th>
        <th>Binary (2.320 MB)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>REST</td>
        <td>0.0583</td>
        <td>0.8056</td>
        <td>0.1909 (x1.03)</td>
      </tr>
      <tr>
        <td>gRPC</td>
        <td>0.0097 (x6)</td>
        <td>0.2793 (x2.9)</td>
        <td>0.1974</td>
      </tr>
    </tbody>
  </table>

  <p>Table 3. Results for large images: 1080x1920</p>

</div>

<p>We can extract some conclussion from previous tables:</p>

<ol>
  <li>gRPC achieves around a x6 improvement with respect to REST for structured data (Basic column). The results <a href="https://medium.com/@EmperorRXF/evaluating-performance-of-rest-vs-grpc-1b8bdf0b22da#:~:text=gRPC%20is%20roughly%207%20times,of%20HTTP%2F2%20by%20gRPC." target="_blank" rel="noopener noreferrer">match online benchmarks</a> and we know this is because we are taking advantage of Protobuf serialization and HTTP2 protocol.</li>
  <li>For Base64 and Binary we observe a relation between image size and gRPC performance. As the image size increase, the difference between REST and gRPC are smaller.</li>
  <li>In the Base64 case, gRPC helps to serialize faster and in a more optimal way the string. We know from <a href="https://nilsmagnus.github.io/post/proto-json-sizes/" target="_blank" rel="noopener noreferrer">this post</a> that Protobuf loses its advantage when message size increases.</li>
  <li>Binary is a special case as we know we are not getting any advantage from using Protobuf for our serialization and message size (this is determined by the image format we chose). On the contrary it is harming our performance. There is still  some encoding going on in Protobuf, since it needs to format our chunk of image bytes inside the Protobuf message format. This little thing might be making REST as good as gRPC for large images!</li>
</ol>

<h2 id="torchserve-benchmark">Torchserve benchmark</h2>

<p>I have been using <a href="https://pytorch.org/serve/" target="_blank" rel="noopener noreferrer">TorchServe</a> for a while now and I am quite happy with it. It provides all the flexibility I need and it is quite simple to set up. Model handlers allow you to customize every detail for your specific model without really worrying about other complex things such as batching and queing requests. 
I do not intend to give an overview of TorchServe or make a comparison of its advantages compared to other inference servers, I will leave that for a plausible future post.</p>

<p>The documentation for Torchserve’s <a href="https://pytorch.org/serve/grpc_api.html" target="_blank" rel="noopener noreferrer">gRPC API</a> could be improved, as it currently requires users to download the official repository to generate a Python gRPC client stub from the proto files. However, I have attached these files to the repository, so you can easily run the benchmark without having to worry about this step.</p>

<p>The experiment is very similar to the previous one, sending 20 concurrent request and repeating that 10 times to measure the average time. I am going to use one of the pytorch vision model examples, <a href="https://pytorch.org/hub/pytorch_vision_densenet/" target="_blank" rel="noopener noreferrer">densenet161</a>. The model is not important here since we do not really care about inference results. Let’s see some results:</p>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th> </th>
        <th>B64 (0.306 MB)</th>
        <th>Binary (0.229)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>REST</td>
        <td>0.884</td>
        <td>0.628</td>
      </tr>
      <tr>
        <td>gRPC</td>
        <td>X</td>
        <td>0.645</td>
      </tr>
    </tbody>
  </table>

  <p>Table 4. Results for small images: 360x640</p>

</div>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th> </th>
        <th>B64 (0.306 MB)</th>
        <th>Binary (0.229)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>REST</td>
        <td>1.262</td>
        <td>0.946</td>
      </tr>
      <tr>
        <td>gRPC</td>
        <td>X</td>
        <td>0.927</td>
      </tr>
    </tbody>
  </table>

  <p>Table 5. Results for medium images: 720x1280</p>

</div>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th> </th>
        <th>B64 (0.306 MB)</th>
        <th>Binary (0.229)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>REST</td>
        <td>2.188</td>
        <td>1.384</td>
      </tr>
      <tr>
        <td>gRPC</td>
        <td>X</td>
        <td>1.422</td>
      </tr>
    </tbody>
  </table>

  <p>Table 6. Results for large images: 1080x1920</p>

</div>

<p>Note there are not results for B64 gRPC since this is not allowed by Torchserve schema definition.</p>

<p>Translating the insights gained from benchmarking with the base servers can be challenging. The tables indicate that Base64 encoding should be avoided and that there are no significant performance differences between using gRPC and REST.</p>

<p>Two factors contribute to the similar performance results for gRPC and REST. Firstly, the model’s inference time is considerably longer than the networking time, making it difficult to discern the small gains obtained by changing the transmission protocol. For example, sending 20 large images concurrently in the simple base case (Table 3) took roughly 0.19s, whereas we are now spending approximately 1.4 seconds (Table 6), highlighting the significant impact of model inference time on the comparison.</p>

<p>Secondly, the Torchserve implementation plays a role in these results. It has been observed that Torchserve’s <code class="language-plaintext highlighter-rouge">.proto</code> definition for <a href="https://github.com/pytorch/serve/blob/master/frontend/server/src/main/resources/proto/inference.proto#L20-L23" target="_blank" rel="noopener noreferrer">prediction response</a> is too generic and it cannot be personalized with your model specifics.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">message</span> <span class="n">PredictionResponse</span> <span class="p">{</span>
    <span class="o">//</span> <span class="n">Response</span> <span class="n">content</span> <span class="k">for</span> <span class="n">prediction</span>
    <span class="nb">bytes</span> <span class="n">prediction</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>This means that your response will be converted to a chunk of bytes so you would not be getting any advantage from Protobuf serialization (similar to what happens with images). For example if our model returns three lists of bounding boxes, class and scores, the <code class="language-plaintext highlighter-rouge">.proto</code> file for our response could be something like:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">message</span> <span class="n">PredictionResponse</span> <span class="p">{</span>
    <span class="n">repeated</span> <span class="nb">float</span> <span class="n">scores</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
    <span class="n">repeated</span> <span class="n">int32</span> <span class="n">scores</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
    <span class="n">repeated</span> <span class="n">repeated</span> <span class="n">int32</span> <span class="n">bboxes</span> <span class="o">=</span> <span class="mi">3</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The differences between this response and the one provided by Torchserve are clear. You do not get any of the Protobuf advantage since the Torchserve schema definition is too general. A better or more customizable definition such as the one provided by Tfserving, of the <code class="language-plaintext highlighter-rouge">.proto</code> file could help boost performance.</p>

<h2 id="conclusion">Conclusion</h2>

<p>If you’re using Torchserve to serve computer vision models, it’s recommended to steer clear of gRPC. Our findings show that there are no performance benefits to using gRPC. Moreover, it adds code complexity while hindering debugging due to its non-human-readable messages. Since REST is more commonly used, most developers are already familiar with it. Switching to gRPC in this scenario comes with a learning curve that doesn’t offer any significant advantages.</p>

<p><em>Any ideas for future posts or is there something you would like to comment? Please feel free to reach out via <a href="https://twitter.com/mmeendez8" target="_blank" rel="noopener noreferrer">Twitter</a> or <a href="https://github.com/mmeendez8" target="_blank" rel="noopener noreferrer">Github</a></em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[This post compares the performance of Torchserve's REST and gRPC communication protocols for transmitting images to a computer vision deep learning model. We conducted benchmarks for structured data, Base64 encoded images, and binary image transmission. The goal is to help practitioners make informed decisions when choosing the right communication protocol for their specific use case, taking into account factors such as ease of implementation and familiarity with the technology.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://miguel-mendez-ai.com/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/thumbnail.jpg" /><media:content medium="image" url="https://miguel-mendez-ai.com/assets/images/fullsize/posts/2023-02-24-torchserve-grpc/thumbnail.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">CNNs &amp;amp; Transformers Explainability: What do they see?</title><link href="https://miguel-mendez-ai.com/2021/12/09/cnn-vs-transformers.html" rel="alternate" type="text/html" title="CNNs &amp;amp; Transformers Explainability: What do they see?" /><published>2021-12-09T00:00:00+00:00</published><updated>2021-12-09T00:00:00+00:00</updated><id>https://miguel-mendez-ai.com/2021/12/09/cnn-vs-transformers</id><content type="html" xml:base="https://miguel-mendez-ai.com/2021/12/09/cnn-vs-transformers.html"><![CDATA[<p><em>I created a small demo in <a href="https://huggingface.co/spaces/mmeendez/cnn_transformer_explainability" target="_blank" rel="noopener noreferrer">Hugging Face Spaces</a> to play with the code</em></p>

<p>A couple of weeks ago I saw a very interesting <a href="https://pytorch.org/blog/FX-feature-extraction-torchvision/" target="_blank" rel="noopener noreferrer">post by Alexander Soare and Francisco Massa</a> on Pytorch Blog. The authors explained that the latest version of Torchivision  <a href="https://github.com/pytorch/vision/releases/tag/v0.11.0" target="_blank" rel="noopener noreferrer"><code class="language-plaintext highlighter-rouge">v0.11.0</code></a> included a new utility that allows us to access intermediate transformations of an input during the step-forward of a PyTorch module. That is, we don’t need more complex code to get the intermediate activations of a model, we can simply point to a specific layer and get its results. The article analyzes the different methods that were used to carry out this task, comparing their advantages and disadvantages. This is a remarkably clear post (as generally on the Pytorch blog) that not only explains you how this new feature works, but also provides insight into the other common methods.</p>

<p>So … I couldn’t resist, I really wanted to try this and see how it works! I’ve been thinking about the differences between Transformer and CNN when classifying images and was wondering if I could compare them. So I rechecked the Class Activation Map paper<a href="https://arxiv.org/pdf/1512.04150.pdf" target="_blank" rel="noopener noreferrer">[1]</a> from 2015. This is a classic job that shows how to paint activation maps from your last conv layer, conditioned on your model output label. For the case of transformers, I based my experiments on Attention Flow <a href="https://arxiv.org/pdf/2005.00928.pdf" target="_blank" rel="noopener noreferrer">[2]</a> which seems to be the standard method in the community.</p>

<p>This post was created with the intention of improving my knowledge on ViT, TorchVision and model’s explainability. I do not pretend to compare ResNet against ViT since they have been trained with different datasets. ViT was pre-trained on ImageNet-21k and finetuned on ImageNet whileas ResNet50 was only trained on ImageNet.</p>

<p>Now, let’s see how to implement both methods and visualize some results!</p>

<h2 id="class-activation-map---resnet">Class Activation Map - ResNet</h2>

<h3 id="a-small-review">A small review</h3>

<p>In <a href="https://arxiv.org/pdf/1512.04150.pdf" target="_blank" rel="noopener noreferrer">[1]</a> authors propose a way to relate last layer activations to the input image. Conv layers apply a set of filters to the input data and they return the stacked filter responses. In this paper authors show how each of this stacked responses contribute to decide the output label. The trick is very simple, they propose to add a Global Average Pooling (GAP) layer over each of the 2D features outputted from the last convolutional layer. Thanks to this, we can figure out how much is each filter contributing to the final classification of the image. As usually an image is worth a thousand words, so have a look at the figure below extracted from the paper:</p>

<p><a href="/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/gap.jpg">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/gap-800-8518703f0.jpg" alt="CNNs &amp; Transformers Explainability: What do they see?" srcset="/generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/gap-400-c0f0bfcc7.webp 400w, /generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/gap-800-c0f0bfcc7.webp 800w, /generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/gap-1002-c0f0bfcc7.webp 1002w" sizes="(max-width: 767px) 100vw, 80vw" width="1002" height="476" />
</a></p>

<p>See how the GAP layer reduces each of the filter outputs to a single averaged element. Then, we will have a vector of size <code class="language-plaintext highlighter-rouge">n_filters</code> that will be multiplied by a linear layer which weights will be a matrix of size <code class="language-plaintext highlighter-rouge">n_filters</code> x <code class="language-plaintext highlighter-rouge">n_classes</code>. Once you know the classification output, you can “isolate” the weight vector related with that class and multiply it by the activations. In math notation this would be expressed by:</p>

\[M_c(x,y) = \sum_{k}w_k^c f_k(x,y)\]

<p>where \(k\) represents the number of filters in the last conv layer, \(w_k^c\)  are the linear layer weights and \(f_k(x,y)\) represents the 2D stacked filter responses.</p>

<p>This paper was publised in 2015 and at that time popular architectures did not have GAP layers so they have to be finetuned with these extra layers… But we are going to use a ResNet architecture which already has a GAP layer at the end! You can check <a href="https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L203" target="_blank" rel="noopener noreferrer">here</a> torchvision implementation of ResNets to be sure of this.</p>

<p>There have been multiple works that have evolved CAM idea, you can check a few implementions of them in <a href="https://github.com/frgfm/torch-cam" target="_blank" rel="noopener noreferrer">torch-cam</a> repo.</p>

<h3 id="code">Code</h3>

<p>First of all we need to get the pretrained ResNet50 model from torchvision and put it in eval model. Then we can get extract the features we need by specifying their names. We can check all the names of the layers with <code class="language-plaintext highlighter-rouge">get_graph_node_names</code> function. In this case I need to extract last conv layer activation, this is <code class="language-plaintext highlighter-rouge">layer4</code>. One of the advantages of using the new feature extractor is that it would automatically mark the <code class="language-plaintext highlighter-rouge">layer4</code> as a leaf of the computation graph, so following layers would not be computed (and that’s awesome!). Unfortunately, we also need to get the classification output of the network so we are not really getting the full power of the feature_extractor. Let’s code this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">models</span>
<span class="kn">from</span> <span class="n">torchvision.models.feature_extraction</span> <span class="kn">import</span> <span class="n">create_feature_extractor</span>

<span class="n">resnet50</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="nf">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">)</span>
<span class="n">resnet50</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

<span class="n">feature_extractor</span> <span class="o">=</span> <span class="nf">create_feature_extractor</span><span class="p">(</span><span class="n">resnet50</span><span class="p">,</span> <span class="n">return_nodes</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">layer4</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">fc</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div>

<p>For computing the CAM we just need to apply the previous formula. First we need to get linear layer weight matrix, select the row that relates with the predicted output class and multiply it by the extracted features, then we can apply min-max normalization so that the CAM is between 0 and 1.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fc_layer_weights</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">.</span><span class="n">fc</span><span class="p">.</span><span class="n">weight</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="c1"># Extract features and remove batch dim
</span>    <span class="n">output</span> <span class="o">=</span> <span class="nf">feature_extractor</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">)</span>
    <span class="n">cnn_features</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="sh">"</span><span class="s">layer4</span><span class="sh">"</span><span class="p">].</span><span class="nf">squeeze</span><span class="p">()</span>
    <span class="n">class_id</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="sh">"</span><span class="s">fc</span><span class="sh">"</span><span class="p">].</span><span class="nf">argmax</span><span class="p">()</span>

    <span class="c1"># Linear combination of class weights and cnn features
</span>    <span class="n">cam</span> <span class="o">=</span> <span class="n">fc_layer_weights</span><span class="p">[</span><span class="n">class_id</span><span class="p">].</span><span class="nf">matmul</span><span class="p">(</span><span class="n">cnn_features</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># Reshape back to 2D
</span><span class="n">cam</span> <span class="o">=</span> <span class="n">cam</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">cnn_features</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">cnn_features</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</code></pre></div></div>

<p>That’s all! Just a few lines, let’s see a few simple examples:</p>

<div class="post-center-image">
<a href="/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/CAMR.jpg">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/CAMR-800-333ec12b6.jpg" alt="CAM" srcset="/generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/CAMR-400-d5f6fcf29.webp 400w, /generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/CAMR-800-d5f6fcf29.webp 800w, /generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/CAMR-1100-d5f6fcf29.webp 1100w" sizes="(max-width: 767px) 100vw, 80vw" width="1100" height="200" />
</a>

</div>

<h2 id="vit-attention-map">ViT Attention Map</h2>

<h3 id="another-brief-review">Another brief review</h3>

<p>ViT paper<a href="https://arxiv.org/pdf/2010.11929.pdf" target="_blank" rel="noopener noreferrer">[3]</a> was publised at the end of 2020 and it has already become a reference in the field. There are an incredible large number of works<a href="https://arxiv.org/abs/2101.01169" target="_blank" rel="noopener noreferrer">[4]</a> that have used it as a baseline to build new methods upon its ideas. The authors found a simple way to treat images as sequences so they can feed them to a Transformer encoder, simply divide them into fixed-size patches.</p>

<div class="post-center-image">
<a href="/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/vit.jpg">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/vit-800-da40c3cdb.jpg" alt="Vision Transformer" srcset="/generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/vit-400-23824f5be.webp 400w, /generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/vit-800-23824f5be.webp 800w, /generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/vit-879-23824f5be.webp 879w" sizes="(max-width: 767px) 100vw, 80vw" width="879" height="470" />
</a>

</div>

<p>The attentions mechanism allows us to figure out what parts or patches of the image are key for the classification result. This will allow us to interpret model’s decision.</p>

\[Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\]

<p>In the attention formula, the dot product between the query and the key represents the raw attention scores. I like to imagine this as a similarity matrix, where each position represents how “similar” the query and key embeddings are. So when both vectors are not aligned the dot product will tend to zero.</p>

<p>At the very first attention layer, the input vectors are the linear projections of the flattened patches:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pseudocode simplification from HF implementation
</span><span class="n">patch_embeddings</span> <span class="o">=</span> <span class="nc">PatchEmbeddings</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">patch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">flat_patch_embeddings</span> <span class="o">=</span> <span class="nc">Flatten</span><span class="p">(</span><span class="n">patch_embeddings</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">linear_projections</span> <span class="o">=</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">patch_embeddings</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">embedding_size</span><span class="p">)</span>
</code></pre></div></div>

<p>So it would be very easy to visualize attention weights at this very first layer because they directly relate to the image embeddings. This task becomes harder when we stack multiple Transformer layers (there are 12 layers in ViT). In <a href="https://arxiv.org/abs/2005.00928" target="_blank" rel="noopener noreferrer">[2]</a> two different methods are proposed with the aim of easing this task, Attention Rollout and Attention Flow. We are going to use the first of them because of its simplicity.</p>

<h3 id="attention-rollout">Attention Rollout</h3>

<p>We can model the information flow as a graph where input patches and hidden embeddings are the nodes and the edges represent the attentions from the nodes in one layer to the next layer. These edges are weighted by the attention weights which determine the amount of information that is passed from one layer to the next. Hence, if we want to compute the attention that a node at layer \(i\) receives from all previous layer nodes, we can simply multiply the attention weights matrices from the input layer until our target \(i\). Check the following animation to see how this works:</p>

<div class="post-center-image">
<a href="/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/attention_rollout.jpg">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/attention_rollout-774-2f6a36848.jpg" alt="Attention Rollout" srcset="/generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/attention_rollout-400-d83342cea.webp 400w, /generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/attention_rollout-774-d83342cea.webp 774w" sizes="(max-width: 767px) 100vw, 80vw" width="774" height="658" />
</a>

</div>

<p class="image-caption"><em>Attention rollout simulation obtained from <a href="https://samiraabnar.github.io/articles/2020-04/attention_flow" target="_blank" rel="noopener noreferrer">Samira Abnar’s blog</a></em></p>

<p>This is super straight-forward and easy to understand but we are missing the influence of residual connections. Paper authors handle this in a very elegant way, they realize that the output at layer \(V_{l+1}\) depends on the previous output and the attention weights: \(V_{l+1} = V_{l} + W_{att}V_l\), where \(W_{att}\) is the attention matrix. This can also be expressed as \(V_{l+1} = (W_{att} + I)V_l\). Thus, re-normalizing the weights, the raw attention updated by residual connections can be expressed as: \(A = 0.5W_{att} + 0.5I\).</p>

<p>Note I have seen other implementations of this method that instead of averaging the attention between the different heads of each layer, use min or max operator since it seems to work better in practice (see <a href="https://github.com/jacobgil/vit-explain" target="_blank" rel="noopener noreferrer">this implementation</a>)</p>

<h3 id="code-1">Code</h3>

<p>First of all we need to setup our ViT model, unfortunately at the moment of writing this post we cannot use Torchvision’s ViT because it is not included in latest version <code class="language-plaintext highlighter-rouge">0.11.1</code> (it has been recently added see <a href="https://github.com/pytorch/vision/pull/4594" target="_blank" rel="noopener noreferrer">this PR</a>). For this reason, we cannot use the new feature extractor and we need to find another implementation. I will use Hugging Face library because it is simple and allows me get all attention matrices directly.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">ViTForImageClassification</span>
<span class="n">vit</span> <span class="o">=</span> <span class="n">ViTForImageClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">google/vit-base-patch16-224</span><span class="sh">"</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">)</span>
<span class="n">vit</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
</code></pre></div></div>

<p>You can check the <a href="https://huggingface.co/docs/transformers/model_doc/vit#transformers.ViTModel" target="_blank" rel="noopener noreferrer">official documentation</a> to see how we can use <code class="language-plaintext highlighter-rouge">output_attentions</code> parameter to get the attentions tensors of all attention layers. Attention rollout code would consist on:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Inference
</span><span class="n">result</span> <span class="o">=</span> <span class="nf">vit</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># Stack all layers attention
</span><span class="n">attention_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">1</span><span class="p">]).</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Average the attention at each layer over all heads
</span><span class="n">attention_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Add residual and re-normalize
</span><span class="n">residual</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)).</span><span class="nf">to</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">)</span>
<span class="n">attention_probs</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">attention_probs</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">residual</span>

<span class="c1"># Normalize by layer
</span><span class="n">attention_probs</span> <span class="o">=</span> <span class="n">attention_probs</span> <span class="o">/</span> <span class="n">attention_probs</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Compute rollout
</span><span class="n">attention_rollout</span> <span class="o">=</span> <span class="n">attention_probs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">attention_probs</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
    <span class="n">attention_rollout</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">attention_rollout</span><span class="p">)</span>

<span class="c1"># Attentions between CLS token and patches
</span><span class="n">mask</span> <span class="o">=</span> <span class="n">attention_rollout</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span>
<span class="c1"># Reshape back to 2D
</span><span class="n">mask_size</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">mask</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">mask_size</span><span class="p">,</span> <span class="n">mask_size</span><span class="p">)</span>
</code></pre></div></div>

<p>Pretty simple, let’s see a few examples:</p>

<div class="post-center-image">
<a href="/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/ROLLOUTR.jpg">
  <img loading="lazy" src="/generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/ROLLOUTR-800-62565fb71.jpg" alt="Rollout Transform" srcset="/generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/ROLLOUTR-400-c56709655.webp 400w, /generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/ROLLOUTR-800-c56709655.webp 800w, /generated/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/ROLLOUTR-1100-c56709655.webp 1100w" sizes="(max-width: 767px) 100vw, 80vw" width="1100" height="200" />
</a>

</div>

<p>There seems to be a larger noise when we comparing these results wrt CAM ones. One plausible option to reduce this effect is to filter very low attentions and keep only the strongest ones. I will stick with the original implementation but you find about this in <a href="https://github.com/jacobgil/vit-explain" target="_blank" rel="noopener noreferrer">this repo</a>.</p>

<h2 id="conclusion">Conclusion</h2>

<p>We have covered two important methods that can give us some intuition on how CNNs and Transformers work internally. A few key aspects that we must keep in mind:</p>

<ul>
  <li>
    <p>The idea behind this post was to improve my understanding of the ViT architecture, TorchVision new features, GAP and Attention Rollout. This should not be used as a comparison between ResNet and ViT, since ViT was pre-trained on ImageNet-21k and finetuned on ImageNet whileas ResNet50 was only trained on ImageNet.</p>
  </li>
  <li>
    <p>CAM does not generalize to models without global average pooling. You would need to retrain your model with a GAP layer or use a different method. <a href="https://github.com/frgfm/torch-cam" target="_blank" rel="noopener noreferrer">Here</a> you can check some different implementations.</p>
  </li>
  <li>
    <p>I have used Hugging Face’s ViT implementation since it is not yet available on latest Torchvision version.</p>
  </li>
  <li>
    <p>Do not forget to check the <a href="https://huggingface.co/spaces/mmeendez/cnn_transformer_explainability" target="_blank" rel="noopener noreferrer">Hugging Face Space</a> I created for this post!</p>
  </li>
</ul>

<h2 id="references">References</h2>

<ul>
  <li>[1] Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., &amp; Torralba, A. (2016). Learning deep features for discriminative localization. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2921-2929).</li>
  <li>[2] Abnar, S., &amp; Zuidema, W. (2020). Quantifying attention flow in transformers. arXiv preprint arXiv:2005.00928.</li>
  <li>[3] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., … &amp; Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.</li>
  <li>[4] Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., &amp; Shah, M. (2021). Transformers in vision: A survey. arXiv preprint arXiv:2101.01169.</li>
</ul>

<p><em>Any ideas for future posts or is there something you would like to comment? Please feel free to reach out via <a href="https://twitter.com/mmeendez8" target="_blank" rel="noopener noreferrer">Twitter</a> or <a href="https://github.com/mmeendez8" target="_blank" rel="noopener noreferrer">Github</a></em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[A Hugging Face Space to compare ResNet Class Activation Map to Vit Attention Rollout]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://miguel-mendez-ai.com/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/thumbnail.jpg" /><media:content medium="image" url="https://miguel-mendez-ai.com/assets/images/fullsize/posts/2021-12-09-cnn-vs-transformers/thumbnail.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>