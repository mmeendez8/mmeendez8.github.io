---
layout: post
title: "HPA explained"
subtitle: ""
description: ""
image: "/assets/images/fullsize/posts/2024-04-15-video-segmentation/thumbnail.jpg"
selected: y
mathjax: n
---


1. intro: what happened that day, mention the tool

2. tool: just introduce it, it is very simple, formula and just to play

3. hpa intro

4. show grafana, show requests and limits, mention 3 pods running but both metrics below target??

5. explain what hap


A few weeks ago, while reviewing service metrics in Grafana, I noticed unexpected behavior in one of our servicesâ€”there were more pods than necessary given the current traffic load. This led me to uncover that the extra pods were spawned by the Horizontal Pod Autoscaler (HPA) based on the metrics we had configured (a while ago). Understanding HPA took me a few hours.This is a task typically handled by specialized teams in larger companies, but working at a startup forces you to wear many hats and I often find myself analyzing how models perform in production. In this post, I'll discuss the issues I encountered with HPA and demonstrate how a simple visualization tool can help anticipate the number of replicas needed.

## Visualization tool

Before we explore how HPA works, I want to showcase the visualization tool. It simplifies understanding the HPA formula used to calculate the desired number of replicas:

```
desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )]
```

You can interact with this tool to see how changes in metrics affect the number of replicas.

{% include hpa.html %}


## What is HPA?

The [Horizontal Pod Autoscaler (HPA)](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) in Kubernetes automatically adjusts the number of pod replicas in a deployment, replicaset, or statefulset based on observed CPU utilization or other select metrics. This feature is very useful for managing application scalability and resource efficiency, particularly in environments with variable workloads.

For example. Statsbomb can use HPA to handle increased traffic during a weekend when there are more games being played. The HPA can automatically scale up the number of web server pods to maintain performance, and scale down during off-peak hours to reduce costs. This dynamic adjustment helps ensure that the application consistently meets performance targets without manual intervention.

In next sections I will briefly explain how HPA works and how to use this simple tool to ease your scaling decisions.

## How does HPA work?

First of all we need to make sure we understand the concepts of `request` and `limits` since they are fundamental to how resources are allocated and managed across the pods in a cluster.

- **Requests**: This value specifies the amount of CPU or memory that Kubernetes guarantees to a pod. When a pod is scheduled, the Kubernetes scheduler uses this request value to decide on which node the pod can fit. So this number ensures the pod has the resources it needs to run.
- **Limits**: This value specifies the maximum amount of CPU or memory that a pod can use. If a pod exceeds this limit, Kubernetes will throttle the pod or kill it. This is how k8s ensures that a single pod does not consume all the resources in a node.

Imagine our deployment has the following setup:

```yaml
# deployment.yaml
resources:
  limits:
    memory: 2000Mi
    cpu: 1500m
  requests:
    memory: 1350Mi
    cpu: 400m
    
```

And our HPA is configured in the following manner:

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: autoscaler-name
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: autoscaler-name
  minReplicas: 1
  maxReplicas: 3
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 90
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 90
```

So what does this mean? Well if you are a proper engineer what you would do is check the official docs and try to carefully understand this. But if you are like me... you probably would make some assumptions and hope for the best (for finally ending up reading the docs ðŸ˜…). 
My initial assumption was that this would configure HPA to basically detect when memory or cpu usage was above 90% and then scale up the number of pods. But you also need to include current number of replicas in that computation. The real way HPA works is the following:

```
desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )]
```

Where:
- `currentReplicas` is the number of replicas the deployment is currently running.
- `currentMetricValue` is the current value of the metric we are monitoring (e.g. CPU usage).
- `desiredMetricValue` is the target value for the metric we are monitoring (e.g. 90% requested CPU usage).

So let's see what I observed in Grafana that day.

<div class="post-center-image">
    {% picture pimage /assets/images/fullsize/posts/2024-04-23-hpa/memory_cpu.png --alt Grafana plot showing memory and cpu usages %}
</div>

{:refdef: class="image-caption"}
*Figure 1. Grafana plot showing memory and cpu usages*
{: refdef}

What we can see is that at 13:10 we had 3 pods running with memory usage around 1100Mb and CPU usage was less than 100m. Both seem to be below the 90% of target values (1350Mb and 500m respectively). So what was going on?


## Debugging HPA

Let's start by visualing the expected number of replicas using our tool for both metrics given the previous mentioned values:



<div class="post-center-image">
    {% picture pimage /assets/images/fullsize/posts/2024-04-23-hpa/memory_tool.png --alt HPA memory expected replicas using visualization tool %}
</div>

{:refdef: class="image-caption"}
*Figure 2. HPA memory expected replicas using visualization tool*
{: refdef}

That's awesome, the pod is below the target value so HPA is not going to scale up the number of replicas. We can clearly see in the previous plot that if memory usage goes above 1350Mb the number of replicas would be set to 2.

## The problem 

Things look pretty good so far. But what happened to me that day was that I noticed that although the memory ans CPU usage seem to be below the target values, the number of replicas was set to 3. This was very confusing to me since I was expecting only 1 pod to be running. 

So what happened to me here was the following:

### 1. CPU usage of my application had a large spike at start up (Figure 1)

This was the first thing I noticed. The CPU usage was way above the target value of 500. When this happens, the 'currentMetricValue / desiredMetricValue' becomes larger than 1 and the autoscaler starts scaling up the number of replicas until it reaches maxReplicas.

use plot with x axis current replicas


### 2. HPA decides to scale replicas up to 3 to match this usage (Figure 2)

### 3. CPU usage decreases and goes back to small values (Figure 3)
### 4. HPA does not scale things down because the current memory usage for 3 replicas is above the target value (Figure 4)





The CPU usage at 13:10 was clearly below '200m'. That looks like a very low value and if we use the visualization tool we easily see that the number of replicas would be set to 1:

<div class="post-center-image">
    {% picture pimage /assets/images/fullsize/posts/2024-04-23-hpa/cpu_1310.png --alt CPU expected replicas using visualization tool %}
</div>

{:refdef: class="image-caption"}
*Figure 3. HPA CPU expected replicas using visualization tool*
{: refdef}

The bottleneck has to be then related with the memory usage. HPA uses the highest value between CPU and memory to calculate the desired number of replicas. Since the number of replicas at 13:10 was 3, the visualization tool show us that the expected replicas would be 3:

<div class="post-center-image">
    {% picture pimage /assets/images/fullsize/posts/2024-04-23-hpa/memory_1310.png --alt Memory expected replicas using visualization tool %}
</div>

{:refdef: class="image-caption"}
*Figure 4. HPA memory expected replicas using visualization tool*
{: refdef}

So what happened here is the following:
1. CPU usage of my application had a large spike at start up (Figure 1)
2. HPA decides to scale replicas to 3 to match this usage (Figure 2)
3. CPU usage decreases and goes back to small values
4. HPA does not scale things down because the current memory usage for 3 replicas is above the target value

