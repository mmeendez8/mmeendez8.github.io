<!DOCTYPE html>
<html lang="en" id="top">

<head>

  <!-- Basic Page Needs
  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“ -->

  <meta charset="utf-8">
  <title>Miguel MÃ©ndez | DVC + MMdetection</title>
  <meta name="author" content="Miguel Mendez">
  <meta property="og:website" content="Miguel Mendez personal website">
  <meta name="robots" content="max-image-preview:large">
  
  
  <meta name="description" property="og:description" content="Training a pytorch object detection model with mmdetection combined with DVC (Data Version Control) allows you to version your code, checkpoints and experiments. Learn how to do it and maximize the reproducibility of your experiments!">
  

  
  <meta name="image" property="og:image" content="https://miguel-mendez-ai.com/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/thumbnail.jpg">
  

  <!-- Mobile Specific Metas
  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“ -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- CSS
  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“ -->

  <!-- preload fonts -->
  <link rel="preload" href="/libs/external/fonts/Graphik-Regular.woff2"" as="font" type="font/woff2" crossorigin>
  <link rel="preload" href="/libs/external/fonts/Graphik-Semibold.woff2"" as="font" type="font/woff2" crossorigin>
  <link rel="preload" href="/libs/external/fonts/Tiempos-Headline-Semibold.woff2"" as="font" type="font/woff2" crossorigin>

  <link rel="stylesheet" href=/libs/custom/my_css.css>
  <link rel="stylesheet" href=/libs/external/fonts/fonts.css>

  <!-- hack for non critical css https://web.dev/defer-non-critical-css/ -->
  <link rel="preload" href=/libs/custom/syntax.css as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript><link rel="stylesheet" href=/libs/custom/syntax.css></noscript>
  
  <!-- non critical lighthouse css -->
  <link rel="preload" href=/libs/external/lightbox/lightbox.css as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript><link rel="stylesheet" href=/libs/external/lightbox/lightbox.css></noscript>

  <!-- Fontello
  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“ -->
  <link rel="stylesheet"
    href=/libs/external/fontello-bb2d1770/css/fontello.css>


  <!-- Hihglight.js
  <!-- Preload CSS -->
  <link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css"></noscript>
  
  <!-- Load only necessary languages (example: javascript and python) -->
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/javascript.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/yaml.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/dockerfile.min.js"></script>



  <!-- Favicon
  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“ -->
  <link rel="icon" type="image/png" href=/libs/icon.png>
  <link rel="shortcut icon" type="image/png" href=/libs/icon.png>

  <!-- Google Analytics -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-LMHYVFNF1J"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-LMHYVFNF1J');
</script>
    <!-- Twitter cards -->
  <meta name="twitter:site" content="@https://twitter.com/mmeendez8">
  <meta name="twitter:title" content="DVC + MMdetection">
  
  
  <meta name="twitter:description" content="Training a pytorch object detection model with mmdetection combined with DVC (Data Version Control) allows you to version your code, checkpoints and experiments. Learn how to do it and maximize the reproducibility of your experiments!">
  
  

  
  <meta name="twitter:card"  content="summary_large_image">
  <meta name="twitter:image" content="https://miguel-mendez-ai.com/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/thumbnail.jpg">
  

  <!-- end of Twitter cards -->
  

</head>

<body>

  <header class="the-post-header">
    <div class="container">
      <a href="/">
        <h3>Miguel MÃ©ndez</h3>
      </a>
      <div>
        <a  href=/index.html#posts>
         <h3 class="posts-link">Posts</h3>
        </a>
      </div>
    </div>
</header>

<div class="the-post-title-placeholder">
  <div class="offset">
    <div class="the-post-title-text">
      <span class="the-post-date">August 30, 2021 </span>
      <h1 class="the-post-title">DVC + MMdetection</h1>
      <p>A guide to train, monitor, compare and evaluate your pytorch object detection models</p>
    </div>
  </div>

  <div class="the-post-title-image">
    <img src="/generated/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/thumbnail-800-1f776a3e5.jpg" alt="DVC + MMdetection" srcset="/generated/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/thumbnail-400-b7ca4b2d0.webp 400w, /generated/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/thumbnail-800-b7ca4b2d0.webp 800w, /generated/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/thumbnail-1500-b7ca4b2d0.webp 1500w, /generated/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/thumbnail-1600-b7ca4b2d0.webp 1600w" sizes="(max-width: 767px) 100vw, 50vw" width="1600" height="836">

  </div>
</div>

<div class="share-button">
  <a href="https://twitter.com/share?ref_src=twsrc%5Etfw" class="twitter-share-button" data-show-count="false" ,
    data-size="large" , data-text="DVC + MMdetection" , data-via="mmeendez8" , data-url="https://miguel-mendez-ai.com/2021/08/30/mmdet-dvc-tutorial.html">
  </a>
</div>

<div class="container the-post-content">

  <p>I recently <a href="/2021/07/01/dvc-tutorial.html" target="_blank" rel="noopener noreferrer">published a post</a> where I showed how to use DVC to maintain versions of our datasets so we reduce data reproducibility problems to a minimum. This is the second part of the tutorial where we are going to see how we can combine the power of mmdetection framework and its huge <a href="https://github.com/open-mmlab/mmdetection/blob/master/docs/model_zoo.md" target="_blank" rel="noopener noreferrer">model zoo</a> with DVC for designing ML pipelines, versioning our models and monitor training progress.</p>

<p>It is quite a lot of content to cover, so I will be going through it step by step and trying to keep things as simple as possible. You can find all the code for this tutorial in my <a href="https://github.com/mmeendez8/mmdetection_dvc" target="_blank" rel="noopener noreferrer">Github</a>. So letâ€™s start with it!</p>

<h2 id="1-setup-the-environment">1. Setup the environment</h2>

<p>We are gonna need a few packages to get our project up and running. I have created a <code>conda.yml</code> that you can find in the root of the repository, this is going to install pytorch and cudatoolkit since we are going to train our models using a GPU. You can create the environment by:</p>

<pre><code class="language-bash">conda env create -f conda.yaml
conda activate mmdetection_dvc
</code></pre>

<h2 id="2-import-our-dataset">2. Import our dataset</h2>

<p>In the previous post we used a subset of the COCO dataset created by fast.ai. We push all data to a Google Drive remote storage using DVC and keep all metada files in a Github repository. We need now to import this dataset in our repo and thatâ€™s exactly what <a href="https://dvc.org/doc/command-reference/import" target="_blank" rel="noopener noreferrer">dvc import</a> can do for us!</p>

<pre><code class="language-bash">dvc init
dvc import "git@github.com:mmeendez8/coco_sample.git" "data/" -o "data/"
</code></pre>

<p>Note that we are importing <code>/data</code> folder of the remote repository into the <code>data/</code> directory of our project where all our data will be stored. This may take a while since we need to download all images and annotations from the remote gdrive storage. Letâ€™s now publish the changes on git:</p>

<pre><code class="language-bash">git add . 
git commit -m "Import coco_sample dataset" 
git push
</code></pre>

<p>Once it is downloaded, we can move between different <a href="https://github.com/mmeendez8/coco_sample/releases" target="_blank" rel="noopener noreferrer">versions of the dataset</a> with the <a href="https://dvc.org/doc/command-reference/update" target="_blank" rel="noopener noreferrer">dvc update</a> command. If we would go back to v1.0 of our dataset and push our changes to git:</p>

<pre><code class="language-bash">dvc update --rev v1.0 data.dvc
git add data.dvc
git commit -m "Get version 1.0 of coco_sample"
git push
</code></pre>

<p>Thatâ€™s it! We have imported our dataset and we know how to move between different versions so letâ€™s create a script that will train our model!</p>

<h2 id="3-train-our-model">3. Train our model</h2>

<p><a href="https://github.com/open-mmlab/mmdetection" target="_blank" rel="noopener noreferrer">MMDetection</a> is an open source object detection toolbox based on PyTorch. It is a part of the OpenMMLab project and it is one of the most popular computer vision frameworks. I love it and I am an active contributor since it became my default framework for object detection last year.</p>

<p>They have an extense documentation which really helps first time users. In this post I will skip the very basics and focus on showing how easily can we train a RetinaNet object detector on our coco_sample dataset.</p>

<h3 id="31-model-config">3.1. Model config</h3>

<p>First thing we need to do is to find the config file for our model, so letâ€™s explore mmdet model zoo and more specifically <a href="https://github.com/open-mmlab/mmdetection/tree/master/configs/retinanet" target="_blank" rel="noopener noreferrer">RetinaNet section</a>. Thereâ€™s a bunch of different RetinaNet models there but letâ€™s stick with the base config from the <a href="https://arxiv.org/pdf/1708.02002.pdf" target="_blank" rel="noopener noreferrer">original paper</a>. I have already downloaded this file to my repo and you can find it under <code>configs/retinanet_r50_fpn.py</code>. There are three main sections there:</p>

<ul>
  <li>The backbone definition, which in our case is a ResNet50. Its weights come from some torchvision checkpoint specified at:</li>
</ul>

<pre><code class="language-python">init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'))`
</code></pre>

<p>As a curious fact, I checked out <a href="https://pytorch.org/vision/stable/models.html" target="_blank" rel="noopener noreferrer">official torchvision documentation</a> and it seems this network has been trained with some dataset that is currently lost so there is no chance to reproduce this resultsâ€¦</p>

<center>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Just found this checking <a href="https://twitter.com/hashtag/torchvision?src=hash&amp;ref_src=twsrc%5Etfw">#torchvision</a> stable models, it seems they were trained on some volatile dataset ğŸ˜…<br />cc <a href="https://twitter.com/DVCorg?ref_src=twsrc%5Etfw">@DVCorg</a> <a href="https://t.co/rR8ANSmucI">pic.twitter.com/rR8ANSmucI</a></p>&mdash; Miguel Mendez (@mmeendez8) <a href="https://twitter.com/mmeendez8/status/1418507102465765376?ref_src=twsrc%5Etfw">July 23, 2021</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>

<ul>
  <li>
    <p>The <a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener noreferrer">Feature Pyramid Network (FPN)</a> configuration which is useful to find features at different scale levels.</p>
  </li>
  <li>
    <p>The classification head and the regression head. They predict labels and bounding boxes regression parameters for each of the anchors of the model. I cannot really go deep how this model works and what anchors are but you should check our repo <a href="https://github.com/Gradiant/pyodi" target="_blank" rel="noopener noreferrer">pyodi</a> if you really want to understand all the details.</p>
  </li>
</ul>

<h3 id="32-dataset-config">3.2 Dataset config</h3>

<p>Mmdetection framework also uses config files for datasets. There we define our train and validation data and which types of transformation do we want to apply before images are feed into the network. Since our dataset follows COCO format, I just modified original <a href="https://github.com/open-mmlab/mmdetection/blob/master/configs/_base_/datasets/coco_detection.py" target="_blank" rel="noopener noreferrer">COCO_detection.py</a>. Note that:</p>

<ul>
  <li>I removed the test set since we are not going to use one for this tutorial.</li>
  <li>I added a <code>CLASSES</code> variable with our reduced set of labels.</li>
</ul>

<p>You can check the dataset config file in <code>configs/coco_sample.py</code></p>

<h3 id="33-train-configuration">3.3 Train configuration</h3>

<p>There are multiple training parameters we can configure using mmdetection. For this simple demo we are going to use the default scheduler (see <code>configs/scheduler.py</code>). It uses SGD and a dynamic learning rate policy and thatâ€™s mostly what we need to know for now.</p>

<p>Our runtime definition is under <code>configs/runtime.py</code> and we are going to specify a few interesting things there:</p>

<ul>
  <li><code>checkpoint_config</code>: specifies the checkpoint saving frequency</li>
  <li><code>log_config</code>: allows us to select a specific logger for our training</li>
  <li><code>custom_hooks</code>: extra hooks that we can insert or create for retrieving or adding functionalities to our training</li>
  <li><code>workflow</code>: it defines training workflow, this is, how many training epochs do we want to run before a validation one.</li>
</ul>

<p>Since we are using DVC, we are also going to use the <a href="https://dvc.org/doc/dvclive" target="_blank" rel="noopener noreferrer">DVCLive</a> hook. DVCLive is an open-source Python library for monitoring the progress of metrics during training of machine learning models. It is a recent and super cool library with git integration and thatâ€™s all I need! See how simple is to add this hook:</p>

<pre><code class="language-python">log_config = dict(
    interval=50,
    hooks=[
        dict(type='TextLoggerHook'),
        dict(type="DvcliveLoggerHook", path="training/metrics"),
    ])
</code></pre>

<h2 id="4-data-pipeline">4. Data pipeline</h2>

<p>Letâ€™s create our first <a href="https://dvc.org/doc/start/data-pipelines" target="_blank" rel="noopener noreferrer">data pipeline</a>! Ideally (and following DVC docs) we should use dvc run commands so the pipeline gets automatically generated butâ€¦ I feel more comfortable creating a dvc.yaml and filling it myself.</p>

<h3 id="41-prepare-the-data">4.1 Prepare the data</h3>

<p>Our COCO annotations are missing a few fields because fast.ai guys considered them unnecessary (they actually are) so they removed all extra fields to reduce the final size of the json. Thatâ€™s fair enough, but mmdetection needs them so I created a very simple script that will prepare the data for us, you can find it in <code>src/prepare_data.py</code>.</p>

<p>The first step of our data pipeline will prepare our annotation file and save the modified COCO files into prepare_data. You can simply add the following to your <code>dvc.yaml</code>:</p>

<pre><code class="language-yaml">stages:
  prepare_data:
    foreach:
      - train
      - val
    do:
      cmd: python src/prepare_data.py
        --coco_file data/coco_sample/annotations/split_${item}.json
        --output_file processed_data/${item}_split_with_ann_id.json
      deps:
        - data/coco_sample/annotations/split_${item}.json
      outs:
        - processed_data/${item}_split_with_ann_id.json
</code></pre>

<p>Thereâ€™s a stage called called prepare_data that run a small for loop over values <code>[train, val]</code> and calls the prepare_data script. See how I have specified the original json files as dependencies and the new one as outputs so DVC knows how to track them.</p>

<p>You can now call run the pipeline with <code>dvc repro</code> and the new annotations file should appear!</p>

<h3 id="42-train-the-model">4.2 Train the model</h3>

<p>I have created a simple training script in <code>src/train.py</code> that adjusts to our needs. You could also use <a href="https://github.com/open-mmlab/mmdetection/blob/master/tools/train.py" target="_blank" rel="noopener noreferrer">mmdetection train tool</a> since I just applied some minor modifications to it that will allow us to use dvc params.</p>

<p>We can add a new step to our data pipeline that executes our training step. For example this would be enough for running an experiment with our actual configs:</p>

<pre><code class="language-yaml">train:
  cmd: python src/train.py
    --dataset configs/coco_sample.py
    --model configs/retinanet_r50_fpn.py
    --schedule configs/schedule_1x.py
    --runtime configs/runtime.py
    --work_dir training/checkpoints
  deps:
    - configs/
    - processed_data
    - src/train.py
  outs:
    - training/checkpoints
  live:
    training/metrics:
      summary: true
      html: true
</code></pre>

<p>Note how I have added the <code>live</code> key to notify DVC that our script will be saving metrics in the <code>training/metrics</code> folder. Also, this will generate a html file that we can use to visualize in real time our train progress. So simple!</p>

<p>We can run again DVC repro as many times as we want changing our config files as needed for trying different hyperparameters or model configurations. Nevertheless, DVC guys recommend yo to use <a href="https://dvc.org/doc/start/experiments" target="_blank" rel="noopener noreferrer">DVC experiments</a> when you are tryining different configurations. So thatâ€™s what we are going to do! Note this is a recent feature and I had to open a couple issues since I found a couple â€œbugsâ€ or unexpected behavior such <a href="https://github.com/iterative/dvc/issues/6465">[1]</a>, <a href="https://github.com/iterative/dvc/issues/5477?notification_referrer_id=MDE4Ok5vdGlmaWNhdGlvblRocmVhZDE1OTA3ODE1MTM6MTU5Njk2Njc%3D#issuecomment-905234950">[2]</a>.</p>

<p>Letâ€™s do our first training by running <code>dvc exp run</code>! You can monitor training progress by opening your <code>training/metrics.html</code> file:</p>

<table>
  <tbody>
    <tr>
      <td><img src="/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/loss.jpg" alt="loss curve" /></td>
      <td><img src="/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/vmap.jpg" alt="vmap curve" target="_blank" rel="noopener noreferrer" /></td>
    </tr>
  </tbody>
</table>

<p>Training will be done soon (depening on your GPU and machine) and we can check our results by running:</p>

<pre><code class="language-console">$ dvc exp show
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”
â”ƒ Experiment              â”ƒ Created  â”ƒ  step â”ƒ learning_rate â”ƒ momentum â”ƒ val.bbox_mAP â”ƒ val.bbox_mAP_50 â”ƒ val.bbox_mAP_75 â”ƒ val.bbox_mAP_s â”ƒ val.bbox_mAP_m â”ƒ val.bbox
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”
â”‚ workspace               â”‚ -        â”‚ 13057 â”‚         1e-05 â”‚      0.9 â”‚        0.198 â”‚           0.368 â”‚           0.194 â”‚          0.008 â”‚          0.142 â”‚         
â”‚ main                    â”‚ 11:24 AM â”‚     - â”‚             - â”‚        - â”‚            - â”‚               - â”‚               - â”‚              - â”‚              - â”‚         
â”‚ â””â”€â”€ cefe59e [exp-acc34] â”‚ 01:20 PM â”‚ 13057 â”‚         1e-05 â”‚      0.9 â”‚        0.198 â”‚           0.368 â”‚           0.194 â”‚          0.008 â”‚          0.142 â”‚         
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€
</code></pre>

<p>The possibilty of tracking your hyperparameters is what I most like about experiments. We can change one hyperparameter for a experiment, DVC will remember this for you and will help you to compare different experiments. This is nice but there is room for improvement since at this moment for running an experiment with a different parameter we need to:</p>

<ol>
  <li>Add our parameter to our <code>params.yaml</code></li>
  <li>Specify that our train step depends on this parameter</li>
  <li>Run experiment with -S flag updating the parameter value.</li>
</ol>

<p>These steps are fine when you just change the learning rate or the number of epochs. Nevertheless I consider it does not scales to complex settings where you try a few dozens of different hyperparametersâ€¦ There is <a href="https://github.com/iterative/dvc/issues/5477#issuecomment-905440724" target="_blank" rel="noopener noreferrer">an open issue</a> where I shared my personal opinion, you can go there and read different thinkings since there is a small discussion going on about this new feature.</p>

<p>Letâ€™s increase our L2 regularization or weight decay to see how it affects our results:</p>

<pre><code class="language-yaml"># dvc.yaml
train:
  cmd: python src/train.py
    --dataset configs/coco_sample.py
    --model configs/retinanet_r50_fpn.py
    --schedule configs/schedule_1x.py
    --runtime configs/runtime.py
    --work_dir training/checkpoints
  deps:
    - configs/
    - processed_data
    - src/train.py
  outs:
    - training/checkpoints
  live:
    training/metrics:
      summary: true
      html: true
  params:
    - optimizer.weight_decay
</code></pre>

<pre><code class="language-yaml"># params.yaml
optimizer:
  weight_decay: 0.001 # this is the same value we have in configs/schedule_1x.py
</code></pre>

<p>Now we run the experiment with our new value for <code>weight_decay</code></p>

<pre><code class="language-bash">dvc exp run -S optimizer.weight_decay=0.001
</code></pre>

<p>Experiment will start running and once is finished we can compare our results by running:</p>

<pre><code class="language-console">$ dvc exp show --no-timestamp --include-metrics val.bbox_mAP --include-metrics val.bbox_mAP_50
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Experiment              â”ƒ val.bbox_mAP â”ƒ val.bbox_mAP_50 â”ƒ optimizer.weight_decay â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ workspace               â”‚          0.2 â”‚           0.371 â”‚ 0.001                  â”‚
â”‚ main                    â”‚            - â”‚               - â”‚ -                      â”‚
â”‚ â”œâ”€â”€ cefe59e [exp-acc34] â”‚        0.198 â”‚           0.368 â”‚ 0.0001                 â”‚
â”‚ â””â”€â”€ 36da522 [exp-6d4ed] â”‚          0.2 â”‚           0.371 â”‚ 0.001                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>

<p>Note I have filtered some of the metrics using <code>--include-metrics</code> flag so can easily see the most important ones. It seems that the â€˜weight_decay` parameter has an impact on results, and we have been to increase our mAP by 0.01. For this, letâ€™s keep this experiment and commit our changes:</p>

<pre><code class="language-bash">dvc exp apply exp-6d4ed 
git add src/train.py dvc.lock params.yaml dvc.yaml configs processed_data training/metrics.json
git commit -a -m "Save experiment with `weight_decay=.001`"
</code></pre>

<p>And finally we can send all our results to respective remotes! These are Github and our Gdrive:</p>

<pre><code class="language-bash">git push
dvc push
</code></pre>

<p>We have covered most of the step of the official <a href="https://dvc.org/doc/start/experiments" target="_blank" rel="noopener noreferrer">DVC experiments tutorial</a>. You can go there and check more info about how cleaning up your experiments and how to pull specific ones.</p>

<h3 id="43-results-visualization">4.3 Results Visualization</h3>

<p>We have trained our model and we have an idea of how it performs thanks to the mAP metrics but we all like to the the bounding boxes over our images so we can get a fully understanding of how the model performs! I have create a simple eval script in <code>src/eval.py</code> that will latest model and paint a subset of validation images. We simply need to add a new step to our <code>dvc.yaml</code>:</p>

<pre><code class="language-yaml">eval: 
  cmd: python src/eval.py
    --dataset configs/coco_sample.py
    --model configs/retinanet_r50_fpn.py
    --checkpoint_file training/checkpoints/latest.pth
    --output_dir eval/  
    --n_samples 20
    --score_threshold .5
  deps:
    - src/eval.py
    - training/checkpoints/latest.pth
  outs:
    - eval/
</code></pre>

<p>I am going to run <code>dvc repro</code> since I have already commit and pushed my changes from last experiment. This is going to create the <code>eval</code> folder which contains the painted images, see a few examples below:</p>

<table>
  <tbody>
    <tr>
      <td><img src="/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/doggy.jpg" alt="dog in couch" target="_blank" rel="noopener noreferrer" /></td>
      <td><img src="/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/fish.jpg" alt="tv fishbowl" target="_blank" rel="noopener noreferrer" /></td>
    </tr>
    <tr>
      <td><img src="/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/tv.jpg" alt="abandoned tv" target="_blank" rel="noopener noreferrer" /></td>
      <td><img src="/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/cat.jpg" alt="lonely cat" target="_blank" rel="noopener noreferrer" /></td>
    </tr>
  </tbody>
</table>

<p>It seems our model is not perfectâ€¦ it mistook a fish tank for a TV! Anyway this was expected, the mAP metric is pretty low but even though we can see how it performs pretty well in the other images. You can go and check more results yourself but keep in mind that SOTA models in COCO dataset (80 classes) achieve a mAP ~0.6 and thatâ€™s a large difference wrt to our simple model. If you want to know more about COCO ranking I recommend you to check <a href="https://paperswithcode.com/sota/object-detection-on-coco" target="_blank" rel="noopener noreferrer">paperswithcode</a> web.</p>

<p>Once we have evaluated our model we can commit and push these results!</p>

<pre><code class="language-bash">git add dvc.lock .gitignore 
git commit -m "Run dvc repro eval"
git push
dvc push
</code></pre>

<h2 id="conclusion">Conclusion</h2>

<p>We can use <code>DVC</code> combined with <code>mmdetection</code> to easily train object detection models, compare them and save different versions and experiments. Summarizing this post we have:</p>

<ul>
  <li>
    <p>Import a dataset into our repository using <code>dvc import</code></p>
  </li>
  <li>
    <p>Setup a <code>DVC</code> data pipeline and understand how it works</p>
  </li>
  <li>Train a model using <code>mmdetection</code>:
    <ul>
      <li>Understand <code>mmdetection</code> config files</li>
      <li>Take advantage of DVC metrics to configure our trainings and compare experiments</li>
      <li>Use <code>dvclive</code> to monitor the training progress</li>
    </ul>
  </li>
  <li>Obtain bbox predictions and paint them over our validation set</li>
</ul>

<p><em>Any ideas for future posts or is there something you would like to comment? Please feel free to reach out via <a href="https://twitter.com/mmeendez8" target="_blank" rel="noopener noreferrer">Twitter</a> or <a href="https://github.com/mmeendez8" target="_blank" rel="noopener noreferrer">Github</a></em></p>

  
</div>

<div class="comments">
  <script src="https://utteranc.es/client.js"
        repo="mmeendez8/mmeendez8.github.io"
        issue-term="url"
        theme="github-light"
        crossorigin="anonymous"
        async>
  </script>
</div>

    <div class="footer">
      <div class="footer-sign">
      <p> <a  href=/index.html#top>Miguel MÃ©ndez </a></p>
      </div>

      <div class="footer-thanks">
        <p>based on <a href="http://web.media.mit.edu/~msaveski" target="_blank" rel="noopener">Martin Saveski</a> and <a href='https://marinaaisa.com/' target="_blank" rel="noopener">Marina Aisa</a> templates</p>
      </div>

      <div class="footer-icons">
        <a href='https://github.com/mmeendez8' target="_blank" rel="noopener">
          <i class="icon-github-circled" aria-hidden="true"></i>
        </a>

        <a href='https://www.linkedin.com/in/miguel-mendez/' target="_blank" rel="noopener">
          <i class="icon-linkedin-squared" aria-hidden="true"></i>
        </a>

        <a href='https://twitter.com/mmeendez8' target="_blank" rel="noopener">
          <i class="icon-twitter-squared" aria-hidden="true"></i>
        </a>

        <a href='https://medium.com/@miguelmendez_' target="_blank" rel="noopener">
          <i class="icon-medium" aria-hidden="true"></i>
        </a>

        <a href='https://stackoverflow.com/users/8380638/m33n' target="_blank" rel="noopener">
          <i class="icon-stackoverflow" aria-hidden="true"></i>
        </a>

        <a href='/feed.xml' target="_blank" rel="noopener">
          <i class="icon-rss-squared" aria-hidden="true"></i>
        </a>
        
      </div>

    </div>

  <!-- End Document
  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“ -->
  <script type="text/javascript" src=/libs/external/lightbox/lightbox.js defer></script>
  
  <!-- Mathjax -->
  

  <!-- Highlight.js -->
  <script>
    document.addEventListener('DOMContentLoaded', (event) => {
        hljs.highlightAll();
    });
</script>

</body>

</html>
