<!DOCTYPE html>
<html lang="en" id="top">

<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->

  <meta charset="utf-8">
  <title>Miguel Méndez | DVC + MMdetection</title>
  <meta name="author" content="Miguel Mendez">
  <meta property="og:website" content="Miguel Mendez personal website">
  
  
  <meta name="description" property="og:description" content="Training a pytorch object detection model with mmdetection combined with DVC (Data Version Control) allows you to version your code, checkpoints and experiments. Learn how to do it and maximize the reproducibility of your experiments!">
  

  
  <meta name="image" property="og:image" content="https://mmeendez8.github.io/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/thumbnail.jpg">
  

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->

  <!-- preload fonts -->
  <link rel="preload" href="/libs/external/fonts/Graphik-Regular.woff2"" as="font" type="font/woff2" crossorigin>
  <link rel="preload" href="/libs/external/fonts/Graphik-Semibold.woff2"" as="font" type="font/woff2" crossorigin>
  <link rel="preload" href="/libs/external/fonts/Tiempos-Headline-Semibold.woff2"" as="font" type="font/woff2" crossorigin>

  <link rel="stylesheet" href=/libs/custom/my_css.css>
  <link rel="stylesheet" href=/libs/external/fonts/fonts.css>

  <!-- hack for non critical css https://web.dev/defer-non-critical-css/ -->
  <link rel="preload" href=/libs/custom/syntax.css as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript><link rel="stylesheet" href=/libs/custom/syntax.css></noscript>
  
  <!-- non critical lighthouse css -->
  <link rel="preload" href=/libs/external/lightbox/lightbox.css as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript><link rel="stylesheet" href=/libs/external/lightbox/lightbox.css></noscript>

  <!-- Fontello
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet"
    href=/libs/external/fontello-bb2d1770/css/fontello.css>

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href=/libs/icon.png>
  <link rel="shortcut icon" type="image/png" href=/libs/icon.png>

  <!-- Google Analytics -->
  <!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-PTDG7548');</script>
<!-- End Google Tag Manager -->

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-LMHYVFNF1J"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-LMHYVFNF1J');
</script>
    <!-- From https://developer.twitter.com/en/docs/twitter-for-websites/javascript-api/guides/set-up-twitter-for-websites -->
  <script>window.twttr = (function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0],
      t = window.twttr || {};
    if (d.getElementById(id)) return t;
    js = d.createElement(s);
    js.id = id;
    js.src = "https://platform.twitter.com/widgets.js";
    fjs.parentNode.insertBefore(js, fjs);

    t._e = [];
    t.ready = function(f) {
      t._e.push(f);
    };

    return t;
  }(document, "script", "twitter-wjs"));</script>

  <!-- Twitter cards -->
  <meta name="twitter:site" content="@https://twitter.com/mmeendez8">
  <meta name="twitter:title" content="DVC + MMdetection">
  
  
  <meta name="twitter:description" content="Training a pytorch object detection model with mmdetection combined with DVC (Data Version Control) allows you to version your code, checkpoints and experiments. Learn how to do it and maximize the reproducibility of your experiments!">
  
  

  
  <meta name="twitter:card"  content="summary_large_image">
  <meta name="twitter:image" content="https://mmeendez8.github.io/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/thumbnail.jpg">
  

  <!-- end of Twitter cards -->
  

  <!-- Mathjax -->
  
</head>

<body>

  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PTDG7548"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  <header class="the-post-header">
    <div class="container">
      <a href="/">
        <h3>Miguel Méndez</h3>
      </a>
      <div>
        <a  href=/index.html#posts>
         <h3 class="posts-link">Posts</h3>
        </a>
      </div>
    </div>
</header>

<div class="the-post-title-placeholder">
  <div class="offset">
    <div class="the-post-title-text">
      <span class="the-post-date">August 30, 2021 </span>
      <h1 class="the-post-title">DVC + MMdetection</h1>
      <p>A guide to train, monitor, compare and evaluate your pytorch object detection models</p>
    </div>
  </div>

  <div class="the-post-title-image">
    <img src="/generated/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/thumbnail-800-1f776a3e5.jpg" alt="DVC + MMdetection" srcset="/generated/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/thumbnail-400-b7ca4b2d0.webp 400w, /generated/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/thumbnail-600-b7ca4b2d0.webp 600w, /generated/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/thumbnail-800-b7ca4b2d0.webp 800w, /generated/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/thumbnail-1000-b7ca4b2d0.webp 1000w" sizes="(max-width: 767px) 100vw, 50vw" width="1600" height="836">

  </div>
</div>

<div class="share-button">
  <a href="https://twitter.com/share?ref_src=twsrc%5Etfw" class="twitter-share-button" data-show-count="false" ,
    data-size="large" , data-text="DVC + MMdetection" , data-via="mmeendez8" , data-url="/2021/08/30/mmdet-dvc-tutorial.html">
  </a>
</div>

<div class="container the-post-content">

  <p>I recently <a href="/2021/07/01/dvc-tutorial.html" target="_blank" rel="noopener noreferrer">published a post</a> where I showed how to use DVC to maintain versions of our datasets so we reduce data reproducibility problems to a minimum. This is the second part of the tutorial where we are going to see how we can combine the power of mmdetection framework and its huge <a href="https://github.com/open-mmlab/mmdetection/blob/master/docs/model_zoo.md" target="_blank" rel="noopener noreferrer">model zoo</a> with DVC for designing ML pipelines, versioning our models and monitor training progress.</p>

<p>It is quite a lot of content to cover, so I will be going through it step by step and trying to keep things as simple as possible. You can find all the code for this tutorial in my <a href="https://github.com/mmeendez8/mmdetection_dvc" target="_blank" rel="noopener noreferrer">Github</a>. So let’s start with it!</p>

<h2 id="1-setup-the-environment">1. Setup the environment</h2>

<p>We are gonna need a few packages to get our project up and running. I have created a <code class="language-plaintext highlighter-rouge">conda.yml</code> that you can find in the root of the repository, this is going to install pytorch and cudatoolkit since we are going to train our models using a GPU. You can create the environment by:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">env </span>create <span class="nt">-f</span> conda.yaml
conda activate mmdetection_dvc
</code></pre></div></div>

<h2 id="2-import-our-dataset">2. Import our dataset</h2>

<p>In the previous post we used a subset of the COCO dataset created by fast.ai. We push all data to a Google Drive remote storage using DVC and keep all metada files in a Github repository. We need now to import this dataset in our repo and that’s exactly what <a href="https://dvc.org/doc/command-reference/import" target="_blank" rel="noopener noreferrer">dvc import</a> can do for us!</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dvc init
dvc import <span class="s2">"git@github.com:mmeendez8/coco_sample.git"</span> <span class="s2">"data/"</span> <span class="nt">-o</span> <span class="s2">"data/"</span>
</code></pre></div></div>

<p>Note that we are importing <code class="language-plaintext highlighter-rouge">/data</code> folder of the remote repository into the <code class="language-plaintext highlighter-rouge">data/</code> directory of our project where all our data will be stored. This may take a while since we need to download all images and annotations from the remote gdrive storage. Let’s now publish the changes on git:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git add <span class="nb">.</span> 
git commit <span class="nt">-m</span> <span class="s2">"Import coco_sample dataset"</span> 
git push
</code></pre></div></div>

<p>Once it is downloaded, we can move between different <a href="https://github.com/mmeendez8/coco_sample/releases" target="_blank" rel="noopener noreferrer">versions of the dataset</a> with the <a href="https://dvc.org/doc/command-reference/update" target="_blank" rel="noopener noreferrer">dvc update</a> command. If we would go back to v1.0 of our dataset and push our changes to git:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dvc update <span class="nt">--rev</span> v1.0 data.dvc
git add data.dvc
git commit <span class="nt">-m</span> <span class="s2">"Get version 1.0 of coco_sample"</span>
git push
</code></pre></div></div>

<p>That’s it! We have imported our dataset and we know how to move between different versions so let’s create a script that will train our model!</p>

<h2 id="3-train-our-model">3. Train our model</h2>

<p><a href="https://github.com/open-mmlab/mmdetection" target="_blank" rel="noopener noreferrer">MMDetection</a> is an open source object detection toolbox based on PyTorch. It is a part of the OpenMMLab project and it is one of the most popular computer vision frameworks. I love it and I am an active contributor since it became my default framework for object detection last year.</p>

<p>They have an extense documentation which really helps first time users. In this post I will skip the very basics and focus on showing how easily can we train a RetinaNet object detector on our coco_sample dataset.</p>

<h3 id="31-model-config">3.1. Model config</h3>

<p>First thing we need to do is to find the config file for our model, so let’s explore mmdet model zoo and more specifically <a href="https://github.com/open-mmlab/mmdetection/tree/master/configs/retinanet" target="_blank" rel="noopener noreferrer">RetinaNet section</a>. There’s a bunch of different RetinaNet models there but let’s stick with the base config from the <a href="https://arxiv.org/pdf/1708.02002.pdf" target="_blank" rel="noopener noreferrer">original paper</a>. I have already downloaded this file to my repo and you can find it under <code class="language-plaintext highlighter-rouge">configs/retinanet_r50_fpn.py</code>. There are three main sections there:</p>

<ul>
  <li>The backbone definition, which in our case is a ResNet50. Its weights come from some torchvision checkpoint specified at:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">init_cfg</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="sh">'</span><span class="s">Pretrained</span><span class="sh">'</span><span class="p">,</span> <span class="n">checkpoint</span><span class="o">=</span><span class="sh">'</span><span class="s">torchvision://resnet50</span><span class="sh">'</span><span class="p">))</span><span class="err">`</span>
</code></pre></div></div>

<p>As a curious fact, I checked out <a href="https://pytorch.org/vision/stable/models.html" target="_blank" rel="noopener noreferrer">official torchvision documentation</a> and it seems this network has been trained with some dataset that is currently lost so there is no chance to reproduce this results…</p>

<center>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Just found this checking <a href="https://twitter.com/hashtag/torchvision?src=hash&amp;ref_src=twsrc%5Etfw">#torchvision</a> stable models, it seems they were trained on some volatile dataset 😅<br />cc <a href="https://twitter.com/DVCorg?ref_src=twsrc%5Etfw">@DVCorg</a> <a href="https://t.co/rR8ANSmucI">pic.twitter.com/rR8ANSmucI</a></p>&mdash; Miguel Mendez (@mmeendez8) <a href="https://twitter.com/mmeendez8/status/1418507102465765376?ref_src=twsrc%5Etfw">July 23, 2021</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>

<ul>
  <li>
    <p>The <a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener noreferrer">Feature Pyramid Network (FPN)</a> configuration which is useful to find features at different scale levels.</p>
  </li>
  <li>
    <p>The classification head and the regression head. They predict labels and bounding boxes regression parameters for each of the anchors of the model. I cannot really go deep how this model works and what anchors are but you should check our repo <a href="https://github.com/Gradiant/pyodi" target="_blank" rel="noopener noreferrer">pyodi</a> if you really want to understand all the details.</p>
  </li>
</ul>

<h3 id="32-dataset-config">3.2 Dataset config</h3>

<p>Mmdetection framework also uses config files for datasets. There we define our train and validation data and which types of transformation do we want to apply before images are feed into the network. Since our dataset follows COCO format, I just modified original <a href="https://github.com/open-mmlab/mmdetection/blob/master/configs/_base_/datasets/coco_detection.py" target="_blank" rel="noopener noreferrer">COCO_detection.py</a>. Note that:</p>

<ul>
  <li>I removed the test set since we are not going to use one for this tutorial.</li>
  <li>I added a <code class="language-plaintext highlighter-rouge">CLASSES</code> variable with our reduced set of labels.</li>
</ul>

<p>You can check the dataset config file in <code class="language-plaintext highlighter-rouge">configs/coco_sample.py</code></p>

<h3 id="33-train-configuration">3.3 Train configuration</h3>

<p>There are multiple training parameters we can configure using mmdetection. For this simple demo we are going to use the default scheduler (see <code class="language-plaintext highlighter-rouge">configs/scheduler.py</code>). It uses SGD and a dynamic learning rate policy and that’s mostly what we need to know for now.</p>

<p>Our runtime definition is under <code class="language-plaintext highlighter-rouge">configs/runtime.py</code> and we are going to specify a few interesting things there:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">checkpoint_config</code>: specifies the checkpoint saving frequency</li>
  <li><code class="language-plaintext highlighter-rouge">log_config</code>: allows us to select a specific logger for our training</li>
  <li><code class="language-plaintext highlighter-rouge">custom_hooks</code>: extra hooks that we can insert or create for retrieving or adding functionalities to our training</li>
  <li><code class="language-plaintext highlighter-rouge">workflow</code>: it defines training workflow, this is, how many training epochs do we want to run before a validation one.</li>
</ul>

<p>Since we are using DVC, we are also going to use the <a href="https://dvc.org/doc/dvclive" target="_blank" rel="noopener noreferrer">DVCLive</a> hook. DVCLive is an open-source Python library for monitoring the progress of metrics during training of machine learning models. It is a recent and super cool library with git integration and that’s all I need! See how simple is to add this hook:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">log_config</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">(</span>
    <span class="n">interval</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">hooks</span><span class="o">=</span><span class="p">[</span>
        <span class="nf">dict</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="sh">'</span><span class="s">TextLoggerHook</span><span class="sh">'</span><span class="p">),</span>
        <span class="nf">dict</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="sh">"</span><span class="s">DvcliveLoggerHook</span><span class="sh">"</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="sh">"</span><span class="s">training/metrics</span><span class="sh">"</span><span class="p">),</span>
    <span class="p">])</span>
</code></pre></div></div>

<h2 id="4-data-pipeline">4. Data pipeline</h2>

<p>Let’s create our first <a href="https://dvc.org/doc/start/data-pipelines" target="_blank" rel="noopener noreferrer">data pipeline</a>! Ideally (and following DVC docs) we should use dvc run commands so the pipeline gets automatically generated but… I feel more comfortable creating a dvc.yaml and filling it myself.</p>

<h3 id="41-prepare-the-data">4.1 Prepare the data</h3>

<p>Our COCO annotations are missing a few fields because fast.ai guys considered them unnecessary (they actually are) so they removed all extra fields to reduce the final size of the json. That’s fair enough, but mmdetection needs them so I created a very simple script that will prepare the data for us, you can find it in <code class="language-plaintext highlighter-rouge">src/prepare_data.py</code>.</p>

<p>The first step of our data pipeline will prepare our annotation file and save the modified COCO files into prepare_data. You can simply add the following to your <code class="language-plaintext highlighter-rouge">dvc.yaml</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">stages</span><span class="pi">:</span>
  <span class="na">prepare_data</span><span class="pi">:</span>
    <span class="na">foreach</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">train</span>
      <span class="pi">-</span> <span class="s">val</span>
    <span class="na">do</span><span class="pi">:</span>
      <span class="na">cmd</span><span class="pi">:</span> <span class="s">python src/prepare_data.py</span>
        <span class="s">--coco_file data/coco_sample/annotations/split_${item}.json</span>
        <span class="s">--output_file processed_data/${item}_split_with_ann_id.json</span>
      <span class="na">deps</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">data/coco_sample/annotations/split_${item}.json</span>
      <span class="na">outs</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">processed_data/${item}_split_with_ann_id.json</span>
</code></pre></div></div>

<p>There’s a stage called called prepare_data that run a small for loop over values <code class="language-plaintext highlighter-rouge">[train, val]</code> and calls the prepare_data script. See how I have specified the original json files as dependencies and the new one as outputs so DVC knows how to track them.</p>

<p>You can now call run the pipeline with <code class="language-plaintext highlighter-rouge">dvc repro</code> and the new annotations file should appear!</p>

<h3 id="42-train-the-model">4.2 Train the model</h3>

<p>I have created a simple training script in <code class="language-plaintext highlighter-rouge">src/train.py</code> that adjusts to our needs. You could also use <a href="https://github.com/open-mmlab/mmdetection/blob/master/tools/train.py" target="_blank" rel="noopener noreferrer">mmdetection train tool</a> since I just applied some minor modifications to it that will allow us to use dvc params.</p>

<p>We can add a new step to our data pipeline that executes our training step. For example this would be enough for running an experiment with our actual configs:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">train</span><span class="pi">:</span>
  <span class="na">cmd</span><span class="pi">:</span> <span class="s">python src/train.py</span>
    <span class="s">--dataset configs/coco_sample.py</span>
    <span class="s">--model configs/retinanet_r50_fpn.py</span>
    <span class="s">--schedule configs/schedule_1x.py</span>
    <span class="s">--runtime configs/runtime.py</span>
    <span class="s">--work_dir training/checkpoints</span>
  <span class="na">deps</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">configs/</span>
    <span class="pi">-</span> <span class="s">processed_data</span>
    <span class="pi">-</span> <span class="s">src/train.py</span>
  <span class="na">outs</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">training/checkpoints</span>
  <span class="na">live</span><span class="pi">:</span>
    <span class="na">training/metrics</span><span class="pi">:</span>
      <span class="na">summary</span><span class="pi">:</span> <span class="kc">true</span>
      <span class="na">html</span><span class="pi">:</span> <span class="kc">true</span>
</code></pre></div></div>

<p>Note how I have added the <code class="language-plaintext highlighter-rouge">live</code> key to notify DVC that our script will be saving metrics in the <code class="language-plaintext highlighter-rouge">training/metrics</code> folder. Also, this will generate a html file that we can use to visualize in real time our train progress. So simple!</p>

<p>We can run again DVC repro as many times as we want changing our config files as needed for trying different hyperparameters or model configurations. Nevertheless, DVC guys recommend yo to use <a href="https://dvc.org/doc/start/experiments" target="_blank" rel="noopener noreferrer">DVC experiments</a> when you are tryining different configurations. So that’s what we are going to do! Note this is a recent feature and I had to open a couple issues since I found a couple “bugs” or unexpected behavior such <a href="https://github.com/iterative/dvc/issues/6465">[1]</a>, <a href="https://github.com/iterative/dvc/issues/5477?notification_referrer_id=MDE4Ok5vdGlmaWNhdGlvblRocmVhZDE1OTA3ODE1MTM6MTU5Njk2Njc%3D#issuecomment-905234950">[2]</a>.</p>

<p>Let’s do our first training by running <code class="language-plaintext highlighter-rouge">dvc exp run</code>! You can monitor training progress by opening your <code class="language-plaintext highlighter-rouge">training/metrics.html</code> file:</p>

<table>
  <tbody>
    <tr>
      <td><img src="/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/loss.webp" alt="loss curve" target="_blank" rel="noopener noreferrer" /></td>
      <td><img src="/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/vmap.webp" alt="vmap curve" target="_blank" rel="noopener noreferrer" /></td>
    </tr>
  </tbody>
</table>

<p>Training will be done soon (depening on your GPU and machine) and we can check our results by running:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>dvc exp show
<span class="go">┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━
┃ Experiment              ┃ Created  ┃  step ┃ learning_rate ┃ momentum ┃ val.bbox_mAP ┃ val.bbox_mAP_50 ┃ val.bbox_mAP_75 ┃ val.bbox_mAP_s ┃ val.bbox_mAP_m ┃ val.bbox
┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━
│ workspace               │ -        │ 13057 │         1e-05 │      0.9 │        0.198 │           0.368 │           0.194 │          0.008 │          0.142 │         
│ main                    │ 11:24 AM │     - │             - │        - │            - │               - │               - │              - │              - │         
│ └── cefe59e [exp-acc34] │ 01:20 PM │ 13057 │         1e-05 │      0.9 │        0.198 │           0.368 │           0.194 │          0.008 │          0.142 │         
└─────────────────────────┴──────────┴───────┴───────────────┴──────────┴──────────────┴─────────────────┴─────────────────┴────────────────┴────────────────┴─────────
</span></code></pre></div></div>

<p>The possibilty of tracking your hyperparameters is what I most like about experiments. We can change one hyperparameter for a experiment, DVC will remember this for you and will help you to compare different experiments. This is nice but there is room for improvement since at this moment for running an experiment with a different parameter we need to:</p>

<ol>
  <li>Add our parameter to our <code class="language-plaintext highlighter-rouge">params.yaml</code></li>
  <li>Specify that our train step depends on this parameter</li>
  <li>Run experiment with -S flag updating the parameter value.</li>
</ol>

<p>These steps are fine when you just change the learning rate or the number of epochs. Nevertheless I consider it does not scales to complex settings where you try a few dozens of different hyperparameters… There is <a href="https://github.com/iterative/dvc/issues/5477#issuecomment-905440724" target="_blank" rel="noopener noreferrer">an open issue</a> where I shared my personal opinion, you can go there and read different thinkings since there is a small discussion going on about this new feature.</p>

<p>Let’s increase our L2 regularization or weight decay to see how it affects our results:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># dvc.yaml</span>
<span class="na">train</span><span class="pi">:</span>
  <span class="na">cmd</span><span class="pi">:</span> <span class="s">python src/train.py</span>
    <span class="s">--dataset configs/coco_sample.py</span>
    <span class="s">--model configs/retinanet_r50_fpn.py</span>
    <span class="s">--schedule configs/schedule_1x.py</span>
    <span class="s">--runtime configs/runtime.py</span>
    <span class="s">--work_dir training/checkpoints</span>
  <span class="na">deps</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">configs/</span>
    <span class="pi">-</span> <span class="s">processed_data</span>
    <span class="pi">-</span> <span class="s">src/train.py</span>
  <span class="na">outs</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">training/checkpoints</span>
  <span class="na">live</span><span class="pi">:</span>
    <span class="na">training/metrics</span><span class="pi">:</span>
      <span class="na">summary</span><span class="pi">:</span> <span class="kc">true</span>
      <span class="na">html</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">params</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">optimizer.weight_decay</span>
</code></pre></div></div>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># params.yaml</span>
<span class="na">optimizer</span><span class="pi">:</span>
  <span class="na">weight_decay</span><span class="pi">:</span> <span class="m">0.001</span> <span class="c1"># this is the same value we have in configs/schedule_1x.py</span>
</code></pre></div></div>

<p>Now we run the experiment with our new value for <code class="language-plaintext highlighter-rouge">weight_decay</code></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dvc exp run <span class="nt">-S</span> optimizer.weight_decay<span class="o">=</span>0.001
</code></pre></div></div>

<p>Experiment will start running and once is finished we can compare our results by running:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>dvc exp show <span class="nt">--no-timestamp</span> <span class="nt">--include-metrics</span> val.bbox_mAP <span class="nt">--include-metrics</span> val.bbox_mAP_50
<span class="go">┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Experiment              ┃ val.bbox_mAP ┃ val.bbox_mAP_50 ┃ optimizer.weight_decay ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩
│ workspace               │          0.2 │           0.371 │ 0.001                  │
│ main                    │            - │               - │ -                      │
│ ├── cefe59e [exp-acc34] │        0.198 │           0.368 │ 0.0001                 │
│ └── 36da522 [exp-6d4ed] │          0.2 │           0.371 │ 0.001                  │
└─────────────────────────┴──────────────┴─────────────────┴────────────────────────┘
</span></code></pre></div></div>

<p>Note I have filtered some of the metrics using <code class="language-plaintext highlighter-rouge">--include-metrics</code> flag so can easily see the most important ones. It seems that the ‘weight_decay` parameter has an impact on results, and we have been to increase our mAP by 0.01. For this, let’s keep this experiment and commit our changes:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dvc exp apply exp-6d4ed 
git add src/train.py dvc.lock params.yaml dvc.yaml configs processed_data training/metrics.json
git commit <span class="nt">-a</span> <span class="nt">-m</span> <span class="s2">"Save experiment with </span><span class="sb">`</span><span class="nv">weight_decay</span><span class="o">=</span>.001<span class="sb">`</span><span class="s2">"</span>
</code></pre></div></div>

<p>And finally we can send all our results to respective remotes! These are Github and our Gdrive:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git push
dvc push
</code></pre></div></div>

<p>We have covered most of the step of the official <a href="https://dvc.org/doc/start/experiments" target="_blank" rel="noopener noreferrer">DVC experiments tutorial</a>. You can go there and check more info about how cleaning up your experiments and how to pull specific ones.</p>

<h3 id="43-results-visualization">4.3 Results Visualization</h3>

<p>We have trained our model and we have an idea of how it performs thanks to the mAP metrics but we all like to the the bounding boxes over our images so we can get a fully understanding of how the model performs! I have create a simple eval script in <code class="language-plaintext highlighter-rouge">src/eval.py</code> that will latest model and paint a subset of validation images. We simply need to add a new step to our <code class="language-plaintext highlighter-rouge">dvc.yaml</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">eval</span><span class="pi">:</span> 
  <span class="na">cmd</span><span class="pi">:</span> <span class="s">python src/eval.py</span>
    <span class="s">--dataset configs/coco_sample.py</span>
    <span class="s">--model configs/retinanet_r50_fpn.py</span>
    <span class="s">--checkpoint_file training/checkpoints/latest.pth</span>
    <span class="s">--output_dir eval/</span>  
    <span class="s">--n_samples </span><span class="m">20</span>
    <span class="s">--score_threshold .5</span>
  <span class="na">deps</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">src/eval.py</span>
    <span class="pi">-</span> <span class="s">training/checkpoints/latest.pth</span>
  <span class="na">outs</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">eval/</span>
</code></pre></div></div>

<p>I am going to run <code class="language-plaintext highlighter-rouge">dvc repro</code> since I have already commit and pushed my changes from last experiment. This is going to create the <code class="language-plaintext highlighter-rouge">eval</code> folder which contains the painted images, see a few examples below:</p>

<table>
  <tbody>
    <tr>
      <td><img src="/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/doggy.webp" alt="dog in couch" target="_blank" rel="noopener noreferrer" /></td>
      <td><img src="/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/fish.webp" alt="tv fishbowl" target="_blank" rel="noopener noreferrer" /></td>
    </tr>
    <tr>
      <td><img src="/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/tv.webp" alt="abandoned tv" target="_blank" rel="noopener noreferrer" /></td>
      <td><img src="/assets/images/fullsize/posts/2021-08-30-mmdet-dvc-tutorial/cat.webp" alt="lonely cat" target="_blank" rel="noopener noreferrer" /></td>
    </tr>
  </tbody>
</table>

<p>It seems our model is not perfect… it mistook a fish tank for a TV! Anyway this was expected, the mAP metric is pretty low but even though we can see how it performs pretty well in the other images. You can go and check more results yourself but keep in mind that SOTA models in COCO dataset (80 classes) achieve a mAP ~0.6 and that’s a large difference wrt to our simple model. If you want to know more about COCO ranking I recommend you to check <a href="https://paperswithcode.com/sota/object-detection-on-coco" target="_blank" rel="noopener noreferrer">paperswithcode</a> web.</p>

<p>Once we have evaluated our model we can commit and push these results!</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git add dvc.lock .gitignore 
git commit <span class="nt">-m</span> <span class="s2">"Run dvc repro eval"</span>
git push
dvc push
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>We can use <code class="language-plaintext highlighter-rouge">DVC</code> combined with <code class="language-plaintext highlighter-rouge">mmdetection</code> to easily train object detection models, compare them and save different versions and experiments. Summarizing this post we have:</p>

<ul>
  <li>
    <p>Import a dataset into our repository using <code class="language-plaintext highlighter-rouge">dvc import</code></p>
  </li>
  <li>
    <p>Setup a <code class="language-plaintext highlighter-rouge">DVC</code> data pipeline and understand how it works</p>
  </li>
  <li>Train a model using <code class="language-plaintext highlighter-rouge">mmdetection</code>:
    <ul>
      <li>Understand <code class="language-plaintext highlighter-rouge">mmdetection</code> config files</li>
      <li>Take advantage of DVC metrics to configure our trainings and compare experiments</li>
      <li>Use <code class="language-plaintext highlighter-rouge">dvclive</code> to monitor the training progress</li>
    </ul>
  </li>
  <li>Obtain bbox predictions and paint them over our validation set</li>
</ul>

<p><em>Any ideas for future posts or is there something you would like to comment? Please feel free to reach out via <a href="https://twitter.com/mmeendez8" target="_blank" rel="noopener noreferrer">Twitter</a> or <a href="https://github.com/mmeendez8" target="_blank" rel="noopener noreferrer">Github</a></em></p>

  
</div>

<div class="comments">
  <script src="https://utteranc.es/client.js"
        repo="mmeendez8/mmeendez8.github.io"
        issue-term="url"
        theme="github-light"
        crossorigin="anonymous"
        async>
  </script>
</div>

    <div class="footer">
      <div class="footer-sign">
      <p> <a  href=/index.html#top>Miguel Méndez </a></p>
      </div>

      <div class="footer-thanks">
        <p>based on <a href="http://web.media.mit.edu/~msaveski" target="_blank" rel="noopener">Martin Saveski</a> and <a href='https://marinaaisa.com/' target="_blank" rel="noopener">Marina Aisa</a> templates</p>
      </div>

      <div class="footer-icons">
        <a href='https://github.com/mmeendez8' target="_blank" rel="noopener">
          <i class="icon-github-circled" aria-hidden="true"></i>
        </a>

        <a href='https://www.linkedin.com/in/miguel-mendez/' target="_blank" rel="noopener">
          <i class="icon-linkedin-squared" aria-hidden="true"></i>
        </a>

        <a href='https://twitter.com/mmeendez8' target="_blank" rel="noopener">
          <i class="icon-twitter-squared" aria-hidden="true"></i>
        </a>

        <a href='https://medium.com/@miguelmendez_' target="_blank" rel="noopener">
          <i class="icon-medium" aria-hidden="true"></i>
        </a>

        <a href='https://stackoverflow.com/users/8380638/m33n' target="_blank" rel="noopener">
          <i class="icon-stackoverflow" aria-hidden="true"></i>
        </a>

        <a href='/feed.xml' target="_blank" rel="noopener">
          <i class="icon-rss-squared" aria-hidden="true"></i>
        </a>
        
      </div>

    </div>

  <!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <script type="text/javascript" src=/libs/external/lightbox/lightbox.js defer></script>
</body>

</html>
